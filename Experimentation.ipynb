{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06230b9-2be5-4d1a-9026-81013f166081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3134ba92-7590-4d20-8809-3c13a01282ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "from typing import Iterable, Tuple, List\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "001d43b8-ddbd-4c86-aee1-92e033ab154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= Processing folder 0002 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restart 1/2: 100%|███████████████████████████| 50/50 [02:59<00:00,  3.59s/it, Best=-585.028, Curr=-585.028, Temp=0.778]\n",
      "Restart 2/2: 100%|███████████████████████████| 50/50 [03:31<00:00,  4.23s/it, Best=-580.869, Curr=-580.869, Temp=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST MAP for 0002 ===\n",
      "Active potencies (multi-type):\n",
      "   {-7,-8}\n",
      "   {-1,-7,-8}\n",
      "   {-1,-14,-4,-8}\n",
      "   {-1,-14,-7,-8}\n",
      "   {-1,-14,-4,-7,-8,-9}\n",
      "Singletons (always active):\n",
      "   {-1}\n",
      "   {-14}\n",
      "   {-4}\n",
      "   {-7}\n",
      "   {-8}\n",
      "   {-9}\n",
      "\n",
      "Edges:\n",
      "  {-7,-8} -> {-7}\n",
      "  {-7,-8} -> {-8}\n",
      "  {-1,-7,-8} -> {-1}\n",
      "  {-1,-7,-8} -> {-7,-8}\n",
      "  {-1,-14,-7,-8} -> {-14}\n",
      "  {-1,-14,-7,-8} -> {-1,-7,-8}\n",
      "  {-1,-14,-4,-7,-8,-9} -> {-4}\n",
      "  {-1,-14,-4,-7,-8,-9} -> {-8}\n",
      "  {-1,-14,-4,-7,-8,-9} -> {-9}\n",
      "  {-1,-14,-4,-7,-8,-9} -> {-1,-7,-8}\n",
      "  {-1,-14,-4,-7,-8,-9} -> {-1,-14,-7,-8}\n",
      "\n",
      "Scores:\n",
      "  log posterior: -580.868987\n",
      "  Tree 1 log P(T|F*): -151.759835\n",
      "  Tree 2 log P(T|F*): -109.959959\n",
      "  Tree 3 log P(T|F*): -75.160542\n",
      "  Tree 4 log P(T|F*): -77.668814\n",
      "  Tree 5 log P(T|F*): -130.021347\n",
      "\n",
      "=== Ground Truth Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -98.147510\n",
      "Tree 2: log P(T|F) = -85.047742\n",
      "Tree 3: log P(T|F) = -81.850154\n",
      "Tree 4: log P(T|F) = -60.188143\n",
      "Tree 5: log P(T|F) = -106.832160\n",
      "Total log-likelihood = -432.065708\n",
      "\n",
      "Predicted Sets:\n",
      "\n",
      "Predicted Sets:\n",
      "   ['-7', '-8']\n",
      "   ['-1', '-7', '-8']\n",
      "   ['-1', '-14', '-4', '-8']\n",
      "   ['-1', '-14', '-7', '-8']\n",
      "   ['-1', '-14', '-4', '-7', '-8', '-9']\n",
      "\n",
      "Ground Truth Sets:\n",
      "\n",
      "Ground Truth Sets:\n",
      "   ['-14', '-7']\n",
      "   ['-7', '-8']\n",
      "   ['-14', '-7', '-8']\n",
      "   ['-1', '-14', '-7', '-8']\n",
      "   ['-1', '-14', '-4', '-7', '-8', '-9']\n",
      "\n",
      "=== Jaccard Distance ===\n",
      "Jaccard Distance (Predicted vs Ground Truth): 0.571429\n",
      "Predicted map's loss: -580.868987\n",
      "Ground truth's loss: -432.065708\n",
      "\n",
      "================= Processing folder 0003 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restart 1/2: 100%|███████████████████████████| 50/50 [03:12<00:00,  3.84s/it, Best=-871.904, Curr=-871.904, Temp=0.778]\n",
      "Restart 2/2: 100%|███████████████████████████| 50/50 [03:02<00:00,  3.65s/it, Best=-355.273, Curr=-356.659, Temp=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST MAP for 0003 ===\n",
      "Active potencies (multi-type):\n",
      "   {-4,-9}\n",
      "   {-5,-6}\n",
      "   {-16,-5,-6}\n",
      "   {-4,-5,-9}\n",
      "   {-13,-16,-4,-5,-6,-9}\n",
      "Singletons (always active):\n",
      "   {-13}\n",
      "   {-16}\n",
      "   {-4}\n",
      "   {-5}\n",
      "   {-6}\n",
      "   {-9}\n",
      "\n",
      "Edges:\n",
      "  {-4,-9} -> {-4}\n",
      "  {-4,-9} -> {-9}\n",
      "  {-5,-6} -> {-5}\n",
      "  {-5,-6} -> {-6}\n",
      "  {-16,-5,-6} -> {-16}\n",
      "  {-16,-5,-6} -> {-6}\n",
      "  {-4,-5,-9} -> {-4}\n",
      "  {-4,-5,-9} -> {-5}\n",
      "  {-4,-5,-9} -> {-9}\n",
      "  {-16,-5,-6} -> {-5,-6}\n",
      "  {-4,-5,-9} -> {-4,-9}\n",
      "  {-13,-16,-4,-5,-6,-9} -> {-13}\n",
      "  {-13,-16,-4,-5,-6,-9} -> {-4}\n",
      "  {-13,-16,-4,-5,-6,-9} -> {-6}\n",
      "  {-13,-16,-4,-5,-6,-9} -> {-5,-6}\n",
      "  {-13,-16,-4,-5,-6,-9} -> {-16,-5,-6}\n",
      "  {-13,-16,-4,-5,-6,-9} -> {-4,-5,-9}\n",
      "\n",
      "Scores:\n",
      "  log posterior: -355.272778\n",
      "  Tree 1 log P(T|F*): -83.403310\n",
      "  Tree 2 log P(T|F*): -96.794197\n",
      "  Tree 3 log P(T|F*): -31.762153\n",
      "  Tree 4 log P(T|F*): -22.474513\n",
      "  Tree 5 log P(T|F*): -77.114923\n",
      "\n",
      "=== Ground Truth Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -83.403310\n",
      "Tree 2: log P(T|F) = -96.794197\n",
      "Tree 3: log P(T|F) = -31.762153\n",
      "Tree 4: log P(T|F) = -22.474513\n",
      "Tree 5: log P(T|F) = -77.114923\n",
      "Total log-likelihood = -311.549095\n",
      "\n",
      "Predicted Sets:\n",
      "\n",
      "Predicted Sets:\n",
      "   ['-4', '-9']\n",
      "   ['-5', '-6']\n",
      "   ['-16', '-5', '-6']\n",
      "   ['-4', '-5', '-9']\n",
      "   ['-13', '-16', '-4', '-5', '-6', '-9']\n",
      "\n",
      "Ground Truth Sets:\n",
      "\n",
      "Ground Truth Sets:\n",
      "   ['-4', '-9']\n",
      "   ['-5', '-6']\n",
      "   ['-16', '-5', '-6']\n",
      "   ['-4', '-5', '-9']\n",
      "   ['-13', '-16', '-4', '-5', '-6', '-9']\n",
      "\n",
      "=== Jaccard Distance ===\n",
      "Jaccard Distance (Predicted vs Ground Truth): 0.000000\n",
      "Predicted map's loss: -355.272778\n",
      "Ground truth's loss: -311.549095\n",
      "\n",
      "================= Processing folder 0004 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restart 1/2: 100%|███████████████████████████| 50/50 [02:38<00:00,  3.17s/it, Best=-950.456, Curr=-950.456, Temp=0.778]\n",
      "Restart 2/2: 100%|███████████████████████████| 50/50 [03:06<00:00,  3.73s/it, Best=-847.641, Curr=-847.641, Temp=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST MAP for 0004 ===\n",
      "Active potencies (multi-type):\n",
      "   {-1,-9}\n",
      "   {-10,-9}\n",
      "   {-13,-9}\n",
      "   {-1,-13,-9}\n",
      "   {-1,-10,-11,-13,-6,-9}\n",
      "Singletons (always active):\n",
      "   {-1}\n",
      "   {-10}\n",
      "   {-11}\n",
      "   {-13}\n",
      "   {-6}\n",
      "   {-9}\n",
      "\n",
      "Edges:\n",
      "  {-1,-9} -> {-1}\n",
      "  {-1,-9} -> {-9}\n",
      "  {-10,-9} -> {-10}\n",
      "  {-10,-9} -> {-9}\n",
      "  {-13,-9} -> {-13}\n",
      "  {-13,-9} -> {-9}\n",
      "  {-1,-13,-9} -> {-9}\n",
      "  {-1,-13,-9} -> {-1,-9}\n",
      "  {-1,-13,-9} -> {-13,-9}\n",
      "  {-1,-10,-11,-13,-6,-9} -> {-10}\n",
      "  {-1,-10,-11,-13,-6,-9} -> {-11}\n",
      "  {-1,-10,-11,-13,-6,-9} -> {-6}\n",
      "  {-1,-10,-11,-13,-6,-9} -> {-9}\n",
      "  {-1,-10,-11,-13,-6,-9} -> {-10,-9}\n",
      "  {-1,-10,-11,-13,-6,-9} -> {-13,-9}\n",
      "  {-1,-10,-11,-13,-6,-9} -> {-1,-13,-9}\n",
      "\n",
      "Scores:\n",
      "  log posterior: -847.641485\n",
      "  Tree 1 log P(T|F*): -216.451918\n",
      "  Tree 2 log P(T|F*): -79.342172\n",
      "  Tree 3 log P(T|F*): -112.401717\n",
      "  Tree 4 log P(T|F*): -244.095979\n",
      "  Tree 5 log P(T|F*): -153.235454\n",
      "\n",
      "=== Ground Truth Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -86.677931\n",
      "Tree 2: log P(T|F) = -86.599164\n",
      "Tree 3: log P(T|F) = -76.579082\n",
      "Tree 4: log P(T|F) = -136.739474\n",
      "Tree 5: log P(T|F) = -106.270250\n",
      "Total log-likelihood = -492.865902\n",
      "\n",
      "Predicted Sets:\n",
      "\n",
      "Predicted Sets:\n",
      "   ['-1', '-9']\n",
      "   ['-10', '-9']\n",
      "   ['-13', '-9']\n",
      "   ['-1', '-13', '-9']\n",
      "   ['-1', '-10', '-11', '-13', '-6', '-9']\n",
      "\n",
      "Ground Truth Sets:\n",
      "\n",
      "Ground Truth Sets:\n",
      "   ['-1', '-9']\n",
      "   ['-10', '-9']\n",
      "   ['-1', '-13', '-9']\n",
      "   ['-10', '-11', '-9']\n",
      "   ['-1', '-10', '-11', '-13', '-6', '-9']\n",
      "\n",
      "=== Jaccard Distance ===\n",
      "Jaccard Distance (Predicted vs Ground Truth): 0.333333\n",
      "Predicted map's loss: -847.641485\n",
      "Ground truth's loss: -492.865902\n",
      "\n",
      "================= Processing folder 0005 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restart 1/2: 100%|███████████████████████████| 50/50 [03:05<00:00,  3.70s/it, Best=-305.853, Curr=-305.853, Temp=0.778]\n",
      "Restart 2/2: 100%|███████████████████████████| 50/50 [03:13<00:00,  3.88s/it, Best=-304.467, Curr=-304.467, Temp=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST MAP for 0005 ===\n",
      "Active potencies (multi-type):\n",
      "   {-14,-15}\n",
      "   {-14,-2}\n",
      "   {-3,-4}\n",
      "   {-14,-2,-3,-4}\n",
      "   {-10,-14,-15,-2,-3,-4}\n",
      "Singletons (always active):\n",
      "   {-10}\n",
      "   {-14}\n",
      "   {-15}\n",
      "   {-2}\n",
      "   {-3}\n",
      "   {-4}\n",
      "\n",
      "Edges:\n",
      "  {-14,-15} -> {-14}\n",
      "  {-14,-15} -> {-15}\n",
      "  {-14,-2} -> {-14}\n",
      "  {-14,-2} -> {-2}\n",
      "  {-3,-4} -> {-3}\n",
      "  {-3,-4} -> {-4}\n",
      "  {-14,-2,-3,-4} -> {-14}\n",
      "  {-14,-2,-3,-4} -> {-2}\n",
      "  {-14,-2,-3,-4} -> {-4}\n",
      "  {-14,-2,-3,-4} -> {-14,-2}\n",
      "  {-14,-2,-3,-4} -> {-3,-4}\n",
      "  {-10,-14,-15,-2,-3,-4} -> {-10}\n",
      "  {-10,-14,-15,-2,-3,-4} -> {-14}\n",
      "  {-10,-14,-15,-2,-3,-4} -> {-15}\n",
      "  {-10,-14,-15,-2,-3,-4} -> {-2}\n",
      "  {-10,-14,-15,-2,-3,-4} -> {-14,-15}\n",
      "  {-10,-14,-15,-2,-3,-4} -> {-14,-2}\n",
      "  {-10,-14,-15,-2,-3,-4} -> {-3,-4}\n",
      "  {-10,-14,-15,-2,-3,-4} -> {-14,-2,-3,-4}\n",
      "\n",
      "Scores:\n",
      "  log posterior: -304.467205\n",
      "  Tree 1 log P(T|F*): -93.528862\n",
      "  Tree 2 log P(T|F*): -63.038765\n",
      "  Tree 3 log P(T|F*): -31.791794\n",
      "  Tree 4 log P(T|F*): -57.468047\n",
      "  Tree 5 log P(T|F*): -12.143465\n",
      "\n",
      "=== Ground Truth Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -93.528862\n",
      "Tree 2: log P(T|F) = -63.038765\n",
      "Tree 3: log P(T|F) = -31.791794\n",
      "Tree 4: log P(T|F) = -57.468047\n",
      "Tree 5: log P(T|F) = -12.143465\n",
      "Total log-likelihood = -257.970934\n",
      "\n",
      "Predicted Sets:\n",
      "\n",
      "Predicted Sets:\n",
      "   ['-14', '-15']\n",
      "   ['-14', '-2']\n",
      "   ['-3', '-4']\n",
      "   ['-14', '-2', '-3', '-4']\n",
      "   ['-10', '-14', '-15', '-2', '-3', '-4']\n",
      "\n",
      "Ground Truth Sets:\n",
      "\n",
      "Ground Truth Sets:\n",
      "   ['-14', '-15']\n",
      "   ['-14', '-2']\n",
      "   ['-3', '-4']\n",
      "   ['-14', '-2', '-3', '-4']\n",
      "   ['-10', '-14', '-15', '-2', '-3', '-4']\n",
      "\n",
      "=== Jaccard Distance ===\n",
      "Jaccard Distance (Predicted vs Ground Truth): 0.000000\n",
      "Predicted map's loss: -304.467205\n",
      "Ground truth's loss: -257.970934\n",
      "\n",
      "================= Processing folder 0006 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restart 1/2: 100%|███████████████████████████| 50/50 [03:27<00:00,  4.15s/it, Best=-759.254, Curr=-759.254, Temp=0.778]\n",
      "Restart 2/2: 100%|███████████████████████████| 50/50 [02:46<00:00,  3.33s/it, Best=-666.499, Curr=-666.499, Temp=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST MAP for 0006 ===\n",
      "Active potencies (multi-type):\n",
      "   {-13,-14}\n",
      "   {-4,-6}\n",
      "   {-10,-13,-14}\n",
      "   {-10,-14,-6,-8}\n",
      "   {-10,-13,-14,-4,-6,-8}\n",
      "Singletons (always active):\n",
      "   {-10}\n",
      "   {-13}\n",
      "   {-14}\n",
      "   {-4}\n",
      "   {-6}\n",
      "   {-8}\n",
      "\n",
      "Edges:\n",
      "  {-13,-14} -> {-13}\n",
      "  {-13,-14} -> {-14}\n",
      "  {-4,-6} -> {-4}\n",
      "  {-4,-6} -> {-6}\n",
      "  {-10,-13,-14} -> {-10}\n",
      "  {-10,-13,-14} -> {-13}\n",
      "  {-10,-13,-14} -> {-13,-14}\n",
      "  {-10,-13,-14,-4,-6,-8} -> {-10}\n",
      "  {-10,-13,-14,-4,-6,-8} -> {-13}\n",
      "  {-10,-13,-14,-4,-6,-8} -> {-4}\n",
      "  {-10,-13,-14,-4,-6,-8} -> {-6}\n",
      "  {-10,-13,-14,-4,-6,-8} -> {-8}\n",
      "  {-10,-13,-14,-4,-6,-8} -> {-13,-14}\n",
      "  {-10,-13,-14,-4,-6,-8} -> {-4,-6}\n",
      "  {-10,-13,-14,-4,-6,-8} -> {-10,-13,-14}\n",
      "\n",
      "Scores:\n",
      "  log posterior: -666.498987\n",
      "  Tree 1 log P(T|F*): -162.150781\n",
      "  Tree 2 log P(T|F*): -162.354550\n",
      "  Tree 3 log P(T|F*): -126.098293\n",
      "  Tree 4 log P(T|F*): -44.439791\n",
      "  Tree 5 log P(T|F*): -130.504478\n",
      "\n",
      "=== Ground Truth Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -97.407073\n",
      "Tree 2: log P(T|F) = -80.653811\n",
      "Tree 3: log P(T|F) = -80.020889\n",
      "Tree 4: log P(T|F) = -17.641414\n",
      "Tree 5: log P(T|F) = -56.465385\n",
      "Total log-likelihood = -332.188572\n",
      "\n",
      "Predicted Sets:\n",
      "\n",
      "Predicted Sets:\n",
      "   ['-13', '-14']\n",
      "   ['-4', '-6']\n",
      "   ['-10', '-13', '-14']\n",
      "   ['-10', '-14', '-6', '-8']\n",
      "   ['-10', '-13', '-14', '-4', '-6', '-8']\n",
      "\n",
      "Ground Truth Sets:\n",
      "\n",
      "Ground Truth Sets:\n",
      "   ['-13', '-14']\n",
      "   ['-4', '-6']\n",
      "   ['-10', '-13', '-14']\n",
      "   ['-14', '-4', '-6', '-8']\n",
      "   ['-10', '-13', '-14', '-4', '-6', '-8']\n",
      "\n",
      "=== Jaccard Distance ===\n",
      "Jaccard Distance (Predicted vs Ground Truth): 0.333333\n",
      "Predicted map's loss: -666.498987\n",
      "Ground truth's loss: -332.188572\n",
      "\n",
      "================= Processing folder 0007 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restart 1/2: 100%|███████████████████████████| 50/50 [02:09<00:00,  2.59s/it, Best=-704.559, Curr=-704.559, Temp=0.778]\n",
      "Restart 2/2: 100%|███████████████████████████| 50/50 [02:33<00:00,  3.08s/it, Best=-250.468, Curr=-250.468, Temp=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST MAP for 0007 ===\n",
      "Active potencies (multi-type):\n",
      "   {-13,-15}\n",
      "   {-15,-4}\n",
      "   {-1,-13,-15}\n",
      "   {-1,-13,-15,-4}\n",
      "   {-1,-10,-11,-13,-15,-4}\n",
      "Singletons (always active):\n",
      "   {-1}\n",
      "   {-10}\n",
      "   {-11}\n",
      "   {-13}\n",
      "   {-15}\n",
      "   {-4}\n",
      "\n",
      "Edges:\n",
      "  {-13,-15} -> {-13}\n",
      "  {-13,-15} -> {-15}\n",
      "  {-15,-4} -> {-15}\n",
      "  {-15,-4} -> {-4}\n",
      "  {-1,-13,-15} -> {-1}\n",
      "  {-1,-13,-15} -> {-13}\n",
      "  {-1,-13,-15} -> {-15}\n",
      "  {-1,-13,-15} -> {-13,-15}\n",
      "  {-1,-13,-15,-4} -> {-1}\n",
      "  {-1,-13,-15,-4} -> {-15,-4}\n",
      "  {-1,-13,-15,-4} -> {-1,-13,-15}\n",
      "  {-1,-10,-11,-13,-15,-4} -> {-10}\n",
      "  {-1,-10,-11,-13,-15,-4} -> {-11}\n",
      "  {-1,-10,-11,-13,-15,-4} -> {-15}\n",
      "  {-1,-10,-11,-13,-15,-4} -> {-1,-13,-15,-4}\n",
      "\n",
      "Scores:\n",
      "  log posterior: -250.467982\n",
      "  Tree 1 log P(T|F*): -32.172463\n",
      "  Tree 2 log P(T|F*): -63.825796\n",
      "  Tree 3 log P(T|F*): -45.086634\n",
      "  Tree 4 log P(T|F*): -55.604961\n",
      "  Tree 5 log P(T|F*): -12.157604\n",
      "\n",
      "=== Ground Truth Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -32.172463\n",
      "Tree 2: log P(T|F) = -63.825796\n",
      "Tree 3: log P(T|F) = -45.086634\n",
      "Tree 4: log P(T|F) = -55.604961\n",
      "Tree 5: log P(T|F) = -12.157604\n",
      "Total log-likelihood = -208.847458\n",
      "\n",
      "Predicted Sets:\n",
      "\n",
      "Predicted Sets:\n",
      "   ['-13', '-15']\n",
      "   ['-15', '-4']\n",
      "   ['-1', '-13', '-15']\n",
      "   ['-1', '-13', '-15', '-4']\n",
      "   ['-1', '-10', '-11', '-13', '-15', '-4']\n",
      "\n",
      "Ground Truth Sets:\n",
      "\n",
      "Ground Truth Sets:\n",
      "   ['-13', '-15']\n",
      "   ['-15', '-4']\n",
      "   ['-1', '-13', '-15']\n",
      "   ['-1', '-13', '-15', '-4']\n",
      "   ['-1', '-10', '-11', '-13', '-15', '-4']\n",
      "\n",
      "=== Jaccard Distance ===\n",
      "Jaccard Distance (Predicted vs Ground Truth): 0.000000\n",
      "Predicted map's loss: -250.467982\n",
      "Ground truth's loss: -208.847458\n",
      "\n",
      "================= Processing folder 0008 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restart 1/2: 100%|███████████████████████████| 50/50 [03:01<00:00,  3.63s/it, Best=-493.453, Curr=-493.453, Temp=0.778]\n",
      "Restart 2/2: 100%|███████████████████████████| 50/50 [03:25<00:00,  4.10s/it, Best=-341.880, Curr=-343.267, Temp=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST MAP for 0008 ===\n",
      "Active potencies (multi-type):\n",
      "   {-3,-4}\n",
      "   {-3,-9}\n",
      "   {-13,-3,-4}\n",
      "   {-13,-15,-3,-4}\n",
      "   {-13,-15,-3,-4,-8,-9}\n",
      "Singletons (always active):\n",
      "   {-13}\n",
      "   {-15}\n",
      "   {-3}\n",
      "   {-4}\n",
      "   {-8}\n",
      "   {-9}\n",
      "\n",
      "Edges:\n",
      "  {-3,-4} -> {-3}\n",
      "  {-3,-4} -> {-4}\n",
      "  {-3,-9} -> {-3}\n",
      "  {-3,-9} -> {-9}\n",
      "  {-13,-3,-4} -> {-13}\n",
      "  {-13,-3,-4} -> {-3}\n",
      "  {-13,-3,-4} -> {-4}\n",
      "  {-13,-3,-4} -> {-3,-4}\n",
      "  {-13,-15,-3,-4} -> {-15}\n",
      "  {-13,-15,-3,-4} -> {-13,-3,-4}\n",
      "  {-13,-15,-3,-4,-8,-9} -> {-4}\n",
      "  {-13,-15,-3,-4,-8,-9} -> {-8}\n",
      "  {-13,-15,-3,-4,-8,-9} -> {-9}\n",
      "  {-13,-15,-3,-4,-8,-9} -> {-3,-4}\n",
      "  {-13,-15,-3,-4,-8,-9} -> {-3,-9}\n",
      "  {-13,-15,-3,-4,-8,-9} -> {-13,-15,-3,-4}\n",
      "\n",
      "Scores:\n",
      "  log posterior: -341.880285\n",
      "  Tree 1 log P(T|F*): -66.533296\n",
      "  Tree 2 log P(T|F*): -73.811254\n",
      "  Tree 3 log P(T|F*): -27.345112\n",
      "  Tree 4 log P(T|F*): -91.178317\n",
      "  Tree 5 log P(T|F*): -40.228631\n",
      "\n",
      "=== Ground Truth Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -73.279239\n",
      "Tree 2: log P(T|F) = -66.906091\n",
      "Tree 3: log P(T|F) = -22.631160\n",
      "Tree 4: log P(T|F) = -100.463080\n",
      "Tree 5: log P(T|F) = -40.228671\n",
      "Total log-likelihood = -303.508241\n",
      "\n",
      "Predicted Sets:\n",
      "\n",
      "Predicted Sets:\n",
      "   ['-3', '-4']\n",
      "   ['-3', '-9']\n",
      "   ['-13', '-3', '-4']\n",
      "   ['-13', '-15', '-3', '-4']\n",
      "   ['-13', '-15', '-3', '-4', '-8', '-9']\n",
      "\n",
      "Ground Truth Sets:\n",
      "\n",
      "Ground Truth Sets:\n",
      "   ['-3', '-4']\n",
      "   ['-3', '-9']\n",
      "   ['-13', '-3', '-4']\n",
      "   ['-13', '-15', '-3', '-4', '-9']\n",
      "   ['-13', '-15', '-3', '-4', '-8', '-9']\n",
      "\n",
      "=== Jaccard Distance ===\n",
      "Jaccard Distance (Predicted vs Ground Truth): 0.333333\n",
      "Predicted map's loss: -341.880285\n",
      "Ground truth's loss: -303.508241\n",
      "\n",
      "================= Processing folder 0009 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restart 1/2: 100%|███████████████████████████| 50/50 [02:57<00:00,  3.55s/it, Best=-785.452, Curr=-785.719, Temp=0.778]\n",
      "Restart 2/2: 100%|███████████████████████████| 50/50 [03:07<00:00,  3.74s/it, Best=-785.452, Curr=-836.539, Temp=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST MAP for 0009 ===\n",
      "Active potencies (multi-type):\n",
      "   {-10,-9}\n",
      "   {-15,-3}\n",
      "   {-3,-4}\n",
      "   {-3,-4,-9}\n",
      "   {-10,-15,-3,-4,-7,-9}\n",
      "Singletons (always active):\n",
      "   {-10}\n",
      "   {-15}\n",
      "   {-3}\n",
      "   {-4}\n",
      "   {-7}\n",
      "   {-9}\n",
      "\n",
      "Edges:\n",
      "  {-10,-9} -> {-10}\n",
      "  {-10,-9} -> {-9}\n",
      "  {-15,-3} -> {-15}\n",
      "  {-3,-4} -> {-4}\n",
      "  {-3,-4,-9} -> {-3}\n",
      "  {-3,-4,-9} -> {-9}\n",
      "  {-3,-4,-9} -> {-3,-4}\n",
      "  {-10,-15,-3,-4,-7,-9} -> {-10}\n",
      "  {-10,-15,-3,-4,-7,-9} -> {-3}\n",
      "  {-10,-15,-3,-4,-7,-9} -> {-4}\n",
      "  {-10,-15,-3,-4,-7,-9} -> {-7}\n",
      "  {-10,-15,-3,-4,-7,-9} -> {-9}\n",
      "  {-10,-15,-3,-4,-7,-9} -> {-10,-9}\n",
      "  {-10,-15,-3,-4,-7,-9} -> {-15,-3}\n",
      "  {-10,-15,-3,-4,-7,-9} -> {-3,-4,-9}\n",
      "\n",
      "Scores:\n",
      "  log posterior: -785.452044\n",
      "  Tree 1 log P(T|F*): -199.341157\n",
      "  Tree 2 log P(T|F*): -132.985647\n",
      "  Tree 3 log P(T|F*): -124.939255\n",
      "  Tree 4 log P(T|F*): -170.301677\n",
      "  Tree 5 log P(T|F*): -117.379500\n",
      "\n",
      "=== Ground Truth Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -165.443611\n",
      "Tree 2: log P(T|F) = -55.460988\n",
      "Tree 3: log P(T|F) = -69.890954\n",
      "Tree 4: log P(T|F) = -79.750365\n",
      "Tree 5: log P(T|F) = -52.191015\n",
      "Total log-likelihood = -422.736931\n",
      "\n",
      "Predicted Sets:\n",
      "\n",
      "Predicted Sets:\n",
      "   ['-10', '-9']\n",
      "   ['-15', '-3']\n",
      "   ['-3', '-4']\n",
      "   ['-3', '-4', '-9']\n",
      "   ['-10', '-15', '-3', '-4', '-7', '-9']\n",
      "\n",
      "Ground Truth Sets:\n",
      "\n",
      "Ground Truth Sets:\n",
      "   ['-10', '-9']\n",
      "   ['-3', '-4']\n",
      "   ['-3', '-4', '-9']\n",
      "   ['-10', '-15', '-3', '-4', '-9']\n",
      "   ['-10', '-15', '-3', '-4', '-7', '-9']\n",
      "\n",
      "=== Jaccard Distance ===\n",
      "Jaccard Distance (Predicted vs Ground Truth): 0.333333\n",
      "Predicted map's loss: -785.452044\n",
      "Ground truth's loss: -422.736931\n",
      "\n",
      "================= Processing folder 0010 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restart 1/2: 100%|███████████████████████████| 50/50 [02:48<00:00,  3.37s/it, Best=-274.417, Curr=-274.417, Temp=0.778]\n",
      "Restart 2/2: 100%|███████████████████████████| 50/50 [02:57<00:00,  3.56s/it, Best=-271.645, Curr=-273.031, Temp=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST MAP for 0010 ===\n",
      "Active potencies (multi-type):\n",
      "   {-1,-4}\n",
      "   {-13,-14}\n",
      "   {-4,-6}\n",
      "   {-1,-14,-4}\n",
      "   {-1,-13,-14,-4,-6,-7}\n",
      "Singletons (always active):\n",
      "   {-1}\n",
      "   {-13}\n",
      "   {-14}\n",
      "   {-4}\n",
      "   {-6}\n",
      "   {-7}\n",
      "\n",
      "Edges:\n",
      "  {-1,-4} -> {-1}\n",
      "  {-1,-4} -> {-4}\n",
      "  {-13,-14} -> {-13}\n",
      "  {-13,-14} -> {-14}\n",
      "  {-4,-6} -> {-4}\n",
      "  {-4,-6} -> {-6}\n",
      "  {-1,-14,-4} -> {-1}\n",
      "  {-1,-14,-4} -> {-14}\n",
      "  {-1,-14,-4} -> {-1,-4}\n",
      "  {-1,-13,-14,-4,-6,-7} -> {-1}\n",
      "  {-1,-13,-14,-4,-6,-7} -> {-7}\n",
      "  {-1,-13,-14,-4,-6,-7} -> {-13,-14}\n",
      "  {-1,-13,-14,-4,-6,-7} -> {-4,-6}\n",
      "  {-1,-13,-14,-4,-6,-7} -> {-1,-14,-4}\n",
      "\n",
      "Scores:\n",
      "  log posterior: -271.644571\n",
      "  Tree 1 log P(T|F*): -22.823949\n",
      "  Tree 2 log P(T|F*): -65.339998\n",
      "  Tree 3 log P(T|F*): -75.682413\n",
      "  Tree 4 log P(T|F*): -40.925861\n",
      "  Tree 5 log P(T|F*): -27.753837\n",
      "\n",
      "=== Ground Truth Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -22.823949\n",
      "Tree 2: log P(T|F) = -65.339998\n",
      "Tree 3: log P(T|F) = -75.682413\n",
      "Tree 4: log P(T|F) = -40.925861\n",
      "Tree 5: log P(T|F) = -27.753837\n",
      "Total log-likelihood = -232.526058\n",
      "\n",
      "Predicted Sets:\n",
      "\n",
      "Predicted Sets:\n",
      "   ['-1', '-4']\n",
      "   ['-13', '-14']\n",
      "   ['-4', '-6']\n",
      "   ['-1', '-14', '-4']\n",
      "   ['-1', '-13', '-14', '-4', '-6', '-7']\n",
      "\n",
      "Ground Truth Sets:\n",
      "\n",
      "Ground Truth Sets:\n",
      "   ['-1', '-4']\n",
      "   ['-13', '-14']\n",
      "   ['-4', '-6']\n",
      "   ['-1', '-14', '-4']\n",
      "   ['-1', '-13', '-14', '-4', '-6', '-7']\n",
      "\n",
      "=== Jaccard Distance ===\n",
      "Jaccard Distance (Predicted vs Ground Truth): 0.000000\n",
      "Predicted map's loss: -271.644571\n",
      "Ground truth's loss: -232.526058\n",
      "\n",
      "================= Processing folder 0011 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restart 1/2: 100%|███████████████████████████| 50/50 [02:28<00:00,  2.98s/it, Best=-518.077, Curr=-518.077, Temp=0.778]\n",
      "Restart 2/2: 100%|███████████████████████████| 50/50 [03:25<00:00,  4.11s/it, Best=-273.974, Curr=-273.974, Temp=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BEST MAP for 0011 ===\n",
      "Active potencies (multi-type):\n",
      "   {-1,-12}\n",
      "   {-1,-2}\n",
      "   {-5,-6}\n",
      "   {-1,-12,-4}\n",
      "   {-1,-12,-2,-4,-5,-6}\n",
      "Singletons (always active):\n",
      "   {-1}\n",
      "   {-12}\n",
      "   {-2}\n",
      "   {-4}\n",
      "   {-5}\n",
      "   {-6}\n",
      "\n",
      "Edges:\n",
      "  {-1,-12} -> {-1}\n",
      "  {-1,-12} -> {-12}\n",
      "  {-1,-2} -> {-1}\n",
      "  {-1,-2} -> {-2}\n",
      "  {-5,-6} -> {-5}\n",
      "  {-5,-6} -> {-6}\n",
      "  {-1,-12,-4} -> {-12}\n",
      "  {-1,-12,-4} -> {-4}\n",
      "  {-1,-12,-4} -> {-1,-12}\n",
      "  {-1,-12,-2,-4,-5,-6} -> {-2}\n",
      "  {-1,-12,-2,-4,-5,-6} -> {-1,-2}\n",
      "  {-1,-12,-2,-4,-5,-6} -> {-5,-6}\n",
      "  {-1,-12,-2,-4,-5,-6} -> {-1,-12,-4}\n",
      "\n",
      "Scores:\n",
      "  log posterior: -273.974094\n",
      "  Tree 1 log P(T|F*): -22.614459\n",
      "  Tree 2 log P(T|F*): -56.745073\n",
      "  Tree 3 log P(T|F*): -84.474779\n",
      "  Tree 4 log P(T|F*): -27.596315\n",
      "  Tree 5 log P(T|F*): -44.811250\n",
      "\n",
      "=== Ground Truth Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -22.614459\n",
      "Tree 2: log P(T|F) = -56.745073\n",
      "Tree 3: log P(T|F) = -84.474779\n",
      "Tree 4: log P(T|F) = -27.596315\n",
      "Tree 5: log P(T|F) = -44.811250\n",
      "Total log-likelihood = -236.241876\n",
      "\n",
      "Predicted Sets:\n",
      "\n",
      "Predicted Sets:\n",
      "   ['-1', '-12']\n",
      "   ['-1', '-2']\n",
      "   ['-5', '-6']\n",
      "   ['-1', '-12', '-4']\n",
      "   ['-1', '-12', '-2', '-4', '-5', '-6']\n",
      "\n",
      "Ground Truth Sets:\n",
      "\n",
      "Ground Truth Sets:\n",
      "   ['-1', '-12']\n",
      "   ['-1', '-2']\n",
      "   ['-5', '-6']\n",
      "   ['-1', '-12', '-4']\n",
      "   ['-1', '-12', '-2', '-4', '-5', '-6']\n",
      "\n",
      "=== Jaccard Distance ===\n",
      "Jaccard Distance (Predicted vs Ground Truth): 0.000000\n",
      "Predicted map's loss: -273.974094\n",
      "Ground truth's loss: -236.241876\n",
      "\n",
      "================= Summary Table =================\n",
      "Folder     Jaccard Dist    GT Loss         Pred Loss      \n",
      "0002       0.571429        -432.065708     -580.868987    \n",
      "0003       0.000000        -311.549095     -355.272778    \n",
      "0004       0.333333        -492.865902     -847.641485    \n",
      "0005       0.000000        -257.970934     -304.467205    \n",
      "0006       0.333333        -332.188572     -666.498987    \n",
      "0007       0.000000        -208.847458     -250.467982    \n",
      "0008       0.333333        -303.508241     -341.880285    \n",
      "0009       0.333333        -422.736931     -785.452044    \n",
      "0010       0.000000        -232.526058     -271.644571    \n",
      "0011       0.000000        -236.241876     -273.974094    \n",
      "\n",
      "Summary saved to summary_results.csv\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "MAP structure search for Carta-CDMIP model.\n",
    "\n",
    "Score(F) = log P(F) + sum_T log P(T|F),\n",
    "where P(T|F) = sum_{labelings} B(O+1, D+1), with per-node counts:\n",
    "  obs = |L ∩ B(v)|, miss = |L \\ B(v)|, and B(·) is Beta function.\n",
    "p ~ Beta(1,1) is integrated out exactly.\n",
    "\n",
    "- Newick parser (no external deps)\n",
    "- DP over labelings with (O,D) sparse tables\n",
    "- Priors: fixed-k (uniform over potency sets) OR Bernoulli(pi_P); edges Bernoulli(rho)\n",
    "- Stochastic hill-climb + simulated annealing over F=(Z,A)\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Tuple, List, Optional, Set, FrozenSet\n",
    "\n",
    "# ----------------------------\n",
    "# Tree structures and Newick\n",
    "# ----------------------------\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        self.name: Optional[str] = name\n",
    "        self.children: List[\"TreeNode\"] = []\n",
    "        self.parent: Optional[\"TreeNode\"] = None\n",
    "\n",
    "    def is_leaf(self): return len(self.children) == 0\n",
    "    def add_child(self, child: \"TreeNode\"):\n",
    "        self.children.append(child); child.parent = self\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Leaf({self.name})\" if self.is_leaf() else f\"Node({self.name}, k={len(self.children)})\"\n",
    "\n",
    "\n",
    "# def parse_newick(newick: str) -> TreeNode:\n",
    "#     s = newick.strip()\n",
    "#     if not s.endswith(\";\"): raise ValueError(\"Newick must end with ';'\")\n",
    "#     s = s[:-1]; i = 0\n",
    "#     def parse() -> TreeNode:\n",
    "#         nonlocal i, s\n",
    "#         if i >= len(s): raise ValueError(\"Unexpected end\")\n",
    "#         if s[i] == '(':\n",
    "#             i += 1\n",
    "#             node = TreeNode()\n",
    "#             while True:\n",
    "#                 node.add_child(parse())\n",
    "#                 if i >= len(s): raise ValueError(\"Unbalanced\")\n",
    "#                 if s[i] == ',':\n",
    "#                     i += 1; continue\n",
    "#                 elif s[i] == ')':\n",
    "#                     i += 1; break\n",
    "#                 else: raise ValueError(f\"Unexpected char: {s[i]} at {i}\")\n",
    "#             j = i\n",
    "#             while j < len(s) and s[j] not in ',()': j += 1\n",
    "#             name = s[i:j].strip()\n",
    "#             if name: node.name = name\n",
    "#             i = j\n",
    "#             return node\n",
    "#         else:\n",
    "#             j = i\n",
    "#             while j < len(s) and s[j] not in ',()': j += 1\n",
    "#             name = s[i:j].strip()\n",
    "#             if not name: raise ValueError(\"Leaf without name\")\n",
    "#             i = j\n",
    "#             return TreeNode(name=name)\n",
    "#     root = parse()\n",
    "#     if i != len(s): raise ValueError(f\"Trailing characters: '{s[i:]}'\")\n",
    "#     return root\n",
    "\n",
    "def iter_edges(root: TreeNode) -> Iterable[Tuple[TreeNode, TreeNode]]:\n",
    "    \"\"\"Yield (parent, child) for every directed edge in the rooted tree.\"\"\"\n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        for child in node.children:\n",
    "            yield (node, child)\n",
    "            stack.append(child)\n",
    "\n",
    "\n",
    "def count_edges(root: TreeNode) -> int:\n",
    "    \"\"\"Count number of directed edges in tree rooted at `root`.\"\"\"\n",
    "    return sum(1 for _ in iter_edges(root))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Union-only Fitch labeling\n",
    "# -------------------------\n",
    "def assign_union_potency(root: TreeNode, leaf_type_map: Dict[str, str]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Post-order union-only labeling. Sets `node.potency` for every node (as a Python set).\n",
    "    For leaves, looks up leaf_type_map[node.name] to get the leaf cell type.\n",
    "    Returns the potency set at `root`.\n",
    "    \"\"\"\n",
    "    if root.is_leaf():\n",
    "        if root.name is None:\n",
    "            raise KeyError(\"Leaf has no .name; cannot map to leaf_type_map\")\n",
    "        if root.name not in leaf_type_map:\n",
    "            raise KeyError(f\"Leaf name '{root.name}' not found in leaf_type_map\")\n",
    "        root.potency = {leaf_type_map[root.name]}\n",
    "        return root.potency\n",
    "\n",
    "    union_set: Set[str] = set()\n",
    "    for child in root.children:\n",
    "        child_set = assign_union_potency(child, leaf_type_map)\n",
    "        union_set |= child_set\n",
    "    root.potency = union_set\n",
    "    return root.potency\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Per-tree transition counts\n",
    "# -------------------------\n",
    "def per_tree_transition_counts(root: TreeNode) -> Counter:\n",
    "    \"\"\"\n",
    "    Count transitions (parent_set -> child_set) for all direct edges in the tree,\n",
    "    excluding edges where parent.potency == child.potency.\n",
    "    Returns Counter with keys (frozenset_parent, frozenset_child) -> count (int).\n",
    "    \"\"\"\n",
    "    C = Counter()\n",
    "    for (u, v) in iter_edges(root):\n",
    "        su = frozenset(u.potency if u.potency is not None else set())\n",
    "        sv = frozenset(v.potency if v.potency is not None else set())\n",
    "        if su != sv:\n",
    "            C[(su, sv)] += 1\n",
    "    return C\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Aggregation + top-k picking\n",
    "# -------------------------\n",
    "def init_progenitors_union_fitch(\n",
    "    S: List[str],\n",
    "    trees: List[\"TreeNode\"],\n",
    "    leaf_type_maps: List[Dict[str, str]],\n",
    "    k: int,\n",
    ") -> Tuple[Dict[Tuple[frozenset, frozenset], float], Set[frozenset]]:\n",
    "    \"\"\"\n",
    "    Run union-Fitch on each tree, compute normalized transition counts per tree (only real transitions),\n",
    "    aggregate across trees, compute row-sums and return:\n",
    "      - aggregated_transitions: dict ( (frozenset_i, frozenset_j) -> float )\n",
    "      - Z_init: set of frozensets including:\n",
    "          * ROOT potency (all leaf types)\n",
    "          * all singleton potencies (each leaf type)\n",
    "          * top (k-1) progenitor states (size >= 2, excluding ROOT)\n",
    "    \"\"\"\n",
    "    if len(trees) != len(leaf_type_maps):\n",
    "        raise ValueError(\"Provide exactly one leaf_type_map per tree (same order).\")\n",
    "\n",
    "    ROOT = frozenset(S)  # absolute root potency (all leaf types)\n",
    "    aggregated_transitions: Dict[Tuple[frozenset, frozenset], float] = defaultdict(float)\n",
    "    row_sum: Dict[frozenset, float] = defaultdict(float)\n",
    "\n",
    "    for tree, ltm in zip(trees, leaf_type_maps):\n",
    "        # Assign potencies\n",
    "        assign_union_potency(tree, ltm)\n",
    "        # Count only real transitions\n",
    "        C_T = per_tree_transition_counts(tree)\n",
    "        T = sum(C_T.values())  # number of actual transitions\n",
    "        if T == 0:\n",
    "            continue\n",
    "        # Normalize and aggregate\n",
    "        for (i_set, j_set), cnt in C_T.items():\n",
    "            incr = cnt / T\n",
    "            aggregated_transitions[(i_set, j_set)] += incr\n",
    "            row_sum[i_set] += incr\n",
    "\n",
    "    # Start Z_init with root and all singletons\n",
    "    Z_init: Set[frozenset] = {ROOT} | {frozenset([cell]) for cell in S}\n",
    "\n",
    "    # Candidates: only potency sets of size >= 2 (excluding ROOT)\n",
    "    candidates = [ps for ps in row_sum.keys() if ps != ROOT and len(ps) >= 2]\n",
    "\n",
    "    # Sort candidates by score: row_sum desc, then size desc, then lexicographic\n",
    "    candidates.sort(key=lambda ps: (-row_sum[ps], -len(ps), tuple(sorted(ps))))\n",
    "\n",
    "    # Take top (k-1) progenitors (excluding root, which is already in)\n",
    "    top_progenitors = candidates[:max(0, k - 1)]\n",
    "\n",
    "    # Add these progenitors to Z_init\n",
    "    Z_init |= set(top_progenitors)\n",
    "\n",
    "    return dict(aggregated_transitions), Z_init\n",
    "\n",
    "def parse_newick(newick: str) -> TreeNode:\n",
    "    # Helper: strip branch length and numeric-only labels\n",
    "    def _clean_label(tok: str) -> str:\n",
    "        # remove branch length: keep part before first ':'\n",
    "        tok = tok.split(\":\", 1)[0].strip()\n",
    "        # drop pure numeric internal labels like \"357\"\n",
    "        if tok and tok.replace(\".\", \"\", 1).isdigit():\n",
    "            return \"\"\n",
    "        return tok\n",
    "\n",
    "    s = newick.strip()\n",
    "    if not s.endswith(\";\"): raise ValueError(\"Newick must end with ';'\")\n",
    "    s = s[:-1]; i = 0\n",
    "\n",
    "    def parse() -> TreeNode:\n",
    "        nonlocal i, s\n",
    "        if i >= len(s): raise ValueError(\"Unexpected end\")\n",
    "        if s[i] == '(':\n",
    "            i += 1\n",
    "            node = TreeNode()\n",
    "            while True:\n",
    "                node.add_child(parse())\n",
    "                if i >= len(s): raise ValueError(\"Unbalanced\")\n",
    "                if s[i] == ',':\n",
    "                    i += 1; continue\n",
    "                elif s[i] == ')':\n",
    "                    i += 1; break\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected char: {s[i]} at {i}\")\n",
    "\n",
    "            # optional internal node label (may include branch length)\n",
    "            j = i\n",
    "            while j < len(s) and s[j] not in ',()': j += 1\n",
    "            name_raw = s[i:j].strip()\n",
    "            name = _clean_label(name_raw)\n",
    "            if name:  # keep non-empty, non-numeric labels only\n",
    "                node.name = name\n",
    "            i = j\n",
    "            return node\n",
    "        else:\n",
    "            # leaf label (may include branch length)\n",
    "            j = i\n",
    "            while j < len(s) and s[j] not in ',()': j += 1\n",
    "            name_raw = s[i:j].strip()\n",
    "            name = _clean_label(name_raw)\n",
    "            if not name:\n",
    "                raise ValueError(\"Leaf without name\")\n",
    "            i = j\n",
    "            return TreeNode(name=name)\n",
    "\n",
    "    root = parse()\n",
    "    if i != len(s): raise ValueError(f\"Trailing characters: '{s[i:]}'\")\n",
    "    return root\n",
    "\n",
    "def to_newick(root: TreeNode) -> str:\n",
    "    def rec(n: TreeNode) -> str:\n",
    "        if n.is_leaf(): return n.name or \"\"\n",
    "        return f\"({','.join(rec(c) for c in n.children)}){n.name or ''}\"\n",
    "    return rec(root) + \";\"\n",
    "\n",
    "\n",
    "def read_newick_file(path: str) -> TreeNode:\n",
    "    with open(path, \"r\") as f: s = f.read().strip()\n",
    "    return parse_newick(s)\n",
    "\n",
    "def write_newick_file(path: str, root: TreeNode):\n",
    "    with open(path, \"w\") as f: f.write(to_newick(root) + \"\\n\")\n",
    "\n",
    "def random_tree_newick(n_leaves: int, leaf_prefix=\"L\") -> Tuple[TreeNode, List[str]]:\n",
    "    leaves = [TreeNode(f\"{leaf_prefix}{i+1}\") for i in range(n_leaves)]\n",
    "    nodes = leaves[:]\n",
    "    while len(nodes) > 1:\n",
    "        k = 2 if len(nodes) < 4 else random.choice([2,2,2,3])\n",
    "        k = min(k, len(nodes))\n",
    "        picks = random.sample(nodes, k)\n",
    "        for p in picks: nodes.remove(p)\n",
    "        parent = TreeNode()\n",
    "        for p in picks: parent.add_child(p)\n",
    "        nodes.append(parent)\n",
    "    return nodes[0], [l.name for l in leaves]\n",
    "\n",
    "def collect_leaf_names(root: TreeNode) -> List[str]:\n",
    "    out=[]\n",
    "    def dfs(v):\n",
    "        if v.is_leaf(): out.append(v.name)\n",
    "        else:\n",
    "            for c in v.children: dfs(c)\n",
    "    dfs(root); return out\n",
    "\n",
    "# ----------------------------\n",
    "# Potency universe and structure\n",
    "# ----------------------------\n",
    "\n",
    "def all_nonempty_subsets(S: List[str], max_size: Optional[int]=None) -> List[FrozenSet[str]]:\n",
    "    # Generate all non-empty subsets of the label universe S, optionally capped by max_size.\n",
    "    # Each subset is returned as a frozenset so it can be used as a dict/set key elsewhere.\n",
    "    # Used by: build_Z_active (to enumerate candidate potency sets).\n",
    "    R=len(S); max_k = R if max_size is None else min(max_size, R)\n",
    "    res=[]\n",
    "    # k = subset size (from 1 up to max_k). We exclude k=0 (the empty set).\n",
    "    for k in range(1, max_k+1):\n",
    "        # itertools.combinations yields all size-k subsets of S (as tuples).\n",
    "        # We wrap them in frozenset to make them hashable and order-invariant.\n",
    "        for comb in itertools.combinations(S, k): res.append(frozenset(comb))\n",
    "    return res\n",
    "\n",
    "def singletons(S: List[str]) -> Set[FrozenSet[str]]:\n",
    "    # Return the set of all singleton subsets {t} for each type t in S.\n",
    "    # Singletons represent terminal/atomic potencies (always included in Z).\n",
    "    # Used by: build_Z_active (baseline active nodes).\n",
    "    return {frozenset([t]) for t in S}\n",
    "\n",
    "def build_Z_active(S: List[str], fixed_k: Optional[int], max_potency_size: Optional[int], seed=0) -> Set[FrozenSet[str]]:\n",
    "    # Construct the initial active potency set Z:\n",
    "    # - Always include all singletons {t} for t in S.\n",
    "    # - For multi-type potencies (|P| >= 2), either:\n",
    "    #     * If fixed_k is not None: uniformly sample exactly fixed_k of them.\n",
    "    #     * Else: include ALL multi-type potencies up to max_potency_size.\n",
    "    # This \"Z\" forms the node set of the potency DAG used by Structure.\n",
    "    # Used by: map_search (to initialize candidate structures).\n",
    "    rng = random.Random(seed)\n",
    "    P_all = all_nonempty_subsets(S, max_potency_size)   # all non-empty subsets up to size cap\n",
    "    singles = singletons(S)                             # all {t}\n",
    "    multis = [P for P in P_all if len(P)>=2]            # only multi-type potencies (size >= 2)\n",
    "    Z = set(singles)                                    # start with all singletons\n",
    "    if fixed_k is not None:\n",
    "        if fixed_k > len(multis):\n",
    "            raise ValueError(\"fixed_k too large\")\n",
    "        root = frozenset(S)\n",
    "        Z.add(root)\n",
    "        remaining_multis = [P for P in multis if P != root]\n",
    "        Z.update(rng.sample(remaining_multis, fixed_k - 1))  # pick k-1 more\n",
    "    else:\n",
    "        Z.update(multis)\n",
    "    return Z\n",
    "\n",
    "def admissible_edge(P: FrozenSet[str], Q: FrozenSet[str], unit_drop: bool) -> bool:\n",
    "    # Decide if an edge P -> Q is allowed in the potency DAG.\n",
    "    # Constraints:\n",
    "    #  - Q must be a proper subset of P (monotone decreasing).\n",
    "    #  - If unit_drop=True, exactly one element must be dropped: |P \\ Q| == 1.\n",
    "    #  - No self-loops.\n",
    "    # Used by: build_edges (to enumerate valid edges).\n",
    "    if Q == P: return False\n",
    "    if not Q.issubset(P): return False\n",
    "    if len(Q) >= len(P): return False\n",
    "    if unit_drop and len(P - Q) != 1: return False\n",
    "    return True\n",
    "\n",
    "def build_edges(Z_active: Set[FrozenSet[str]], forbid_fn=None, unit_drop=True) -> Dict[Tuple[FrozenSet[str],FrozenSet[str]], int]:\n",
    "    # Build the adjacency dictionary A over the active potency set Z_active.\n",
    "    # For every admissible pair (P, Q), create an edge indicator A[(P, Q)] = 1.\n",
    "    # Optionally skip edges if forbid_fn(P,Q) returns True.\n",
    "    # The unit_drop flag enforces |P \\ Q| == 1 if True; otherwise any strict subset is allowed.\n",
    "    # Used by: map_search (to initialize a connected structure so labels can \"flow\" down).\n",
    "    A={}\n",
    "    for P in Z_active:\n",
    "        for Q in Z_active:\n",
    "            if not admissible_edge(P,Q,unit_drop): continue\n",
    "            if forbid_fn and forbid_fn(P,Q): continue\n",
    "            A[(P,Q)] = 1\n",
    "    return A\n",
    "\n",
    "def build_mid_sized_connected_dag(Z_active, keep_prob=0.3, unit_drop=False, rng=None):\n",
    "    \"\"\"\n",
    "    Build a valid mid-density DAG:\n",
    "      • Uses only admissible edges\n",
    "      • Guarantees connectivity from the root node (frozenset of all singletons)\n",
    "      • Keeps density moderate, controlled by `keep_prob`\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = random.Random()\n",
    "\n",
    "    # --- Identify root node (the potency containing all singletons) ---\n",
    "    root = frozenset().union(*Z_active)  # union of all labels gives the full set\n",
    "    if root not in Z_active:\n",
    "        raise ValueError(\"Root potency (all singletons) not present in Z_active.\")\n",
    "\n",
    "    nodes = list(Z_active)\n",
    "\n",
    "    # --- Step 1: Build full admissible edge set ---\n",
    "    full_edges = {\n",
    "        (P, Q): 1\n",
    "        for P in Z_active\n",
    "        for Q in Z_active\n",
    "        if P != Q and admissible_edge(P, Q, unit_drop)\n",
    "    }\n",
    "\n",
    "    # --- Step 2: Start with a spanning tree to guarantee connectivity ---\n",
    "    A = {}\n",
    "    visited = {root}\n",
    "    to_visit = set(nodes) - {root}\n",
    "\n",
    "    while to_visit:\n",
    "        # pick a node already in the tree\n",
    "        parent = rng.choice(list(visited))\n",
    "\n",
    "        # find valid edges from parent to some unvisited node\n",
    "        candidates = [(parent, q) for q in to_visit if (parent, q) in full_edges]\n",
    "\n",
    "        if not candidates:\n",
    "            # fallback: pick any edge between visited and unvisited nodes\n",
    "            candidates = [\n",
    "                (p, q) for p in visited for q in to_visit if (p, q) in full_edges\n",
    "            ]\n",
    "\n",
    "        edge = rng.choice(candidates)\n",
    "        A[edge] = 1\n",
    "        visited.add(edge[1])\n",
    "        to_visit.remove(edge[1])\n",
    "\n",
    "    # --- Step 3: Add extra edges randomly to reach desired density ---\n",
    "    for edge in full_edges:\n",
    "        if edge in A:\n",
    "            continue\n",
    "        if rng.random() < keep_prob:\n",
    "            A[edge] = 1\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "def transitive_closure(labels: List[FrozenSet[str]], A: Dict[Tuple[FrozenSet[str],FrozenSet[str]], int]) -> Dict[FrozenSet[str], Set[FrozenSet[str]]]:\n",
    "    # Compute reachability (transitive closure) over the directed graph (labels, A).\n",
    "    # Result: Reach[L] = set of nodes U such that there is a path L ->* U (including L itself).\n",
    "    # Implementation details:\n",
    "    #  - Build an index for labels and a boolean adjacency matrix M.\n",
    "    #  - Set M[i][i] = True (reflexive reachability).\n",
    "    #  - For each edge (P,Q) with A[(P,Q)]==1, mark M[i][j] = True.\n",
    "    #  - Floyd–Warshall-style closure: if i->k and k->j then i->j.\n",
    "    # Used by: Structure.__init__/recompute_reach (to query allowed label transitions during DP).\n",
    "    idx = {L:i for i,L in enumerate(labels)}\n",
    "    n=len(labels)\n",
    "    M=[[False]*n for _ in range(n)]\n",
    "    for i in range(n): M[i][i]=True                 # every node reaches itself\n",
    "    for (P,Q),v in A.items():\n",
    "        if v:\n",
    "            i,j=idx[P],idx[Q]; M[i][j]=True         # direct edges from A\n",
    "\n",
    "    # Triple loop closure (standard transitive closure).\n",
    "    for k in range(n):\n",
    "        Mk=M[k]\n",
    "        for i in range(n):\n",
    "            if M[i][k]:\n",
    "                Mi=M[i]\n",
    "                for j in range(n):\n",
    "                    if Mk[j]: Mi[j]=True\n",
    "\n",
    "    # Rehydrate into a dict keyed by the actual frozenset labels.\n",
    "    Reach={L:set() for L in labels}\n",
    "    for i,L in enumerate(labels):\n",
    "        for j,U in enumerate(labels):\n",
    "            if M[i][j]: Reach[L].add(U)\n",
    "    return Reach\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# DP over labelings (integrated Beta)\n",
    "# ----------------------------\n",
    "\n",
    "def compute_B_sets(root: TreeNode, leaf_to_type: Dict[str,str]) -> Dict[TreeNode, Set[str]]:\n",
    "    # Build, for every node v in the tree, the set B[v] of observed types that appear\n",
    "    # in v's subtree (union over leaves below v).\n",
    "    #\n",
    "    # Key detail for robustness (as you requested):\n",
    "    # - If a leaf's name is missing from `leaf_to_type`, we *ignore* that leaf by contributing an empty set.\n",
    "    #\n",
    "    # Used by: score_structure() before DP, which passes B_sets into dp_tree_root_table().\n",
    "    B={}\n",
    "    def post(v: TreeNode) -> Set[str]:\n",
    "        if v.is_leaf():\n",
    "            # If the leaf has a mapping, add that single type; else contribute empty set.\n",
    "            t = leaf_to_type.get(v.name)\n",
    "            # Missing mapping? Ignore this leaf by contributing an empty set.\n",
    "            B[v] = {t} if t is not None else set()\n",
    "            return B[v]\n",
    "        # Internal node: union of children's type-sets.\n",
    "        acc=set()\n",
    "        for c in v.children: acc |= post(c)\n",
    "        B[v]=acc; return acc\n",
    "    post(root); return B\n",
    "\n",
    "def beta_integral(O:int,D:int)->float:\n",
    "    # Compute the integral ∫_0^1 p^O (1-p)^D dp, which is the Beta function B(O+1, D+1).\n",
    "    # We do it in log-space using lgamma for numerical stability:\n",
    "    #   B(a,b) = Γ(a)Γ(b) / Γ(a+b)\n",
    "    # Here a = O+1, b = D+1.\n",
    "    #\n",
    "    # Used by: tree_marginal_from_root_table() to integrate out Bernoulli p ~ Beta(1,1).\n",
    "    return math.exp(math.lgamma(O+1)+math.lgamma(D+1)-math.lgamma(O+D+2))\n",
    "\n",
    "def sparse_convolve_2d(A: Dict[Tuple[int,int],float], B: Dict[Tuple[int,int],float]) -> Dict[Tuple[int,int],float]:\n",
    "    # Convolution over 2D count tables:\n",
    "    # Each dict maps (O,D) -> weight. We want:\n",
    "    #   (A * B)[(o1+o2, d1+d2)] += A[(o1,d1)] * B[(o2,d2)]\n",
    "    #\n",
    "    # This is used to combine children's DP messages (sum of counts across subtrees).\n",
    "    #\n",
    "    # Edge cases:\n",
    "    # - If one table is empty, return a copy of the other (identity for convolution).\n",
    "    if not A: return B.copy()\n",
    "    if not B: return A.copy()\n",
    "    out=defaultdict(float)\n",
    "    for (o1,d1),w1 in A.items():\n",
    "        for (o2,d2),w2 in B.items():\n",
    "            out[(o1+o2,d1+d2)] += w1*w2\n",
    "    return dict(out)\n",
    "\n",
    "def dp_tree_root_table(\n",
    "    root: TreeNode,\n",
    "    active_labels: List[FrozenSet[str]],\n",
    "    Reach: Dict[FrozenSet[str], Set[FrozenSet[str]]],\n",
    "    B_sets: Dict[TreeNode, Set[str]],\n",
    "    prune_eps: float = 0.0\n",
    ")->Dict[Tuple[int,int],float]:\n",
    "    # Dynamic program over the tree to build a sparse table at the root:\n",
    "    #   C[(O,D)] = total weight of all labelings that yield (O,D) at the root,\n",
    "    # where for a node v labeled by set L, we count:\n",
    "    #   O_local = |L ∩ B(v)|   (observed hits within v's subtree)\n",
    "    #   D_local = |L \\ B(v)|   (misses: types in L not present under v)\n",
    "    #\n",
    "    # The parent-child label constraint is enforced by `Reach`:\n",
    "    #   If parent has label P, allowed child labels are any L in Reach[P].\n",
    "    # At the root, P=None means we consider *all* active labels as possible root labels.\n",
    "    #\n",
    "    # Pruning:\n",
    "    # - If prune_eps > 0, we drop entries whose weight < prune_eps * sum(weights) at that node.\n",
    "    #\n",
    "    # Used by: score_structure(); its output goes into tree_marginal_from_root_table().\n",
    "\n",
    "    # Map label -> integer index (for memoization keys).\n",
    "    label_index={L:i for i,L in enumerate(active_labels)}\n",
    "    # Memo maps (node-id, parent-label-index-or-(-1 for None)) -> sparse table dict\n",
    "    memo: Dict[Tuple[int,int], Dict[Tuple[int,int],float]]={}\n",
    "    def nid(v:TreeNode)->int: return id(v)\n",
    "\n",
    "    def M(v:TreeNode, P: Optional[FrozenSet[str]])->Dict[Tuple[int,int],float]:\n",
    "        # Return the sparse (O,D)->weight table for subtree rooted at v,\n",
    "        # conditioned on: parent of v has label P (P may be None at the root).\n",
    "        key=(nid(v), -1 if P is None else label_index[P])\n",
    "        if key in memo: return memo[key]\n",
    "\n",
    "        if v.is_leaf():\n",
    "            # For leaves, there is no subtree below: the children \"conv\" is the identity {(0,0):1}.\n",
    "            # Note: O_local/D_local are *added* at the PARENT level (where v is processed as a child).\n",
    "            # So at the leaf node itself, we just return the neutral table.\n",
    "            memo[key] = {(0,0):1.0}; return memo[key]\n",
    "\n",
    "        # Bv is the set of observed types that appear anywhere under v (from compute_B_sets()).\n",
    "        Bv=B_sets[v]\n",
    "        out=defaultdict(float)\n",
    "\n",
    "        # Which labels can v take given its parent label P?\n",
    "        # - If P is None (we're at the root), we try all active labels.\n",
    "        # - Else we restrict to labels reachable from P according to the potency DAG closure.\n",
    "        if P is None:\n",
    "            parent_reach = active_labels\n",
    "        else:\n",
    "            parent_reach = list(Reach[P])\n",
    "\n",
    "        # Try each candidate label L for node v.\n",
    "        for L in parent_reach:\n",
    "            # Containment constraint: the observed types in v's subtree must be a subset of L,\n",
    "            # otherwise L would \"claim\" types that don't exist under v or miss types that do exist\n",
    "            # (the model enforces this monotonic consistency).\n",
    "            if not Bv.issubset(L):  # containment constraint\n",
    "                continue\n",
    "\n",
    "            # Local O/D contributions if v is labeled with L:\n",
    "            #   - Observed hits are types in both L and Bv.\n",
    "            #   - Misses are types in L that do not appear under v at all.\n",
    "            o_local=len(L & Bv); d_local=len(L - Bv)\n",
    "\n",
    "            # Recurse on children conditioned on v's label being L.\n",
    "            child_tabs=[]\n",
    "            ok=True\n",
    "            for u in v.children:\n",
    "                tab = M(u, L)\n",
    "                if not tab: ok=False; break\n",
    "                child_tabs.append(tab)\n",
    "            if not ok: continue\n",
    "\n",
    "            # Convolve the children's tables to aggregate counts across subtrees.\n",
    "            # If there are no children (shouldn't happen for non-leaf), the identity {(0,0):1.0} is used.\n",
    "            conv = child_tabs[0] if child_tabs else {(0,0):1.0}\n",
    "            for t in child_tabs[1:]:\n",
    "                conv = sparse_convolve_2d(conv, t)\n",
    "\n",
    "            # Add v's local (o_local, d_local) to every child combination.\n",
    "            for (Oc,Dc),w in conv.items():\n",
    "                out[(Oc+o_local, Dc+d_local)] += w\n",
    "\n",
    "        # Optional pruning to keep the table small (drop tiny weights).\n",
    "        if prune_eps>0 and out:\n",
    "            total=sum(out.values()); thresh=prune_eps*total\n",
    "            out={k:v for k,v in out.items() if v>=thresh}\n",
    "\n",
    "        memo[key]=dict(out); return memo[key]\n",
    "\n",
    "    # Kick off the DP from the root with P=None (meaning \"try all root labels\").\n",
    "    return M(root, None)\n",
    "\n",
    "def tree_marginal_from_root_table(C: Dict[Tuple[int,int],float])->float:\n",
    "    # Turn the root's (O,D)->weight table into a scalar probability by integrating out p:\n",
    "    #   P(T | F) = Σ_{O,D}  weight(O,D) * B(O+1, D+1)\n",
    "    # where B(·,·) is the Beta function (computed by beta_integral).\n",
    "    #\n",
    "    # Used by: score_structure() to compute per-tree likelihoods.\n",
    "    return sum(w * beta_integral(O,D) for (O,D),w in C.items())\n",
    "\n",
    "# ----------------------------\n",
    "# Priors and scoring\n",
    "# ----------------------------\n",
    "\n",
    "class Priors:\n",
    "    def __init__(self,\n",
    "                 potency_mode:str=\"fixed_k\",  # \"fixed_k\" or \"bernoulli\"\n",
    "                 fixed_k:int=2,\n",
    "                 pi_P:float=0.25,    # used if potency_mode=\"bernoulli\"\n",
    "                 rho:float=0.25):    # edge Bernoulli prob\n",
    "        # ------------------------------------------------------------\n",
    "        # Stores hyperparameters for the prior over the structure F=(Z,A)\n",
    "        #     Z: The latent assignment of “potencies” or features to nodes (the sets like {A,B,C}, {B,C,D}, etc. that you saw in the MAP output).\n",
    "        #     A: The active structure (the adjacency or edge set) consistent with those potencies — basically the graph/hypergraph that the algorithm thinks best explains the observed trees.\n",
    "        #   - potency_mode: which prior to use over active multi-type potencies Z\n",
    "        #       * \"fixed_k\": exactly k multi-type potencies are active (uniform over choices)\n",
    "        #       * \"bernoulli\": each multi-type potency is independently active with prob pi_P\n",
    "        #   - fixed_k: number of multi-type potencies when potency_mode == \"fixed_k\"\n",
    "        #   - pi_P: inclusion probability for each multi-type potency when using \"bernoulli\" mode\n",
    "        #   - rho: prior probability that any admissible edge (P->Q) exists\n",
    "        # ------------------------------------------------------------\n",
    "        self.potency_mode=potency_mode\n",
    "        self.fixed_k=fixed_k\n",
    "        self.pi_P=pi_P\n",
    "        self.rho=rho\n",
    "\n",
    "    def log_prior_Z(self, S: List[str], Z_active:Set[FrozenSet[str]])->float: #Z_active = the set of active potencies (both singletons and multis).\n",
    "        # ------------------------------------------------------------\n",
    "        # Computes log P(Z): the log prior over WHICH potencies are active.\n",
    "        #\n",
    "        # Inputs:\n",
    "        #   - S: list of all cell types (leaf types), e.g., [\"A\",\"B\",\"C\",\"D\"]\n",
    "        #   - Z_active: set of active potencies (as frozensets). Includes singletons by construction.\n",
    "        #\n",
    "        # Key ideas:\n",
    "        #   - Singletons are always considered active (terminal states), we don't penalize/score them.\n",
    "        #   - We only place a prior over multi-type potencies (size >= 2).\n",
    "        #   - Two modes:\n",
    "        #       * \"fixed_k\": valid only if exactly `fixed_k` multis are active.\n",
    "        #                    Prior is uniform over all C(M, k) choices, where M = #all possible multis.\n",
    "        #       * \"bernoulli\": each possible multi is included independently with prob pi_P.\n",
    "        #                      log prior sums log(pi_P) for included multis and log(1-pi_P) for excluded ones.\n",
    "        # Returns:\n",
    "        #   - log prior (float), or -inf if configuration violates \"fixed_k\".\n",
    "        # ------------------------------------------------------------\n",
    "        singles = singletons(S)\n",
    "        multis = [P for P in Z_active if len(P)>=2] #P is a particular potency set\n",
    "        # count available multi potencies (for fixed-k uniform)\n",
    "        all_multis = [P for P in all_nonempty_subsets(S) if len(P)>=2]\n",
    "\n",
    "        if self.potency_mode==\"fixed_k\":\n",
    "            # ------------------------------\n",
    "            # Uniform prior over all subsets of multi-type potencies with EXACTLY k elements.\n",
    "            # If the current Z_active has not exactly k multis, return -inf (outside prior support).\n",
    "            # Otherwise, log prior = -log( number of such subsets ) = -log( nCk ).\n",
    "            # ------------------------------\n",
    "            k=len(multis)\n",
    "            if k!=self.fixed_k:\n",
    "                return float(\"-inf\")\n",
    "            # uniform over all C(|all_multis|, k)\n",
    "            total = math.comb(len(all_multis), k) #this is nCk\n",
    "            return -math.log(total) if total>0 else float(\"-inf\")\n",
    "        else:\n",
    "            # ------------------------------\n",
    "            # Bernoulli prior on each multi-type potency:\n",
    "            #   P(Z) = ∏_{P in all_multis} pi_P^{I[P in Z]} (1 - pi_P)^{I[P not in Z]}\n",
    "            # We sum logs across all possible multi-type potencies (singletons ignored).\n",
    "            # ------------------------------\n",
    "            k_log=0.0\n",
    "            for P in all_multis:\n",
    "                if P in Z_active: k_log += math.log(self.pi_P)\n",
    "                else: k_log += math.log(1-self.pi_P)\n",
    "            return k_log\n",
    "\n",
    "    def log_prior_A(self, Z_active:Set[FrozenSet[str]], A:Dict[Tuple[FrozenSet[str],FrozenSet[str]],int], unit_drop=True)->float:\n",
    "        # ------------------------------------------------------------\n",
    "        # Computes log P(A | Z): the log prior over EDGE EXISTENCE between active potencies.\n",
    "        #\n",
    "        # Inputs:\n",
    "        #   - Z_active: set of active potencies (nodes in the potency DAG)\n",
    "        #   - A: adjacency dictionary mapping (P,Q) -> {0,1}, indicating whether edge P->Q is present\n",
    "        #   - unit_drop: if True, an admissible edge must drop EXACTLY one fate (|P\\Q| == 1);\n",
    "        #                otherwise any monotone subset drop (Q ⊂ P) is admissible.\n",
    "        #\n",
    "        # Prior:\n",
    "        #   - For every admissible pair (P,Q):\n",
    "        #         A_{P->Q} ~ Bernoulli(rho)\n",
    "        #     So:\n",
    "        #         log P(A|Z) = ∑_{(P,Q) admissible} [ A_{P->Q} log(rho) + (1 - A_{P->Q}) log(1 - rho) ]\n",
    "        #\n",
    "        # Notes:\n",
    "        #   - \"Admissible\" enforces graph shape constraints (subset-monotone and possibly unit-drop).\n",
    "        #   - If an edge (P,Q) is not admissible, it does not contribute to the product/sum at all.\n",
    "        # ------------------------------------------------------------\n",
    "        labels=list(Z_active)\n",
    "        # admissible set is pairs with subset monotone (and optionally unit-drop)\n",
    "        logp=0.0\n",
    "        for P in labels:\n",
    "            for Q in labels:\n",
    "                if admissible_edge(P,Q,unit_drop):\n",
    "                    # a == 1 if the edge is present in A, else 0\n",
    "                    a = 1 if A.get((P,Q),0)==1 else 0\n",
    "                    # add Bernoulli log-prob for this edge\n",
    "                    logp += math.log(self.rho) if a==1 else math.log(1-self.rho)\n",
    "        return logp\n",
    "\n",
    "# ----------------------------\n",
    "# Structure container and proposals\n",
    "# ----------------------------\n",
    "\n",
    "class Structure:\n",
    "    def __init__(self,\n",
    "                 S: List[str],\n",
    "                 Z_active: Set[FrozenSet[str]],\n",
    "                 A: Dict[Tuple[FrozenSet[str],FrozenSet[str]],int],\n",
    "                 unit_drop: bool = True):\n",
    "        # The \"model structure\" F = (Z, A) that the search optimizes.\n",
    "        # - S: universe of primitive types (e.g., {\"-7\",\"-8\",\"-9\"}).\n",
    "        # - Z_active: active potency sets (nodes of the DAG). Always includes singletons {t} for t in S.\n",
    "        #             May also include multi-type sets like {\"-7\",\"-8\"} depending on the prior/moves.\n",
    "        # - A: adjacency over Z_active, A[(P,Q)] ∈ {0,1}, indicating presence of edge P -> Q.\n",
    "        #      Edges are subset-monotone (and may enforce |P\\Q|=1 if unit_drop=True).\n",
    "        # - unit_drop: if True, only allow edges that drop exactly one element (|P\\Q| == 1).\n",
    "        #\n",
    "        # Where it’s used:\n",
    "        # - Created/updated inside map_search() during the annealed hill climb.\n",
    "        # - Passed into score_structure() which uses:\n",
    "        #     * struct.labels_list (sorted Z) and\n",
    "        #     * struct.Reach (transitive closure over A)\n",
    "        #   to run the DP (dp_tree_root_table) and compute the likelihood.\n",
    "        self.S=S\n",
    "        self.Z_active=set(Z_active)  # copy to decouple from caller; Z includes singletons and selected multis\n",
    "        self.A=dict(A)               # copy adjacency dict (edges)\n",
    "        self.unit_drop=unit_drop\n",
    "        # A consistent ordering of labels (frozensets) for indexing/memoization in DP\n",
    "        self.labels_list=self._sorted_labels()\n",
    "        # Reachability closure used by DP to constrain child labels given a parent label\n",
    "        self.Reach = transitive_closure(self.labels_list, self.A)\n",
    "\n",
    "    def _sorted_labels(self)->List[FrozenSet[str]]:\n",
    "        # Provide a stable, human-logical ordering of the active labels:\n",
    "        #   1) by set size (|L|), then\n",
    "        #   2) lexicographically by the sorted elements of the set.\n",
    "        # This keeps DP indices stable and makes printed output neat.\n",
    "        return sorted(list(self.Z_active), key=lambda x: (len(x), tuple(sorted(list(x)))))\n",
    "\n",
    "    def recompute_reach(self):\n",
    "        # Recompute both the sorted label list and the transitive closure Reach\n",
    "        # after any structural change (adding/removing potencies or edges).\n",
    "        # Called by all propose_* methods after they mutate Z_active or A.\n",
    "        self.labels_list=self._sorted_labels()\n",
    "        self.Reach = transitive_closure(self.labels_list, self.A)\n",
    "\n",
    "    def clone(self)->\"Structure\":\n",
    "        # Return a deep-enough copy to test/accept a proposal without mutating the current state.\n",
    "        # Used heavily during the stochastic search (map_search) to try local moves.\n",
    "        return Structure(self.S, set(self.Z_active), dict(self.A), self.unit_drop)\n",
    "\n",
    "    # --- Moves ---\n",
    "    def potencies_multi_all(self)->List[FrozenSet[str]]:\n",
    "        # Enumerate ALL candidate multi-type potencies (|P|>=2) over S.\n",
    "        # This is the proposal pool for adding or swapping potencies.\n",
    "        return [P for P in all_nonempty_subsets(self.S) if len(P)>=2]\n",
    "\n",
    "    def propose_add_potency(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        # Propose: add one multi-type potency (node) not yet in Z_active.\n",
    "        # Leaves edges A unchanged (edge proposals are separate); only Z is changed here.\n",
    "        # Returns a NEW Structure if a candidate exists; otherwise None.\n",
    "        candidates = [P for P in self.potencies_multi_all() if P not in self.Z_active]\n",
    "        if not candidates: return None\n",
    "        P = rng.choice(candidates)\n",
    "        new = self.clone()\n",
    "        new.Z_active.add(P)\n",
    "        # Keep the edge set as-is; we only ensure Reach is recomputed to stay consistent.\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "\n",
    "    def propose_remove_potency(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        # Propose: remove one existing multi-type potency from Z_active.\n",
    "        # Also removes any incident edges from A that reference that potency.\n",
    "        # Returns a NEW Structure, or None if there are no multis to remove.\n",
    "        candidates = [P for P in self.Z_active if len(P)>=2]\n",
    "        if not candidates: return None\n",
    "        P = rng.choice(candidates)\n",
    "        new = self.clone()\n",
    "        # remove potency and incident edges\n",
    "        new.Z_active.remove(P)\n",
    "        new.A = {e:v for e,v in new.A.items() if P not in e}\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "\n",
    "    def propose_swap_potency(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        # Propose: swap out one existing multi-type potency for a different one not currently active.\n",
    "        # Useful in fixed-k mode to keep the number of multi-type nodes constant while exploring.\n",
    "        remove_candidates = [P for P in self.Z_active if len(P)>=2]\n",
    "        add_candidates = [P for P in self.potencies_multi_all() if P not in self.Z_active]\n",
    "        if not remove_candidates or not add_candidates: return None\n",
    "        P_rm = rng.choice(remove_candidates)\n",
    "        P_add = rng.choice(add_candidates)\n",
    "        new = self.clone()\n",
    "        new.Z_active.remove(P_rm)\n",
    "        new.A = {e:v for e,v in new.A.items() if P_rm not in e}\n",
    "        new.Z_active.add(P_add)\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "\n",
    "    def all_edge_pairs(self)->List[Tuple[FrozenSet[str],FrozenSet[str]]]:\n",
    "        # Enumerate all admissible ordered pairs (P,Q) among the currently active potencies.\n",
    "        # Uses admissible_edge(P,Q, unit_drop) to enforce subset-monotone (and unit-drop if requested).\n",
    "        # This is the proposal pool for add-edge moves.\n",
    "        L=list(self.Z_active)\n",
    "        pairs=[]\n",
    "        for P in L:\n",
    "            for Q in L:\n",
    "                if admissible_edge(P,Q,self.unit_drop):\n",
    "                    pairs.append((P,Q))\n",
    "        return pairs\n",
    "\n",
    "    def propose_add_edge(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        # Propose: add a single admissible edge (P->Q) that is currently absent (A[(P,Q)] == 0).\n",
    "        # Returns a NEW Structure or None if no addable edge exists.\n",
    "        pairs = [e for e in self.all_edge_pairs() if self.A.get(e,0)==0]\n",
    "        if not pairs: return None\n",
    "        e = rng.choice(pairs)\n",
    "        new = self.clone()\n",
    "        new.A[e]=1\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "\n",
    "    def propose_remove_edge(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        # Propose: remove a single existing edge (P->Q) where A[(P,Q)] == 1.\n",
    "        # Returns a NEW Structure or None if there are no edges to remove.\n",
    "        edges = [e for e,v in self.A.items() if v==1]\n",
    "        if not edges: return None\n",
    "        e = rng.choice(edges)\n",
    "        new = self.clone()\n",
    "        del new.A[e]\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "        \n",
    "# ----------------------------\n",
    "# Scoring: log posterior\n",
    "# ----------------------------\n",
    "\n",
    "def score_structure(struct: Structure,\n",
    "                    trees: List[TreeNode],\n",
    "                    leaf_type_maps: List[Dict[str,str]],\n",
    "                    priors: Priors,\n",
    "                    prune_eps: float = 0.0) -> Tuple[float, List[float]]:\n",
    "    # Compute the (log) posterior score of a candidate structure F = (Z_active, A).\n",
    "    #\n",
    "    # Posterior:\n",
    "    #   log P(F | data) ∝ log P(F) + Σ_T log P(T | F)\n",
    "    # where:\n",
    "    #   - log P(F) splits into:\n",
    "    #       * log P(Z_active): prior over which potencies (nodes) are active\n",
    "    #       * log P(A | Z_active): prior over which admissible edges exist\n",
    "    #   - log P(T | F) is the tree likelihood computed by the DP + Beta integral.\n",
    "    #\n",
    "    # Inputs\n",
    "    #   struct         : Structure holding S, Z_active, A, labels_list (sorted Z), and Reach (transitive closure)\n",
    "    #   trees          : list of tree roots (each a TreeNode) to be scored under F\n",
    "    #   leaf_type_maps : parallel list of dicts mapping leaf name -> observed type (as strings)\n",
    "    #   priors         : Priors object encapsulating hyperparameters and prior computations\n",
    "    #   prune_eps      : optional DP pruning threshold (passed down to dp_tree_root_table)\n",
    "    #\n",
    "    # Outputs\n",
    "    #   (log_post, per_tree_logLs)\n",
    "    #     log_post      : total log posterior = log prior + sum of per-tree log likelihoods\n",
    "    #     per_tree_logLs: list of per-tree log-likelihoods log P(T | F) (one float per tree)\n",
    "    #\n",
    "    # Where this is used:\n",
    "    #   - Called repeatedly from map_search() to evaluate proposals during the\n",
    "    #     hill-climb / simulated annealing over (Z_active, A).\n",
    "\n",
    "    # ---- Prior over structure F ----\n",
    "    # log P(Z_active): fixed-k or Bernoulli over multi-type potencies (singles implicit/free)\n",
    "    logp = priors.log_prior_Z(struct.S, struct.Z_active)\n",
    "    # If Z violates the prior's support (e.g., wrong k in fixed-k), we get -inf; bail early.\n",
    "    if not math.isfinite(logp): return float(\"-inf\"), []\n",
    "    # log P(A | Z_active): independent Bernoulli(rho) over admissible edges under unit_drop flag\n",
    "    logp += priors.log_prior_A(struct.Z_active, struct.A, unit_drop=struct.unit_drop)\n",
    "\n",
    "    # ---- Likelihood over all trees ----\n",
    "    logLs=[]\n",
    "    for root, leaf_to_type in zip(trees, leaf_type_maps):\n",
    "        # Precompute B_sets[v] = set of observed types anywhere under node v.\n",
    "        # Robust behavior: leaves missing from the map contribute empty sets (ignored).\n",
    "        B_sets = compute_B_sets(root, leaf_to_type)\n",
    "\n",
    "        # Neutral-evidence shortcut:\n",
    "        # If the root accumulates *no observed types at all* (e.g., map has no usable labels\n",
    "        # for this tree after filtering), then this tree carries no information about F.\n",
    "        # We treat it as contributing log-likelihood 0.0 (i.e., multiplicative factor 1).\n",
    "        # This prevents crashes where P(T|F) would be numerically zero for all F.\n",
    "        root_labels = B_sets.get(root, set())\n",
    "        if not root_labels:\n",
    "            logLs.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Dynamic program over labelings constrained by struct.Reach:\n",
    "        # Builds a sparse table at the root: C[(O,D)] = total weight for that count pair,\n",
    "        # where O = observed hits, D = misses, given the root label and subtree constraints.\n",
    "        C = dp_tree_root_table(root, struct.labels_list, struct.Reach, B_sets, prune_eps=prune_eps)\n",
    "\n",
    "        # Integrate out p ~ Beta(1,1): P(T | F) = Σ_{(O,D)} C[(O,D)] * B(O+1, D+1).\n",
    "        P_T = tree_marginal_from_root_table(C)\n",
    "\n",
    "        # If numerical underflow or structural inconsistency yields P_T <= 0, the score is invalid.\n",
    "        if P_T <= 0 or not math.isfinite(P_T):\n",
    "            return float(\"-inf\"), []\n",
    "\n",
    "        # Accumulate per-tree log-likelihood\n",
    "        logLs.append(math.log(P_T))\n",
    "\n",
    "    # Total posterior score = log prior + sum of per-tree log-likelihoods\n",
    "    return logp + sum(logLs), logLs\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Annealed stochastic search\n",
    "# ----------------------------\n",
    "\n",
    "def map_search(\n",
    "    S: List[str],\n",
    "    trees: List[TreeNode],\n",
    "    leaf_type_maps: List[Dict[str,str]],\n",
    "    priors: Priors,\n",
    "    unit_drop_edges: bool = True,\n",
    "    fixed_k: Optional[int] = None,\n",
    "    init_seed: int = 0,\n",
    "    iters: int = 500,\n",
    "    restarts: int = 3,\n",
    "    temp_init: float = 1.0,\n",
    "    temp_decay: float = 0.995,\n",
    "    move_probs = (0.25, 0.25, 0.25, 0.25),  # addP, rmP, addE, rmE (swap used when fixed_k)\n",
    "    prune_eps: float = 0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Stochastic MAP (maximum a posteriori) structure search over F = (Z_active, A).\n",
    "\n",
    "    What this does (high level):\n",
    "      • Repeatedly constructs a candidate structure F = (Z, A) consisting of:\n",
    "          - Z: active potency sets (nodes) selected from all non-empty subsets of S\n",
    "          - A: directed edges between admissible pairs of potencies\n",
    "      • Scores each F using `score_structure` (log prior + sum log likelihoods over trees).\n",
    "      • Performs a simulated-annealing-style random local search with moves that\n",
    "        add/remove/swap potencies and add/remove edges, keeping the best seen solution.\n",
    "\n",
    "    Where this is used:\n",
    "      • Called by your main script after loading trees and leaf→type maps.\n",
    "      • Returns (best_structure, best_log_posterior, per_tree_log_likelihoods).\n",
    "    \"\"\"\n",
    "    rng = random.Random(init_seed)\n",
    "\n",
    "    best_global = None        # best Structure found across all restarts\n",
    "    best_score = float(\"-inf\")\n",
    "    best_logs = None          # (optional) store per-tree logs for the best\n",
    "\n",
    "    for rs in range(restarts):\n",
    "        # --- Initialization of a starting structure for this restart ---\n",
    "        if priors.potency_mode==\"fixed_k\":\n",
    "            # Start with singletons plus exactly `fixed_k` randomly sampled multi-type potencies.\n",
    "            # Z = build_Z_active(S, fixed_k=priors.fixed_k, max_potency_size=len(S), seed=rng.randint(0,10**9))\n",
    "\n",
    "            aggregated_transitions, Z = init_progenitors_union_fitch(S, trees, leaf_type_maps, fixed_k)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # Start with singletons + a few multis (here: fixed_k=0 means only singletons to start).\n",
    "            base = build_Z_active(S, fixed_k=0, max_potency_size=len(S), seed=rng.randint(0,10**9))\n",
    "            Z = base\n",
    "\n",
    "        # Initial edge set: here empty (no edges). You can explore edges via moves later.\n",
    "        # NOTE: with A = {}, parent/child labels cannot \"drop\" unless identical; feasibility then\n",
    "        # depends on Z containing a superset label compatible with each tree.\n",
    "        # A = build_edges(Z, ...) would initialize a fully connected admissible DAG instead.\n",
    "        # A = {}\n",
    "        \n",
    "        A = build_mid_sized_connected_dag(Z,keep_prob = 0.3,rng = None)\n",
    "\n",
    "        # Build the initial Structure and score it.\n",
    "        current = Structure(S, Z, A, unit_drop=unit_drop_edges)\n",
    "        curr_score, _ = score_structure(current, trees, leaf_type_maps, priors, prune_eps)\n",
    "\n",
    "        # --- Fallback: resample a few times if the initialization is invalid (score = -inf) ---\n",
    "        attempts=0\n",
    "        while not math.isfinite(curr_score) and attempts<20:\n",
    "            # Rebuild Z again (same logic as above) and try a fresh start.\n",
    "            # Z = build_Z_active(S, fixed_k=(priors.fixed_k if priors.potency_mode==\"fixed_k\" else 0),\n",
    "            #                    max_potency_size=len(S), seed=rng.randint(0,10**9))\n",
    "            aggregated_transitions, Z = init_progenitors_union_fitch(S, trees, leaf_type_maps, fixed_k)\n",
    "\n",
    "            # Start again with no edges; exploration will add/remove edges during the search.\n",
    "            # A = {}\n",
    "            A = build_mid_sized_connected_dag(Z,keep_prob = 0.3,rng = None)\n",
    "\n",
    "            # (Debug prints you added: show sampled Z and A, and the resulting score attempt)\n",
    "            print(f\"Z:{Z}\")\n",
    "            print(f\"A:{A}\")\n",
    "\n",
    "            current = Structure(S, Z, A, unit_drop=unit_drop_edges)\n",
    "            curr_score, _ = score_structure(current, trees, leaf_type_maps, priors, prune_eps)\n",
    "            print(curr_score)\n",
    "            attempts+=1\n",
    "\n",
    "        # If after several attempts we still don't have a finite score, bail out for this run.\n",
    "        if not math.isfinite(curr_score):\n",
    "            # This typically indicates that, with the chosen initialization (e.g., A = {} and\n",
    "            # the sampled Z), no feasible labeling exists for at least one tree, so P(T|F)=0.\n",
    "            # Consider easing the settings, e.g., initializing with edges or ensuring Z has a superset.\n",
    "            raise RuntimeError(\"Failed to initialize a valid structure; consider easing settings.\")\n",
    "\n",
    "        # Track the best for this restart (local best) and across restarts (global best).\n",
    "        local_best = current.clone()\n",
    "        local_best_score = curr_score\n",
    "        if (best_score < curr_score):\n",
    "            best_score = curr_score\n",
    "            best_global = current.clone();\n",
    "\n",
    "        # Simulated annealing temperature and move probabilities\n",
    "        tau = temp_init\n",
    "        addP, rmP, addE, rmE = move_probs\n",
    "\n",
    "        # ---- Main annealed local search loop ----\n",
    "        pbar = trange(iters, desc=f\"Restart {rs+1}/{restarts}\", leave=True)\n",
    "        for it in pbar:\n",
    "            # --- Propose a neighboring structure by one of the local moves ---\n",
    "            prop=None\n",
    "            if priors.potency_mode==\"fixed_k\":\n",
    "                # In fixed-k mode over multis:\n",
    "                #   - Prefer to modify edges; to change potencies while keeping |multis|=k, use \"swap\".\n",
    "                r = rng.random()\n",
    "                if r < addE:\n",
    "                    prop = current.propose_add_edge(rng)     # add a single admissible edge\n",
    "                elif r < addE + rmE:\n",
    "                    prop = current.propose_remove_edge(rng)  # remove a single existing edge\n",
    "                else:\n",
    "                    prop = current.propose_swap_potency(rng) # swap one multi for another\n",
    "            else:\n",
    "                # In non-fixed-k mode:\n",
    "                #   - Add/remove potencies and add/remove edges according to move_probs.\n",
    "                r = rng.random()\n",
    "                if r < addP:\n",
    "                    prop = current.propose_add_potency(rng)\n",
    "                elif r < addP + rmP:\n",
    "                    prop = current.propose_remove_potency(rng)\n",
    "                elif r < addP + rmP + addE:\n",
    "                    prop = current.propose_add_edge(rng)\n",
    "                else:\n",
    "                    prop = current.propose_remove_edge(rng)\n",
    "\n",
    "            if prop is None:\n",
    "                # If the chosen move had no available candidate (e.g., no edges to remove), just cool.\n",
    "                tau *= temp_decay\n",
    "                # Progress bar diagnostics: global best, current score, and temperature.\n",
    "                pbar.set_postfix({\"Best\": f\"{best_score:.3f}\", \"Curr\": f\"{curr_score:.3f}\", \"Temp\": f\"{tau:.3f}\"})\n",
    "                continue\n",
    "\n",
    "            # --- Score the proposed structure ---\n",
    "            prop_score, _ = score_structure(prop, trees, leaf_type_maps, priors, prune_eps)\n",
    "\n",
    "            # --- Metropolis/annealing acceptance ---\n",
    "            delta = prop_score - curr_score\n",
    "            accept = (delta >= 0) or (rng.random() < math.exp(delta / max(tau,1e-12))) #Probability injected here\n",
    "            if accept:\n",
    "                # Move to the proposal\n",
    "                current = prop\n",
    "                curr_score = prop_score\n",
    "\n",
    "                # Update local best (within this restart)\n",
    "                if curr_score > local_best_score:\n",
    "                    local_best = current.clone()\n",
    "                    local_best_score = curr_score\n",
    "\n",
    "                # Update global best (across all restarts)\n",
    "                if curr_score > best_score:\n",
    "                    best_global = current.clone()\n",
    "                    best_score = curr_score\n",
    "                    # print(\"New best_score is\", best_score);\n",
    "                    best_logs = None  # defer detailed per-tree logs until the end\n",
    "\n",
    "            # Geometric cooling of the temperature\n",
    "            tau *= temp_decay\n",
    "\n",
    "            # Progress bar diagnostics each iteration\n",
    "            pbar.set_postfix({\"Best\": f\"{best_score:.3f}\", \"Curr\": f\"{curr_score:.3f}\", \"Temp\": f\"{tau:.3f}\"})\n",
    "            # print(f\"Current global best: {best_score}\")\n",
    "        # end of one restart; loop begins again if more restarts remain\n",
    "\n",
    "    # After all restarts, recompute per-tree logs for the best structure found\n",
    "    final_score, logLs = score_structure(best_global, trees, leaf_type_maps, priors, prune_eps)\n",
    "    return best_global, final_score, logLs\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def read_leaf_type_map(path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Read a leaf->type mapping from a file.\n",
    "\n",
    "    Supported:\n",
    "      - JSON dict: { \"LeafName\": \"Type\", ... }\n",
    "      - CSV/TSV/TXT with 2 columns (header optional):\n",
    "          * If header present, typical field names could be:\n",
    "              - leaf, type\n",
    "              - cellBC, cell_state (your .txt example)\n",
    "    Returns: dict {leaf_name: type_symbol} (types are coerced to str)\n",
    "    \"\"\"\n",
    "    import os, csv, json\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".json\",):\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(f\"{path}: JSON must be an object mapping leaf->type.\")\n",
    "        return {str(k): str(v) for k, v in data.items()}\n",
    "\n",
    "    elif ext in (\".csv\", \".tsv\", \".txt\"):\n",
    "        # treat .txt as TSV by default (your example is tab-delimited)\n",
    "        delim = \"\\t\" if ext in (\".tsv\", \".txt\") else \",\"\n",
    "        out = {}\n",
    "        with open(path, \"r\", newline=\"\") as f:\n",
    "            reader = csv.reader(f, delimiter=delim)\n",
    "            rows = list(reader)\n",
    "            if not rows:\n",
    "                raise ValueError(f\"{path}: empty file\")\n",
    "\n",
    "            # Detect header\n",
    "            start_idx = 0\n",
    "            header = [h.strip().lower() for h in rows[0]] if rows and rows[0] else []\n",
    "            has_header = False\n",
    "            if len(header) >= 2:\n",
    "                # Common header names we accept\n",
    "                if (\"leaf\" in header[0] or \"cellbc\" in header[0]) and (\"type\" in header[1] or \"cell_state\" in header[1]):\n",
    "                    has_header = True\n",
    "                # Or any header line where at least one of ('leaf','cellbc') and one of ('type','cell_state') appear\n",
    "                if not has_header:\n",
    "                    left_has = any(x in header for x in (\"leaf\", \"cellbc\"))\n",
    "                    right_has = any(x in header for x in (\"type\", \"cell_state\"))\n",
    "                    has_header = left_has and right_has\n",
    "\n",
    "            if has_header:\n",
    "                start_idx = 1\n",
    "\n",
    "            for i in range(start_idx, len(rows)):\n",
    "                row = rows[i]\n",
    "                if len(row) < 2:\n",
    "                    raise ValueError(f\"{path}: line {i+1} needs at least 2 columns (leaf,type)\")\n",
    "                leaf = row[0].strip()\n",
    "                typ  = row[1].strip()\n",
    "                if not leaf or not typ:\n",
    "                    raise ValueError(f\"{path}: line {i+1} has empty leaf/type\")\n",
    "                if leaf in out:\n",
    "                    raise ValueError(f\"{path}: duplicate leaf '{leaf}' at line {i+1}\")\n",
    "                out[leaf] = str(typ)  # coerce types to string (handles negatives like -7, -9)\n",
    "        return out\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mapping file type: {path} (use .csv, .tsv, .txt, or .json)\")\n",
    "        \n",
    "\n",
    "def validate_leaf_type_map(root: TreeNode, leaf_map: Dict[str,str], S: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Ensure mapping covers exactly the leaves in the tree, and types are in S.\n",
    "    Raises ValueError if not valid.\n",
    "    \"\"\"\n",
    "    leaves_in_tree = set(collect_leaf_names(root))\n",
    "    leaves_in_map  = set(leaf_map.keys())\n",
    "\n",
    "    missing = leaves_in_tree - leaves_in_map\n",
    "    extra   = leaves_in_map  - leaves_in_tree\n",
    "    if missing:\n",
    "        raise ValueError(f\"Leaf map missing leaves: {sorted(missing)}\")\n",
    "    if extra:\n",
    "        raise ValueError(f\"Leaf map has unknown leaves not in tree: {sorted(extra)}\")\n",
    "\n",
    "    allowed = set(S)\n",
    "    bad_types = {t for t in leaf_map.values() if t not in allowed}\n",
    "    if bad_types:\n",
    "        raise ValueError(f\"Leaf map contains types not in S={S}: {sorted(bad_types)}\")\n",
    "\n",
    "\n",
    "def filter_leaf_map_to_tree(root: TreeNode, leaf_map: Dict[str, str]) -> Dict[str, str]:\n",
    "    leaves = set(collect_leaf_names(root))\n",
    "    return {leaf: str(typ) for leaf, typ in leaf_map.items() if leaf in leaves}\n",
    "    \n",
    "# ----------------------------\n",
    "# Demo: random trees + search\n",
    "# ----------------------------\n",
    "\n",
    "# def main():\n",
    "#     import random\n",
    "#     random.seed(7)\n",
    "\n",
    "#     # Load Newick strings from .txt (same format as .nwk)\n",
    "#     trees = [read_newick_file(\"./0002_tree_0.txt\"),\n",
    "#              read_newick_file(\"./0002_tree_1.txt\"),\n",
    "#              read_newick_file(\"./0002_tree_2.txt\"),\n",
    "#              read_newick_file(\"./0002_tree_3.txt\"),\n",
    "#              read_newick_file(\"./0002_tree_4.txt\")]\n",
    "#     # trees = [read_newick_file(\"demo_1.nwk\"),\n",
    "#     #           read_newick_file(\"demo_2.nwk\")\n",
    "#     #         ]\n",
    "\n",
    "#     # TAB-delimited maps with header 'cellBC\\tcell_state'\n",
    "#     map_paths = [\n",
    "#         \"./0002_meta_0.txt\",\n",
    "#         \"./0002_meta_1.txt\",\n",
    "#         \"./0002_meta_2.txt\",\n",
    "#         \"./0002_meta_3.txt\",\n",
    "#         \"./0002_meta_4.txt\",\n",
    "#     ]\n",
    "\n",
    "#     # map_paths = [\n",
    "#     #     \"dict_1.txt\",\n",
    "#     #     \"dict_2.txt\"\n",
    "#     # ]\n",
    "#     raw_maps = [read_leaf_type_map(p) for p in map_paths]\n",
    "\n",
    "#     # Drop dictionary entries not present in the corresponding tree\n",
    "#     leaf_type_maps = [filter_leaf_map_to_tree(root, m) for root, m in zip(trees, raw_maps)]\n",
    "\n",
    "#     # Build S from types that are actually used after filtering\n",
    "#     S = sorted({str(t) for m in leaf_type_maps for t in m.values()})\n",
    "\n",
    "#     # (Optional) soft warnings; never raise\n",
    "#     for idx, (root, m_raw, m_used) in enumerate(zip(trees, raw_maps, leaf_type_maps), 1):\n",
    "#         leaves_tree = set(collect_leaf_names(root))\n",
    "#         extra = sorted(set(m_raw.keys()) - leaves_tree)\n",
    "#         missing = sorted(leaves_tree - set(m_used.keys()))  # leaves in tree with no mapping\n",
    "#         if extra:\n",
    "#             print(f\"[warn] Tree {idx}: {len(extra)} map entries are not in the tree and were ignored \"\n",
    "#                   f\"(e.g., {extra[:5]}{'...' if len(extra)>5 else ''})\")\n",
    "#         if missing:\n",
    "#             print(f\"[warn] Tree {idx}: {len(missing)} tree leaves have no mapping and were ignored \"\n",
    "#                   f\"(e.g., {missing[:5]}{'...' if len(missing)>5 else ''})\")\n",
    "#         if not any(True for _ in m_used):\n",
    "#             print(f\"[warn] Tree {idx}: no mapped leaves; treating as neutral evidence.\")\n",
    "\n",
    "#     priors = Priors(potency_mode=\"fixed_k\", fixed_k=5, rho=0.2)\n",
    "\n",
    "#     bestF, best_score, per_tree_logs = map_search(\n",
    "#         S=S,\n",
    "#         trees=trees,\n",
    "#         leaf_type_maps=leaf_type_maps,\n",
    "#         priors=priors,\n",
    "#         unit_drop_edges=False,\n",
    "#         fixed_k=priors.fixed_k if priors.potency_mode==\"fixed_k\" else None,\n",
    "#         init_seed=123,\n",
    "#         iters=500,\n",
    "#         restarts=2,\n",
    "#         temp_init=1.0,\n",
    "#         temp_decay=0.995,\n",
    "#         move_probs=(0.3, 0.2, 0.3, 0.2),\n",
    "#         prune_eps=0.0\n",
    "#     )\n",
    "\n",
    "#     # --- Pretty-print best map ---\n",
    "#     def pot_str(P): return \"{\" + \",\".join(sorted(list(P))) + \"}\"\n",
    "#     print(\"\\n=== BEST MAP (F*) ===\")\n",
    "#     multi_sorted = sorted([P for P in bestF.Z_active if len(P)>=2], key=lambda x:(len(x), tuple(sorted(list(x)))))\n",
    "#     print(\"Active potencies (multi-type):\")\n",
    "#     for P in multi_sorted: print(\"  \", pot_str(P))\n",
    "#     print(\"Singletons (always active):\")\n",
    "#     for t in S: print(\"  \", \"{\"+t+\"}\")\n",
    "\n",
    "#     print(\"\\nEdges:\")\n",
    "#     edges = sorted([e for e,v in bestF.A.items() if v==1], key=lambda e:(len(e[0]), len(e[1]), tuple(sorted(list(e[0]))), tuple(sorted(list(e[1])))))\n",
    "#     for P,Q in edges:\n",
    "#         print(f\"  {pot_str(P)} -> {pot_str(Q)}\")\n",
    "\n",
    "#     print(\"\\nScores:\")\n",
    "#     print(f\"  log posterior: {best_score:.6f}\")\n",
    "#     for i,lg in enumerate(per_tree_logs,1):\n",
    "#         print(f\"  Tree {i} log P(T|F*): {lg:.6f}\")\n",
    "\n",
    "def _read_json_objects_exact(path: str):\n",
    "    \"\"\"Read one JSON object per non-empty line (your file format).\"\"\"\n",
    "    objs = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            objs.append(json.loads(line))\n",
    "    if not objs:\n",
    "        raise ValueError(f\"{path}: no JSON objects found\")\n",
    "    return objs\n",
    "\n",
    "def _extract_vertices_edges_from_adj(adj):\n",
    "    V = set(adj.keys())\n",
    "    for chs in adj.values():\n",
    "        if isinstance(chs, list):\n",
    "            V.update(chs)\n",
    "    E = []\n",
    "    for u, chs in adj.items():\n",
    "        if isinstance(chs, list):\n",
    "            for v in chs:\n",
    "                E.append((str(u), str(v)))\n",
    "    V = sorted(map(str, V), key=lambda x: (len(x), x))\n",
    "    E = sorted(E, key=lambda e: (e[0], e[1]))\n",
    "    return V, E\n",
    "\n",
    "def _normalize_adj_remove_synthetic_root(adj: dict) -> dict:\n",
    "    \"\"\"Drop a synthetic 'root' node (if present) from adjacency for building F.\"\"\"\n",
    "    adj2 = {str(k): (list(v) if isinstance(v, list) else v) for k, v in adj.items()}\n",
    "    if \"root\" in adj2:\n",
    "        ch = adj2[\"root\"]\n",
    "        if not isinstance(ch, list) or len(ch) != 1:\n",
    "            raise ValueError(\"Synthetic 'root' must have exactly one child\")\n",
    "        del adj2[\"root\"]\n",
    "    return adj2\n",
    "\n",
    "def _resolve_id_to_set(id_str: str, comp_map: dict, memo: dict, visiting: set) -> frozenset:\n",
    "    \"\"\"\n",
    "    Recursively resolve an id to a frozenset of base (negative-string) types.\n",
    "    - negative id: returns {id}\n",
    "    - list value: union of resolves\n",
    "    - single value: resolve that\n",
    "    Detects cycles and missing entries.\n",
    "    \"\"\"\n",
    "    id_str = str(id_str)\n",
    "    if id_str.startswith(\"-\"):\n",
    "        return frozenset([id_str])\n",
    "    if id_str in memo:\n",
    "        return memo[id_str]\n",
    "    if id_str in visiting:\n",
    "        raise ValueError(f\"Cycle detected while resolving potency '{id_str}'\")\n",
    "    if id_str not in comp_map:\n",
    "        raise ValueError(f\"Positive id '{id_str}' appears but not defined in composition map\")\n",
    "    visiting.add(id_str)\n",
    "    val = comp_map[id_str]\n",
    "    acc = set()\n",
    "    if isinstance(val, list):\n",
    "        for child in val:\n",
    "            acc |= _resolve_id_to_set(str(child), comp_map, memo, visiting)\n",
    "    else:\n",
    "        acc |= _resolve_id_to_set(str(val), comp_map, memo, visiting)\n",
    "    visiting.remove(id_str)\n",
    "    memo[id_str] = frozenset(acc)\n",
    "    return memo[id_str]\n",
    "\n",
    "def _build_ZA_from_txt(adj: dict, comp_map: dict, unit_drop_edges: bool):\n",
    "    \"\"\"\n",
    "    Build F = (Z_active, A) from adjacency + hierarchical composition map.\n",
    "    Returns: Z_active, A, base_types(list), potency_id_to_set(dict id->frozenset)\n",
    "    \"\"\"\n",
    "    # Drop synthetic \"root\" from adjacency for structure building\n",
    "    adj = _normalize_adj_remove_synthetic_root(adj)\n",
    "    # Collect all ids we need to resolve\n",
    "    ids_seen = set(map(str, comp_map.keys()))\n",
    "    for u, chs in adj.items():\n",
    "        ids_seen.add(str(u))\n",
    "        if isinstance(chs, list):\n",
    "            for v in chs:\n",
    "                ids_seen.add(str(v))\n",
    "    memo = {}\n",
    "    potency_id_to_set = {}\n",
    "    base_types = set()\n",
    "    # Resolve every id\n",
    "    for idv in ids_seen:\n",
    "        if idv.startswith(\"-\"):\n",
    "            memo[idv] = frozenset([idv])\n",
    "        else:\n",
    "            s = _resolve_id_to_set(idv, comp_map, memo, visiting=set())\n",
    "            potency_id_to_set[idv] = s\n",
    "    # Gather base types\n",
    "    for s in memo.values():\n",
    "        for t in s:\n",
    "            if t.startswith(\"-\"):\n",
    "                base_types.add(t)\n",
    "    # Z: singletons for all base types + multi-type potencies (size >=2)\n",
    "    Z_active = {frozenset([t]) for t in base_types}\n",
    "    for pid, s in potency_id_to_set.items():\n",
    "        if len(s) >= 2:\n",
    "            Z_active.add(s)\n",
    "    # A: only edges in adjacency, mapped via expansion; keep admissible ones\n",
    "    A = {}\n",
    "    def id_to_set(x: str) -> frozenset:\n",
    "        x = str(x)\n",
    "        if x.startswith(\"-\"):\n",
    "            return frozenset([x])\n",
    "        return potency_id_to_set[x]  # safe after resolution above\n",
    "    for u, chs in adj.items():\n",
    "        Pu = id_to_set(u)\n",
    "        for v in chs:\n",
    "            Qv = id_to_set(v)\n",
    "            if admissible_edge(Pu, Qv, unit_drop_edges):\n",
    "                A[(Pu, Qv)] = 1\n",
    "    return Z_active, A, sorted(base_types), potency_id_to_set\n",
    "\n",
    "def score_given_map_and_trees(txt_path: str, trees, meta_paths, fixed_k,\n",
    "                              unit_drop_edges = False):\n",
    "    \"\"\"\n",
    "    Parses the input file and builds the structure F=(Z,A),\n",
    "    then scores the log-likelihood of the given trees.\n",
    "    Returns:\n",
    "        potency_sets (set of frozenset): all potency states\n",
    "        total_ll (float): total log-likelihood across trees\n",
    "    \"\"\"\n",
    "    objs = _read_json_objects_exact(txt_path)\n",
    "    if len(objs) < 4:\n",
    "        raise ValueError(\"Expected at least 4 JSON lines (adjacency, weights, composition map, root).\")\n",
    "\n",
    "    # 1) adjacency\n",
    "    adj = None\n",
    "    for o in objs:\n",
    "        if isinstance(o, dict) and any(isinstance(v, list) for v in o.values()):\n",
    "            adj = {str(k): [str(x) for x in v] for k, v in o.items() if isinstance(v, list)}\n",
    "            break\n",
    "    if adj is None:\n",
    "        raise ValueError(\"Could not locate adjacency dict in the file.\")\n",
    "\n",
    "    # 2) composition map\n",
    "    comp_map = objs[2]\n",
    "    if not isinstance(comp_map, dict):\n",
    "        raise ValueError(\"Third JSON must be the composition map (dict).\")\n",
    "\n",
    "    # 3) root id\n",
    "    root_id = objs[3]\n",
    "    if isinstance(root_id, dict) and \"root_id\" in root_id:\n",
    "        root_id = root_id[\"root_id\"]\n",
    "    root_id = str(root_id)\n",
    "\n",
    "    # Print vertices and edges\n",
    "    V, E = _extract_vertices_edges_from_adj(adj)\n",
    "    # print(\"=== Parsed Graph: Vertices ===\")\n",
    "    # for v in V: \n",
    "    #     print(\" \", v)\n",
    "    # print(\"\\n=== Parsed Graph: Edges (u -> v) ===\")\n",
    "    # for u, v in E: \n",
    "    #     print(f\"  {u} -> {v}\")\n",
    "\n",
    "    # Build Z, A, and potency definitions\n",
    "    Z_from_map, A_from_map, base_types_map, potency_def = _build_ZA_from_txt(\n",
    "        adj=adj,\n",
    "        comp_map=comp_map,\n",
    "        unit_drop_edges=unit_drop_edges\n",
    "    )\n",
    "\n",
    "    # Print potency definitions\n",
    "    # print(\"\\n=== Potency definitions (expanded) ===\")\n",
    "    # for pid in sorted(potency_def, key=lambda x: (len(x), x)):\n",
    "    #     s = \",\".join(sorted(potency_def[pid]))\n",
    "    #     print(f\"  {pid} := {{{s}}}\")\n",
    "\n",
    "    # Load trees and leaf maps\n",
    "    # trees = [\n",
    "    #     read_newick_file(\"./0002_tree_0.txt\"),\n",
    "    #     read_newick_file(\"./0002_tree_1.txt\"),\n",
    "    #     read_newick_file(\"./0002_tree_2.txt\"),\n",
    "    #     read_newick_file(\"./0002_tree_3.txt\"),\n",
    "    #     read_newick_file(\"./0002_tree_4.txt\")\n",
    "    # ]\n",
    "    # meta_paths = [\n",
    "    #     \"./0002_meta_0.txt\",\n",
    "    #     \"./0002_meta_1.txt\",\n",
    "    #     \"./0002_meta_2.txt\",\n",
    "    #     \"./0002_meta_3.txt\",\n",
    "    #     \"./0002_meta_4.txt\"\n",
    "    # ]\n",
    "    raw_maps = [read_leaf_type_map(p) for p in meta_paths]\n",
    "    leaf_type_maps = [filter_leaf_map_to_tree(root, m) for root, m in zip(trees, raw_maps)]\n",
    "    base_types_data = sorted({str(t) for m in leaf_type_maps for t in m.values()})\n",
    "\n",
    "    # Merge sets for structure\n",
    "    S_all = sorted(set(base_types_map) | set(base_types_data))\n",
    "    Z_active = set(Z_from_map) | {frozenset([t]) for t in S_all}\n",
    "    A = dict(A_from_map)\n",
    "\n",
    "    struct = Structure(S=S_all, Z_active=Z_active, A=A, unit_drop=unit_drop_edges)\n",
    "    dummy_priors = Priors(potency_mode=\"fixed_k\", fixed_k = fixed_k, rho=0.2)\n",
    "\n",
    "    log_post, per_tree_logs = score_structure(\n",
    "        struct=struct,\n",
    "        trees=trees,\n",
    "        leaf_type_maps=leaf_type_maps,\n",
    "        priors=dummy_priors,\n",
    "        prune_eps=0.0\n",
    "    )\n",
    "\n",
    "    total_ll = sum(per_tree_logs)\n",
    "\n",
    "    print(\"\\n=== Ground Truth Log-likelihoods (given F from map) ===\")\n",
    "    for i, lg in enumerate(per_tree_logs, 1):\n",
    "        print(f\"Tree {i}: log P(T|F) = {lg:.6f}\")\n",
    "    print(f\"Total log-likelihood = {total_ll:.6f}\")\n",
    "\n",
    "    # Convert potency_def dict to set of frozensets\n",
    "    potency_sets = {frozenset(members) for members in potency_def.values()}\n",
    "\n",
    "    return potency_sets, total_ll\n",
    "\n",
    "def jaccard_distance(set1, set2):\n",
    "    if not set1 and not set2:\n",
    "        return 0.0\n",
    "    return 1 - len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "\n",
    "def pretty_print_sets(name, sets):\n",
    "    print(f\"\\n{name}:\")\n",
    "    for s in sorted(sets, key=lambda x: (len(x), sorted(x))):\n",
    "        print(\"  \", sorted(list(s)))\n",
    "\n",
    "# def main():\n",
    "#     import random\n",
    "#     random.seed(7)\n",
    "\n",
    "#     # ---------------- Load experimental trees and meta maps ----------------\n",
    "#     trees = [read_newick_file(f\"./0002_tree_{i}.txt\") for i in range(5)]\n",
    "#     meta_paths = [f\"./0002_meta_{i}.txt\" for i in range(5)]\n",
    "#     raw_maps = [read_leaf_type_map(p) for p in meta_paths]\n",
    "#     leaf_type_maps = [filter_leaf_map_to_tree(root, m) for root, m in zip(trees, raw_maps)]\n",
    "\n",
    "#     # Build S from types that are actually used\n",
    "#     S = sorted({str(t) for m in leaf_type_maps for t in m.values()})\n",
    "\n",
    "#     # ---------------- Run search for best inferred map ----------------\n",
    "#     priors = Priors(potency_mode=\"fixed_k\", fixed_k=5, rho=0.2)\n",
    "#     bestF, best_score, per_tree_logs = map_search(\n",
    "#         S=S,\n",
    "#         trees=trees,\n",
    "#         leaf_type_maps=leaf_type_maps,\n",
    "#         priors=priors,\n",
    "#         unit_drop_edges=False,\n",
    "#         fixed_k=priors.fixed_k if priors.potency_mode == \"fixed_k\" else None,\n",
    "#         init_seed=123,\n",
    "#         iters=50,\n",
    "#         restarts=2,\n",
    "#         temp_init=1.0,\n",
    "#         temp_decay=0.995,\n",
    "#         move_probs=(0.3, 0.2, 0.3, 0.2),\n",
    "#         prune_eps=0.0\n",
    "#     )\n",
    "\n",
    "#     # ---------------- Pretty print inferred map ----------------\n",
    "#     def pot_str(P): return \"{\" + \",\".join(sorted(list(P))) + \"}\"\n",
    "#     print(\"\\n=== BEST MAP (F*) ===\")\n",
    "#     multi_sorted = sorted([P for P in bestF.Z_active if len(P) >= 2],\n",
    "#                           key=lambda x: (len(x), tuple(sorted(list(x)))))\n",
    "#     print(\"Active potencies (multi-type):\")\n",
    "#     for P in multi_sorted:\n",
    "#         print(\"  \", pot_str(P))\n",
    "#     print(\"Singletons (always active):\")\n",
    "#     for t in S:\n",
    "#         print(\"  \", \"{\" + t + \"}\")\n",
    "\n",
    "#     print(\"\\nEdges:\")\n",
    "#     edges = sorted([e for e, v in bestF.A.items() if v == 1],\n",
    "#                    key=lambda e: (len(e[0]), len(e[1]), tuple(sorted(list(e[0]))), tuple(sorted(list(e[1])))))\n",
    "#     for P, Q in edges:\n",
    "#         print(f\"  {pot_str(P)} -> {pot_str(Q)}\")\n",
    "\n",
    "#     print(\"\\nScores:\")\n",
    "#     print(f\"  log posterior: {best_score:.6f}\")\n",
    "#     for i, lg in enumerate(per_tree_logs, 1):\n",
    "#         print(f\"  Tree {i} log P(T|F*): {lg:.6f}\")\n",
    "\n",
    "#     # ---------------- Ground truth scoring ----------------\n",
    "#     # ground_truth_path = \"./0002_truth.txt\"\n",
    "#     # print(\"\\n=== Ground Truth Log-Likelihood ===\")\n",
    "#     # gt_log_likelihood = score_given_map_and_trees(\n",
    "#     #     txt_path=ground_truth_path,\n",
    "#     #     unit_drop_edges=False\n",
    "#     # )\n",
    "#     # print(f\"Total Ground Truth Log-Likelihood: {gt_log_likelihood:.6f}\")\n",
    "\n",
    "#     # ---------------- Jaccard distance ----------------\n",
    "#     # Extract ground truth potency sets for Jaccard\n",
    "#     # def extract_potency_sets(txt_path):\n",
    "#     #     objs = _read_json_objects_exact(txt_path)\n",
    "#     #     comp_map = objs[2]\n",
    "#     #     return {frozenset(members) for members in comp_map.values()}\n",
    "\n",
    "#     predicted_sets = {p for p in bestF.Z_active if len(p) > 1}\n",
    "#     ground_truth_sets, sc = score_given_map_and_trees(\"main.txt\", trees, meta_paths, fixed_k=5)\n",
    "\n",
    "#     # Print predicted and ground truth sets\n",
    "#     pretty_print_sets(\"Predicted Sets\", predicted_sets)\n",
    "#     pretty_print_sets(\"Ground Truth Sets\", ground_truth_sets)\n",
    "    \n",
    "#     # Compute and print Jaccard distance\n",
    "#     jd = jaccard_distance(predicted_sets, ground_truth_sets)\n",
    "#     print(\"\\n=== Jaccard Distance ===\")\n",
    "#     print(f\"Jaccard Distance (Predicted vs Ground Truth): {jd:.6f}\")\n",
    "#     print(f\"Predicted map's loss: {best_score:.6f}\")\n",
    "#     print(f\"Ground truth's loss: {sc:.6f}\")\n",
    "    \n",
    "#     # print(sc)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from typing import List\n",
    "\n",
    "def pot_str(P):\n",
    "    return \"{\" + \",\".join(sorted(list(P))) + \"}\"\n",
    "\n",
    "def process_folder(folder: str, priors, iters=50, restarts=2):\n",
    "    \"\"\"Process a single folder: run MAP search, print details, and return metrics.\"\"\"\n",
    "    # ---- Load trees and meta maps ----\n",
    "    trees = [read_newick_file(os.path.join(folder, f\"{folder}_tree_{i}.txt\")) for i in range(5)]\n",
    "    meta_paths = [os.path.join(folder, f\"{folder}_meta_{i}.txt\") for i in range(5)]\n",
    "    raw_maps = [read_leaf_type_map(p) for p in meta_paths]\n",
    "    leaf_type_maps = [filter_leaf_map_to_tree(root, m) for root, m in zip(trees, raw_maps)]\n",
    "\n",
    "    # ---- Build S ----\n",
    "    S = sorted({str(t) for m in leaf_type_maps for t in m.values()})\n",
    "\n",
    "    # ---- Run search ----\n",
    "    bestF, best_score, per_tree_logs = map_search(\n",
    "        S=S,\n",
    "        trees=trees,\n",
    "        leaf_type_maps=leaf_type_maps,\n",
    "        priors=priors,\n",
    "        unit_drop_edges=False,\n",
    "        fixed_k=priors.fixed_k if priors.potency_mode == \"fixed_k\" else None,\n",
    "        init_seed=123,\n",
    "        iters=iters,\n",
    "        restarts=restarts,\n",
    "        temp_init=1.0,\n",
    "        temp_decay=0.995,\n",
    "        move_probs=(0.3, 0.2, 0.3, 0.2),\n",
    "        prune_eps=0.0\n",
    "    )\n",
    "\n",
    "    # ---- Pretty print inferred map ----\n",
    "    print(f\"\\n=== BEST MAP for {folder} ===\")\n",
    "    multi_sorted = sorted(\n",
    "        [P for P in bestF.Z_active if len(P) >= 2],\n",
    "        key=lambda x: (len(x), tuple(sorted(list(x))))\n",
    "    )\n",
    "    print(\"Active potencies (multi-type):\")\n",
    "    for P in multi_sorted:\n",
    "        print(\"  \", pot_str(P))\n",
    "    print(\"Singletons (always active):\")\n",
    "    for t in S:\n",
    "        print(\"  \", \"{\" + t + \"}\")\n",
    "\n",
    "    print(\"\\nEdges:\")\n",
    "    edges = sorted(\n",
    "        [e for e, v in bestF.A.items() if v == 1],\n",
    "        key=lambda e: (len(e[0]), len(e[1]), tuple(sorted(list(e[0]))), tuple(sorted(list(e[1]))))\n",
    "    )\n",
    "    for P, Q in edges:\n",
    "        print(f\"  {pot_str(P)} -> {pot_str(Q)}\")\n",
    "\n",
    "    print(\"\\nScores:\")\n",
    "    print(f\"  log posterior: {best_score:.6f}\")\n",
    "    for i, lg in enumerate(per_tree_logs, 1):\n",
    "        print(f\"  Tree {i} log P(T|F*): {lg:.6f}\")\n",
    "\n",
    "    # ---- Ground truth scoring ----\n",
    "    predicted_sets = {p for p in bestF.Z_active if len(p) > 1}\n",
    "    ground_truth_sets, gt_loss = score_given_map_and_trees(\n",
    "        os.path.join(folder, \"main.txt\"),\n",
    "        trees,\n",
    "        meta_paths,\n",
    "        fixed_k=priors.fixed_k\n",
    "    )\n",
    "\n",
    "    print(\"\\nPredicted Sets:\")\n",
    "    pretty_print_sets(\"Predicted Sets\", predicted_sets)\n",
    "    print(\"\\nGround Truth Sets:\")\n",
    "    pretty_print_sets(\"Ground Truth Sets\", ground_truth_sets)\n",
    "\n",
    "    # ---- Jaccard distance ----\n",
    "    jd = jaccard_distance(predicted_sets, ground_truth_sets)\n",
    "    print(\"\\n=== Jaccard Distance ===\")\n",
    "    print(f\"Jaccard Distance (Predicted vs Ground Truth): {jd:.6f}\")\n",
    "    print(f\"Predicted map's loss: {best_score:.6f}\")\n",
    "    print(f\"Ground truth's loss: {gt_loss:.6f}\")\n",
    "\n",
    "    return jd, gt_loss, best_score\n",
    "\n",
    "\n",
    "def main():\n",
    "    random.seed(7)\n",
    "    folders = [\"0002\",\"0003\",\"0004\",\"0005\",\"0006\",\"0007\",\"0008\",\"0009\",\"0010\",\"0011\"]  # <-- specify your folders\n",
    "    priors = Priors(potency_mode=\"fixed_k\", fixed_k=5, rho=0.2)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for folder in folders:\n",
    "        print(f\"\\n================= Processing folder {folder} =================\")\n",
    "        try:\n",
    "            jd, gt_loss, pred_loss = process_folder(folder = folder, priors = priors, iters=50, restarts=2)\n",
    "            results.append((folder, jd, gt_loss, pred_loss))\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing {folder}: {e}\")\n",
    "            results.append((folder, None, None, None))\n",
    "\n",
    "    # ---- Print summary table ----\n",
    "    print(\"\\n================= Summary Table =================\")\n",
    "    print(f\"{'Folder':<10} {'Jaccard Dist':<15} {'GT Loss':<15} {'Pred Loss':<15}\")\n",
    "    for folder, jd, gt_loss, pred_loss in results:\n",
    "        if jd is None:\n",
    "            print(f\"{folder:<10} {'ERROR':<15} {'ERROR':<15} {'ERROR':<15}\")\n",
    "        else:\n",
    "            print(f\"{folder:<10} {jd:<15.6f} {gt_loss:<15.6f} {pred_loss:<15.6f}\")\n",
    "\n",
    "    # ---- Save summary to CSV ----\n",
    "    output_file = \"summary_results.csv\"\n",
    "    with open(output_file, mode=\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Folder\", \"Jaccard Distance\", \"Ground Truth Loss\", \"Predicted Map Loss\"])\n",
    "        for folder, jd, gt_loss, pred_loss in results:\n",
    "            writer.writerow([folder, jd, gt_loss, pred_loss])\n",
    "\n",
    "    print(f\"\\nSummary saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ffb70-12d9-45ed-9fce-e991ec9718a8",
   "metadata": {},
   "source": [
    "Things to do here -\n",
    "ii) Modified Fitch initialization\n",
    "iii) Improve the search function\n",
    "\n",
    "Interesting observations - \n",
    "i) Jaccard distance does not even consider the edges\n",
    "\n",
    "Imp notes about stuff done till now - \n",
    "i) Used modified Fitch algorithm to initialize the potency sets only, the edges are still chosen randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15ace77b-8a84-44ae-85da-e8d8a2f82aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ground Truth Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -98.147510\n",
      "Tree 2: log P(T|F) = -85.047742\n",
      "Tree 3: log P(T|F) = -81.850154\n",
      "Tree 4: log P(T|F) = -60.188143\n",
      "Tree 5: log P(T|F) = -106.832160\n",
      "Total log-likelihood = -432.065708\n",
      "\n",
      "Predicted Sets:\n",
      "   ['-14', '-7']\n",
      "   ['-7', '-8']\n",
      "   ['-14', '-4', '-9']\n",
      "   ['-1', '-14', '-7', '-8']\n",
      "   ['-1', '-14', '-4', '-7', '-8', '-9']\n",
      "\n",
      "Ground Truth Sets:\n",
      "   ['-14', '-7']\n",
      "   ['-7', '-8']\n",
      "   ['-14', '-7', '-8']\n",
      "   ['-1', '-14', '-7', '-8']\n",
      "   ['-1', '-14', '-4', '-7', '-8', '-9']\n",
      "\n",
      "=== Jaccard Distance ===\n",
      "Jaccard Distance (Predicted vs Ground Truth): 0.333333\n"
     ]
    }
   ],
   "source": [
    "def score_given_map_and_trees(txt_path: str, trees, meta_paths,\n",
    "                              unit_drop_edges = False):\n",
    "    \"\"\"\n",
    "    Parses the input file and builds the structure F=(Z,A),\n",
    "    then scores the log-likelihood of the given trees.\n",
    "    Returns:\n",
    "        potency_sets (set of frozenset): all potency states\n",
    "        total_ll (float): total log-likelihood across trees\n",
    "    \"\"\"\n",
    "    objs = _read_json_objects_exact(txt_path)\n",
    "    if len(objs) < 4:\n",
    "        raise ValueError(\"Expected at least 4 JSON lines (adjacency, weights, composition map, root).\")\n",
    "\n",
    "    # 1) adjacency\n",
    "    adj = None\n",
    "    for o in objs:\n",
    "        if isinstance(o, dict) and any(isinstance(v, list) for v in o.values()):\n",
    "            adj = {str(k): [str(x) for x in v] for k, v in o.items() if isinstance(v, list)}\n",
    "            break\n",
    "    if adj is None:\n",
    "        raise ValueError(\"Could not locate adjacency dict in the file.\")\n",
    "\n",
    "    # 2) composition map\n",
    "    comp_map = objs[2]\n",
    "    if not isinstance(comp_map, dict):\n",
    "        raise ValueError(\"Third JSON must be the composition map (dict).\")\n",
    "\n",
    "    # 3) root id\n",
    "    root_id = objs[3]\n",
    "    if isinstance(root_id, dict) and \"root_id\" in root_id:\n",
    "        root_id = root_id[\"root_id\"]\n",
    "    root_id = str(root_id)\n",
    "\n",
    "    # Print vertices and edges\n",
    "    V, E = _extract_vertices_edges_from_adj(adj)\n",
    "    # print(\"=== Parsed Graph: Vertices ===\")\n",
    "    # for v in V: \n",
    "    #     print(\" \", v)\n",
    "    # print(\"\\n=== Parsed Graph: Edges (u -> v) ===\")\n",
    "    # for u, v in E: \n",
    "    #     print(f\"  {u} -> {v}\")\n",
    "\n",
    "    # Build Z, A, and potency definitions\n",
    "    Z_from_map, A_from_map, base_types_map, potency_def = _build_ZA_from_txt(\n",
    "        adj=adj,\n",
    "        comp_map=comp_map,\n",
    "        unit_drop_edges=unit_drop_edges\n",
    "    )\n",
    "\n",
    "    # Print potency definitions\n",
    "    # print(\"\\n=== Potency definitions (expanded) ===\")\n",
    "    # for pid in sorted(potency_def, key=lambda x: (len(x), x)):\n",
    "    #     s = \",\".join(sorted(potency_def[pid]))\n",
    "    #     print(f\"  {pid} := {{{s}}}\")\n",
    "\n",
    "    # Load trees and leaf maps\n",
    "    # trees = [\n",
    "    #     read_newick_file(\"./0002_tree_0.txt\"),\n",
    "    #     read_newick_file(\"./0002_tree_1.txt\"),\n",
    "    #     read_newick_file(\"./0002_tree_2.txt\"),\n",
    "    #     read_newick_file(\"./0002_tree_3.txt\"),\n",
    "    #     read_newick_file(\"./0002_tree_4.txt\")\n",
    "    # ]\n",
    "    # meta_paths = [\n",
    "    #     \"./0002_meta_0.txt\",\n",
    "    #     \"./0002_meta_1.txt\",\n",
    "    #     \"./0002_meta_2.txt\",\n",
    "    #     \"./0002_meta_3.txt\",\n",
    "    #     \"./0002_meta_4.txt\"\n",
    "    # ]\n",
    "    raw_maps = [read_leaf_type_map(p) for p in meta_paths]\n",
    "    leaf_type_maps = [filter_leaf_map_to_tree(root, m) for root, m in zip(trees, raw_maps)]\n",
    "    base_types_data = sorted({str(t) for m in leaf_type_maps for t in m.values()})\n",
    "\n",
    "    # Merge sets for structure\n",
    "    S_all = sorted(set(base_types_map) | set(base_types_data))\n",
    "    Z_active = set(Z_from_map) | {frozenset([t]) for t in S_all}\n",
    "    A = dict(A_from_map)\n",
    "\n",
    "    struct = Structure(S=S_all, Z_active=Z_active, A=A, unit_drop=unit_drop_edges)\n",
    "    dummy_priors = Priors(potency_mode=\"fixed_k\", fixed_k=5, rho=0.2)\n",
    "\n",
    "    log_post, per_tree_logs = score_structure(\n",
    "        struct=struct,\n",
    "        trees=trees,\n",
    "        leaf_type_maps=leaf_type_maps,\n",
    "        priors=dummy_priors,\n",
    "        prune_eps=0.0\n",
    "    )\n",
    "\n",
    "    total_ll = sum(per_tree_logs)\n",
    "\n",
    "    print(\"\\n=== Ground Truth Log-likelihoods (given F from map) ===\")\n",
    "    for i, lg in enumerate(per_tree_logs, 1):\n",
    "        print(f\"Tree {i}: log P(T|F) = {lg:.6f}\")\n",
    "    print(f\"Total log-likelihood = {total_ll:.6f}\")\n",
    "\n",
    "    # Convert potency_def dict to set of frozensets\n",
    "    potency_sets = {frozenset(members) for members in potency_def.values()}\n",
    "\n",
    "    return potency_sets, total_ll\n",
    "\n",
    "trees = [\n",
    "    read_newick_file(\"./0002_tree_0.txt\"),\n",
    "    read_newick_file(\"./0002_tree_1.txt\"),\n",
    "    read_newick_file(\"./0002_tree_2.txt\"),\n",
    "    read_newick_file(\"./0002_tree_3.txt\"),\n",
    "    read_newick_file(\"./0002_tree_4.txt\")\n",
    "]\n",
    "meta_paths = [\n",
    "    \"./0002_meta_0.txt\",\n",
    "    \"./0002_meta_1.txt\",\n",
    "    \"./0002_meta_2.txt\",\n",
    "    \"./0002_meta_3.txt\",\n",
    "    \"./0002_meta_4.txt\"\n",
    "]\n",
    "\n",
    "    \n",
    "\n",
    "predicted_sets = {\n",
    "    frozenset({'-7','-14'}),\n",
    "    frozenset({'-7', '-8'}),\n",
    "    frozenset({'-14', '-4', '-9'}),\n",
    "    frozenset({'-1', '-14', '-7', '-8'}),\n",
    "    frozenset({'-1', '-14', '-4', '-7', '-8', '-9'})\n",
    "}\n",
    "ground_truth_sets, sc = score_given_map_and_trees(\"main.txt\", trees, meta_paths)\n",
    "\n",
    "\n",
    "def jaccard_distance(set1, set2):\n",
    "    if not set1 and not set2:\n",
    "        return 0.0\n",
    "    return 1 - len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "\n",
    "def pretty_print_sets(name, sets):\n",
    "    print(f\"\\n{name}:\")\n",
    "    for s in sorted(sets, key=lambda x: (len(x), sorted(x))):\n",
    "        print(\"  \", sorted(list(s)))\n",
    "\n",
    "\n",
    "# Print predicted and ground truth sets\n",
    "pretty_print_sets(\"Predicted Sets\", predicted_sets)\n",
    "pretty_print_sets(\"Ground Truth Sets\", ground_truth_sets)\n",
    "\n",
    "# Compute and print Jaccard distance\n",
    "jd = jaccard_distance(predicted_sets, ground_truth_sets)\n",
    "print(\"\\n=== Jaccard Distance ===\")\n",
    "print(f\"Jaccard Distance (Predicted vs Ground Truth): {jd:.6f}\")\n",
    "\n",
    "# print(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82bc859c-1812-4313-820f-3dc1e29ac051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated (normalized) transitions:\n",
      "{'A', 'B', 'C'} -> {'A'} : 0.3333\n",
      "{'A', 'B', 'C'} -> {'B'} : 0.3333\n",
      "{'A', 'B', 'C'} -> {'C'} : 0.3333\n",
      "{'A', 'D', 'B', 'C'} -> {'A'} : 0.2000\n",
      "{'A', 'D', 'B', 'C'} -> {'B', 'C'} : 0.2000\n",
      "{'A', 'D', 'B', 'C'} -> {'D'} : 0.2000\n",
      "{'B', 'C'} -> {'B'} : 0.2000\n",
      "{'B', 'C'} -> {'C'} : 0.2000\n",
      "\n",
      "Z initialization (ROOT + top k-1):\n",
      "1: {'A', 'D', 'B', 'C'}\n",
      "2: {'A', 'B', 'C'}\n",
      "3: {'B', 'C'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple, Iterable, Optional, Any, Set\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "\n",
    "# -------------------------\n",
    "# Provided TreeNode class\n",
    "# -------------------------\n",
    "class TreeNode:\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        self.name: Optional[str] = name\n",
    "        self.children: List[\"TreeNode\"] = []\n",
    "        self.parent: Optional[\"TreeNode\"] = None\n",
    "        self.potency: Optional[Set[str]] = None  # to store assigned state\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def add_child(self, child: \"TreeNode\"):\n",
    "        self.children.append(child)\n",
    "        child.parent = self\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Leaf({self.name})\" if self.is_leaf() else f\"Node({self.potency}, k={len(self.children)})\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Core utilities\n",
    "# -------------------------\n",
    "def iter_edges(root: TreeNode) -> Iterable[Tuple[TreeNode, TreeNode]]:\n",
    "    \"\"\"Yield (parent, child) for every directed edge in the rooted tree.\"\"\"\n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        for child in node.children:\n",
    "            yield (node, child)\n",
    "            stack.append(child)\n",
    "\n",
    "\n",
    "def count_edges(root: TreeNode) -> int:\n",
    "    \"\"\"Count number of directed edges in tree rooted at `root`.\"\"\"\n",
    "    return sum(1 for _ in iter_edges(root))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Union-only Fitch labeling\n",
    "# -------------------------\n",
    "def assign_union_potency(root: TreeNode, leaf_type_map: Dict[str, str]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Post-order union-only labeling. Sets `node.potency` for every node (as a Python set).\n",
    "    For leaves, looks up leaf_type_map[node.name] to get the leaf cell type.\n",
    "    Returns the potency set at `root`.\n",
    "    \"\"\"\n",
    "    if root.is_leaf():\n",
    "        if root.name is None:\n",
    "            raise KeyError(\"Leaf has no .name; cannot map to leaf_type_map\")\n",
    "        if root.name not in leaf_type_map:\n",
    "            raise KeyError(f\"Leaf name '{root.name}' not found in leaf_type_map\")\n",
    "        root.potency = {leaf_type_map[root.name]}\n",
    "        return root.potency\n",
    "\n",
    "    union_set: Set[str] = set()\n",
    "    for child in root.children:\n",
    "        child_set = assign_union_potency(child, leaf_type_map)\n",
    "        union_set |= child_set\n",
    "    root.potency = union_set\n",
    "    return root.potency\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Per-tree transition counts\n",
    "# -------------------------\n",
    "def per_tree_transition_counts(root: TreeNode) -> Counter:\n",
    "    \"\"\"\n",
    "    Count transitions (parent_set -> child_set) for all direct edges in the tree,\n",
    "    excluding edges where parent.potency == child.potency.\n",
    "    Returns Counter with keys (frozenset_parent, frozenset_child) -> count (int).\n",
    "    \"\"\"\n",
    "    C = Counter()\n",
    "    for (u, v) in iter_edges(root):\n",
    "        su = frozenset(u.potency if u.potency is not None else set())\n",
    "        sv = frozenset(v.potency if v.potency is not None else set())\n",
    "        if su != sv:\n",
    "            C[(su, sv)] += 1\n",
    "    return C\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Aggregation + top-k picking\n",
    "# -------------------------\n",
    "def init_progenitors_union_fitch(\n",
    "    S: List[str],\n",
    "    trees: List[TreeNode],\n",
    "    leaf_type_maps: List[Dict[str, str]],\n",
    "    k: int,\n",
    ") -> Tuple[Dict[Tuple[frozenset, frozenset], float], List[frozenset]]:\n",
    "    \"\"\"\n",
    "    Run union-Fitch on each tree, compute normalized transition counts per tree (only real transitions),\n",
    "    aggregate across trees, compute row-sums and return:\n",
    "      - aggregated_transitions: dict ( (frozenset_i, frozenset_j) -> float )\n",
    "      - Z_init: list of k frozensets, with ROOT (all leaf types) first, then top (k-1) by row-sum.\n",
    "    \"\"\"\n",
    "    if len(trees) != len(leaf_type_maps):\n",
    "        raise ValueError(\"Provide exactly one leaf_type_map per tree (same order).\")\n",
    "\n",
    "    ROOT = frozenset(S)  # absolute root potency (all leaf types)\n",
    "    aggregated_transitions: Dict[Tuple[frozenset, frozenset], float] = defaultdict(float)\n",
    "    row_sum: Dict[frozenset, float] = defaultdict(float)\n",
    "\n",
    "    for tree, ltm in zip(trees, leaf_type_maps):\n",
    "        # Assign potencies (this populates node.potency for the tree)\n",
    "        assign_union_potency(tree, ltm)\n",
    "        \n",
    "        # Count only real transitions\n",
    "        C_T = per_tree_transition_counts(tree)\n",
    "        T = sum(C_T.values())  # count only edges with actual transitions\n",
    "        \n",
    "        if T == 0:\n",
    "            # no transitions in this tree\n",
    "            continue\n",
    "        \n",
    "        # Normalize and aggregate\n",
    "        for (i_set, j_set), cnt in C_T.items():\n",
    "            incr = cnt / T  # normalize only by edges that actually transitioned\n",
    "            aggregated_transitions[(i_set, j_set)] += incr\n",
    "            row_sum[i_set] += incr\n",
    "\n",
    "\n",
    "    # Rank candidates by row_sum, exclude ROOT from ranking (but include ROOT in final Z)\n",
    "    candidates = [ps for ps in row_sum.keys() if ps != ROOT]\n",
    "\n",
    "    # If no candidates (row_sum empty), we still return ROOT and up to k-1 singletons (if possible)\n",
    "    # Sort by (row_sum desc, set size desc, lexicographic)\n",
    "    candidates.sort(key=lambda ps: (-row_sum[ps], -len(ps), tuple(sorted(ps))))\n",
    "\n",
    "    top_k_minus_root = candidates[:max(0, k - 1)]\n",
    "\n",
    "    # If there are not enough candidates, you might want to supplement with other sets:\n",
    "    # Here we supplement by adding large sets (excluding ROOT) discovered in the trees' node potencies,\n",
    "    # sorted by size (desc) then lexicographically, but only if needed.\n",
    "    if len(top_k_minus_root) < max(0, k - 1):\n",
    "        # gather all potencies observed across all trees\n",
    "        observed = set()\n",
    "        for tree in trees:\n",
    "            # traverse nodes\n",
    "            stack = [tree]\n",
    "            while stack:\n",
    "                n = stack.pop()\n",
    "                if n.potency is not None:\n",
    "                    observed.add(frozenset(n.potency))\n",
    "                stack.extend(n.children)\n",
    "        # remove ROOT and already selected\n",
    "        supplement = [ps for ps in observed if ps != ROOT and ps not in top_k_minus_root]\n",
    "        # sort supplement by size desc, then lexicographically\n",
    "        supplement = sorted(supplement, key=lambda ps: (-len(ps), tuple(sorted(ps))))\n",
    "        need = (k - 1) - len(top_k_minus_root)\n",
    "        top_k_minus_root.extend(supplement[:need])\n",
    "\n",
    "    Z_init = [ROOT] + top_k_minus_root\n",
    "    # Ensure we return at most k items\n",
    "    Z_init = Z_init[:k]\n",
    "\n",
    "    return dict(aggregated_transitions), Z_init\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example usage / test\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example leaf types S\n",
    "    S = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "    # Build tree1\n",
    "    # root1\n",
    "    #  ├ A\n",
    "    #  ├ B\n",
    "    #  └ C\n",
    "    root1 = TreeNode(\"root1\")\n",
    "    leafA = TreeNode(\"A\")\n",
    "    leafB = TreeNode(\"B\")\n",
    "    leafC = TreeNode(\"C\")\n",
    "    root1.add_child(leafA)\n",
    "    root1.add_child(leafB)\n",
    "    root1.add_child(leafC)\n",
    "\n",
    "    ltm1 = { \"A\": \"A\", \"B\": \"B\", \"C\": \"C\" }  # leaf_name -> cell type (string)\n",
    "\n",
    "    # Build tree2\n",
    "    # root2\n",
    "    #  ├ (A)\n",
    "    #  ├ D\n",
    "    #  └ internal_x\n",
    "    #       ├ B\n",
    "    #       └ C\n",
    "    root2 = TreeNode(\"root2\")\n",
    "    leafA2 = TreeNode(\"A\")\n",
    "    leafD = TreeNode(\"D\")\n",
    "    internal_x = TreeNode(\"x\")\n",
    "    leafB2 = TreeNode(\"B\")\n",
    "    leafC2 = TreeNode(\"C\")\n",
    "    internal_x.add_child(leafB2)\n",
    "    internal_x.add_child(leafC2)\n",
    "    root2.add_child(leafA2)\n",
    "    root2.add_child(leafD)\n",
    "    root2.add_child(internal_x)\n",
    "\n",
    "    ltm2 = { \"A\": \"A\", \"D\": \"D\", \"B\": \"B\", \"C\": \"C\" }\n",
    "\n",
    "    trees = [root1, root2]\n",
    "    leaf_type_maps = [ltm1, ltm2]\n",
    "    k = 3\n",
    "\n",
    "    aggregated_transitions, Z_init = init_progenitors_union_fitch(S, trees, leaf_type_maps, k)\n",
    "\n",
    "    print(\"Aggregated (normalized) transitions:\")\n",
    "    for (p, c), v in sorted(aggregated_transitions.items(), key=lambda x: (-x[1], tuple(sorted(x[0][0])), tuple(sorted(x[0][1])))):\n",
    "        print(f\"{set(p)} -> {set(c)} : {v:.4f}\")\n",
    "\n",
    "    print(\"\\nZ initialization (ROOT + top k-1):\")\n",
    "    for idx, z in enumerate(Z_init):\n",
    "        print(f\"{idx+1}: {set(z)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "084f9679-9e01-4067-8f46-21f14dd8b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select mode:\n",
      "  1) Run MAP search demo (uses files in code)\n",
      "  2) Score a given TXT map (compute log-likelihood only)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 1 or 2:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: TXT path required.\n",
      "\n",
      "Parsing and scoring...\n",
      "\n",
      "=== Parsed Graph: Vertices ===\n",
      "  6\n",
      "  9\n",
      "  -1\n",
      "  -4\n",
      "  -7\n",
      "  -8\n",
      "  -9\n",
      "  10\n",
      "  12\n",
      "  15\n",
      "  -14\n",
      "  root\n",
      "\n",
      "=== Parsed Graph: Edges (u -> v) ===\n",
      "  10 -> -14\n",
      "  10 -> 6\n",
      "  12 -> -1\n",
      "  12 -> 10\n",
      "  15 -> -4\n",
      "  15 -> -9\n",
      "  15 -> 12\n",
      "  15 -> 9\n",
      "  6 -> -7\n",
      "  6 -> -8\n",
      "  9 -> -14\n",
      "  9 -> -7\n",
      "  root -> 15\n",
      "\n",
      "=== Potency definitions (expanded) ===\n",
      "  6 := {-7,-8}\n",
      "  9 := {-14,-7}\n",
      "  10 := {-14,-7,-8}\n",
      "  12 := {-1,-14,-7,-8}\n",
      "  15 := {-1,-14,-4,-7,-8,-9}\n",
      "\n",
      "=== Log-likelihoods (given F from map) ===\n",
      "Tree 1: log P(T|F) = -98.147510\n",
      "Tree 2: log P(T|F) = -85.047742\n",
      "Tree 3: log P(T|F) = -81.850154\n",
      "Tree 4: log P(T|F) = -60.188143\n",
      "Tree 5: log P(T|F) = -106.832160\n",
      "Total log-likelihood = -432.065708\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "MAP structure search for Carta-CDMIP model.\n",
    "Score(F) = log P(F) + sum_T log P(T|F),\n",
    "where P(T|F) = sum_{labelings} B(O+1, D+1), with per-node counts:\n",
    "  obs = |L ∩ B(v)|, miss = |L \\ B(v)|, and B(·) is Beta function.\n",
    "p ~ Beta(1,1) is integrated out exactly.\n",
    "- Newick parser (no external deps)\n",
    "- DP over labelings with (O,D) sparse tables\n",
    "- Priors: fixed-k (uniform over potency sets) OR Bernoulli(pi_P); edges Bernoulli(rho)\n",
    "- Stochastic hill-climb + simulated annealing over F=(Z,A)\n",
    "\"\"\"\n",
    "from tqdm import trange\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Tuple, List, Optional, Set, FrozenSet\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# Tree structures and Newick\n",
    "# ----------------------------\n",
    "class TreeNode:\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        self.name: Optional[str] = name\n",
    "        self.children: List[\"TreeNode\"] = []\n",
    "        self.parent: Optional[\"TreeNode\"] = None\n",
    "    def is_leaf(self): return len(self.children) == 0\n",
    "    def add_child(self, child: \"TreeNode\"):\n",
    "        self.children.append(child); child.parent = self\n",
    "    def __repr__(self):\n",
    "        return f\"Leaf({self.name})\" if self.is_leaf() else f\"Node({self.name}, k={len(self.children)})\"\n",
    "\n",
    "# def parse_newick(newick: str) -> TreeNode:\n",
    "#     s = newick.strip()\n",
    "#     if not s.endswith(\";\"): raise ValueError(\"Newick must end with ';'\")\n",
    "#     s = s[:-1]; i = 0\n",
    "#     def parse() -> TreeNode:\n",
    "#         nonlocal i, s\n",
    "#         if i >= len(s): raise ValueError(\"Unexpected end\")\n",
    "#         if s[i] == '(':\n",
    "#             i += 1\n",
    "#             node = TreeNode()\n",
    "#             while True:\n",
    "#                 node.add_child(parse())\n",
    "#                 if i >= len(s): raise ValueError(\"Unbalanced\")\n",
    "#                 if s[i] == ',':\n",
    "#                     i += 1; continue\n",
    "#                 elif s[i] == ')':\n",
    "#                     i += 1; break\n",
    "#                 else: raise ValueError(f\"Unexpected char: {s[i]} at {i}\")\n",
    "#             j = i\n",
    "#             while j < len(s) and s[j] not in ',()': j += 1\n",
    "#             name = s[i:j].strip()\n",
    "#             if name: node.name = name\n",
    "#             i = j\n",
    "#             return node\n",
    "#         else:\n",
    "#             j = i\n",
    "#             while j < len(s) and s[j] not in ',()': j += 1\n",
    "#             name = s[i:j].strip()\n",
    "#             if not name: raise ValueError(\"Leaf without name\")\n",
    "#             i = j\n",
    "#             return TreeNode(name=name)\n",
    "#     root = parse()\n",
    "#     if i != len(s): raise ValueError(f\"Trailing characters: '{s[i:]}'\")\n",
    "#     return root\n",
    "def parse_newick(newick: str) -> TreeNode:\n",
    "    # Helper: strip branch length and numeric-only labels\n",
    "    def _clean_label(tok: str) -> str:\n",
    "        # remove branch length: keep part before first ':'\n",
    "        tok = tok.split(\":\", 1)[0].strip()\n",
    "        # drop pure numeric internal labels like \"357\"\n",
    "        if tok and tok.replace(\".\", \"\", 1).isdigit():\n",
    "            return \"\"\n",
    "        return tok\n",
    "    s = newick.strip()\n",
    "    if not s.endswith(\";\"): raise ValueError(\"Newick must end with ';'\")\n",
    "    s = s[:-1]; i = 0\n",
    "    def parse() -> TreeNode:\n",
    "        nonlocal i, s\n",
    "        if i >= len(s): raise ValueError(\"Unexpected end\")\n",
    "        if s[i] == '(':\n",
    "            i += 1\n",
    "            node = TreeNode()\n",
    "            while True:\n",
    "                node.add_child(parse())\n",
    "                if i >= len(s): raise ValueError(\"Unbalanced\")\n",
    "                if s[i] == ',':\n",
    "                    i += 1; continue\n",
    "                elif s[i] == ')':\n",
    "                    i += 1; break\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected char: {s[i]} at {i}\")\n",
    "            # optional internal node label (may include branch length)\n",
    "            j = i\n",
    "            while j < len(s) and s[j] not in ',()': j += 1\n",
    "            name_raw = s[i:j].strip()\n",
    "            name = _clean_label(name_raw)\n",
    "            if name:  # keep non-empty, non-numeric labels only\n",
    "                node.name = name\n",
    "            i = j\n",
    "            return node\n",
    "        else:\n",
    "            # leaf label (may include branch length)\n",
    "            j = i\n",
    "            while j < len(s) and s[j] not in ',()': j += 1\n",
    "            name_raw = s[i:j].strip()\n",
    "            name = _clean_label(name_raw)\n",
    "            if not name:\n",
    "                raise ValueError(\"Leaf without name\")\n",
    "            i = j\n",
    "            return TreeNode(name=name)\n",
    "    root = parse()\n",
    "    if i != len(s): raise ValueError(f\"Trailing characters: '{s[i:]}'\")\n",
    "    return root\n",
    "def to_newick(root: TreeNode) -> str:\n",
    "    def rec(n: TreeNode) -> str:\n",
    "        if n.is_leaf(): return n.name or \"\"\n",
    "        return f\"({','.join(rec(c) for c in n.children)}){n.name or ''}\"\n",
    "    return rec(root) + \";\"\n",
    "\n",
    "def read_newick_file(path: str) -> TreeNode:\n",
    "    with open(path, \"r\") as f: s = f.read().strip()\n",
    "    return parse_newick(s)\n",
    "def write_newick_file(path: str, root: TreeNode):\n",
    "    with open(path, \"w\") as f: f.write(to_newick(root) + \"\\n\")\n",
    "def random_tree_newick(n_leaves: int, leaf_prefix=\"L\") -> Tuple[TreeNode, List[str]]:\n",
    "    leaves = [TreeNode(f\"{leaf_prefix}{i+1}\") for i in range(n_leaves)]\n",
    "    nodes = leaves[:]\n",
    "    while len(nodes) > 1:\n",
    "        k = 2 if len(nodes) < 4 else random.choice([2,2,2,3])\n",
    "        k = min(k, len(nodes))\n",
    "        picks = random.sample(nodes, k)\n",
    "        for p in picks: nodes.remove(p)\n",
    "        parent = TreeNode()\n",
    "        for p in picks: parent.add_child(p)\n",
    "        nodes.append(parent)\n",
    "    return nodes[0], [l.name for l in leaves]\n",
    "def collect_leaf_names(root: TreeNode) -> List[str]:\n",
    "    out=[]\n",
    "    def dfs(v):\n",
    "        if v.is_leaf(): out.append(v.name)\n",
    "        else:\n",
    "            for c in v.children: dfs(c)\n",
    "    dfs(root); return out\n",
    "# ----------------------------\n",
    "# Potency universe and structure\n",
    "# ----------------------------\n",
    "def all_nonempty_subsets(S: List[str], max_size: Optional[int]=None) -> List[FrozenSet[str]]:\n",
    "    R=len(S); max_k = R if max_size is None else min(max_size, R)\n",
    "    res=[]\n",
    "    for k in range(1, max_k+1):\n",
    "        for comb in itertools.combinations(S, k): res.append(frozenset(comb))\n",
    "    return res\n",
    "def singletons(S: List[str]) -> Set[FrozenSet[str]]:\n",
    "    return {frozenset([t]) for t in S}\n",
    "def build_Z_active(S: List[str], fixed_k: Optional[int], max_potency_size: Optional[int], seed=0) -> Set[FrozenSet[str]]:\n",
    "    rng = random.Random(seed)\n",
    "    P_all = all_nonempty_subsets(S, max_potency_size)\n",
    "    singles = singletons(S)\n",
    "    multis = [P for P in P_all if len(P)>=2]\n",
    "    Z = set(singles)\n",
    "    if fixed_k is not None:\n",
    "        if fixed_k > len(multis): raise ValueError(\"fixed_k too large\")\n",
    "        Z.update(rng.sample(multis, fixed_k))\n",
    "    else:\n",
    "        Z.update(multis)\n",
    "    return Z\n",
    "def admissible_edge(P: FrozenSet[str], Q: FrozenSet[str], unit_drop: bool) -> bool:\n",
    "    if Q == P: return False\n",
    "    if not Q.issubset(P): return False\n",
    "    if len(Q) >= len(P): return False\n",
    "    if unit_drop and len(P - Q) != 1: return False\n",
    "    return True\n",
    "def build_edges(Z_active: Set[FrozenSet[str]], forbid_fn=None, unit_drop=True) -> Dict[Tuple[FrozenSet[str],FrozenSet[str]], int]:\n",
    "    A={}\n",
    "    for P in Z_active:\n",
    "        for Q in Z_active:\n",
    "            if not admissible_edge(P,Q,unit_drop): continue\n",
    "            if forbid_fn and forbid_fn(P,Q): continue\n",
    "            A[(P,Q)] = 1\n",
    "    return A\n",
    "def transitive_closure(labels: List[FrozenSet[str]], A: Dict[Tuple[FrozenSet[str],FrozenSet[str]], int]) -> Dict[FrozenSet[str], Set[FrozenSet[str]]]:\n",
    "    idx = {L:i for i,L in enumerate(labels)}\n",
    "    n=len(labels)\n",
    "    M=[[False]*n for _ in range(n)]\n",
    "    for i in range(n): M[i][i]=True\n",
    "    for (P,Q),v in A.items():\n",
    "        if v:\n",
    "            i,j=idx[P],idx[Q]; M[i][j]=True\n",
    "    for k in range(n):\n",
    "        Mk=M[k]\n",
    "        for i in range(n):\n",
    "            if M[i][k]:\n",
    "                Mi=M[i]\n",
    "                for j in range(n):\n",
    "                    if Mk[j]: Mi[j]=True\n",
    "    Reach={L:set() for L in labels}\n",
    "    for i,L in enumerate(labels):\n",
    "        for j,U in enumerate(labels):\n",
    "            if M[i][j]: Reach[L].add(U)\n",
    "    return Reach\n",
    "# ----------------------------\n",
    "# DP over labelings (integrated Beta)\n",
    "# ----------------------------\n",
    "def compute_B_sets(root: TreeNode, leaf_to_type: Dict[str,str]) -> Dict[TreeNode, Set[str]]:\n",
    "    B={}\n",
    "    def post(v: TreeNode) -> Set[str]:\n",
    "        if v.is_leaf():\n",
    "            t = leaf_to_type.get(v.name)\n",
    "            # Missing mapping? Ignore this leaf by contributing an empty set.\n",
    "            B[v] = {t} if t is not None else set()\n",
    "            return B[v]\n",
    "        acc=set()\n",
    "        for c in v.children: acc |= post(c)\n",
    "        B[v]=acc; return acc\n",
    "    post(root); return B\n",
    "def beta_integral(O:int,D:int)->float:\n",
    "    # ∫ p^O (1-p)^D dp over [0,1] = B(O+1,D+1)\n",
    "    return math.exp(math.lgamma(O+1)+math.lgamma(D+1)-math.lgamma(O+D+2))\n",
    "def sparse_convolve_2d(A: Dict[Tuple[int,int],float], B: Dict[Tuple[int,int],float]) -> Dict[Tuple[int,int],float]:\n",
    "    if not A: return B.copy()\n",
    "    if not B: return A.copy()\n",
    "    out=defaultdict(float)\n",
    "    for (o1,d1),w1 in A.items():\n",
    "        for (o2,d2),w2 in B.items():\n",
    "            out[(o1+o2,d1+d2)] += w1*w2\n",
    "    return dict(out)\n",
    "def dp_tree_root_table(\n",
    "    root: TreeNode,\n",
    "    active_labels: List[FrozenSet[str]],\n",
    "    Reach: Dict[FrozenSet[str], Set[FrozenSet[str]]],\n",
    "    B_sets: Dict[TreeNode, Set[str]],\n",
    "    prune_eps: float = 0.0\n",
    ")->Dict[Tuple[int,int],float]:\n",
    "    label_index={L:i for i,L in enumerate(active_labels)}\n",
    "    memo: Dict[Tuple[int,int], Dict[Tuple[int,int],float]]={}\n",
    "    def nid(v:TreeNode)->int: return id(v)\n",
    "    def M(v:TreeNode, P: Optional[FrozenSet[str]])->Dict[Tuple[int,int],float]:\n",
    "        key=(nid(v), -1 if P is None else label_index[P])\n",
    "        if key in memo: return memo[key]\n",
    "        if v.is_leaf():\n",
    "            memo[key] = {(0,0):1.0}; return memo[key]\n",
    "        Bv=B_sets[v]\n",
    "        out=defaultdict(float)\n",
    "        if P is None:\n",
    "            parent_reach = active_labels\n",
    "        else:\n",
    "            parent_reach = list(Reach[P])\n",
    "        for L in parent_reach:\n",
    "            if not Bv.issubset(L):  # containment constraint\n",
    "                continue\n",
    "            o_local=len(L & Bv); d_local=len(L - Bv)\n",
    "            # children messages conditioned on parent label = L\n",
    "            child_tabs=[]\n",
    "            ok=True\n",
    "            for u in v.children:\n",
    "                tab = M(u, L)\n",
    "                if not tab: ok=False; break\n",
    "                child_tabs.append(tab)\n",
    "            if not ok: continue\n",
    "            conv = child_tabs[0] if child_tabs else {(0,0):1.0}\n",
    "            for t in child_tabs[1:]:\n",
    "                conv = sparse_convolve_2d(conv, t)\n",
    "            for (Oc,Dc),w in conv.items():\n",
    "                out[(Oc+o_local, Dc+d_local)] += w\n",
    "        if prune_eps>0 and out:\n",
    "            total=sum(out.values()); thresh=prune_eps*total\n",
    "            out={k:v for k,v in out.items() if v>=thresh}\n",
    "        memo[key]=dict(out); return memo[key]\n",
    "    return M(root, None)\n",
    "def tree_marginal_from_root_table(C: Dict[Tuple[int,int],float])->float:\n",
    "    return sum(w * beta_integral(O,D) for (O,D),w in C.items())\n",
    "# ----------------------------\n",
    "# Priors and scoring\n",
    "# ----------------------------\n",
    "class Priors:\n",
    "    def __init__(self,\n",
    "                 potency_mode:str=\"fixed_k\",  # \"fixed_k\" or \"bernoulli\"\n",
    "                 fixed_k:int=2,\n",
    "                 pi_P:float=0.25,    # used if potency_mode=\"bernoulli\"\n",
    "                 rho:float=0.25):    # edge Bernoulli prob\n",
    "        # ------------------------------------------------------------\n",
    "        # Stores hyperparameters for the prior over the structure F=(Z,A)\n",
    "        #     Z: The latent assignment of \"potencies\" or features to nodes (the sets like {A,B,C}, {B,C,D}, etc. that you saw in the MAP output).\n",
    "        #     A: The active structure (the adjacency or edge set) consistent with those potencies -- basically the graph/hypergraph that the algorithm thinks best explains the observed trees.\n",
    "        #   - potency_mode: which prior to use over active multi-type potencies Z\n",
    "        #       * \"fixed_k\": exactly k multi-type potencies are active (uniform over choices)\n",
    "        #       * \"bernoulli\": each multi-type potency is independently active with prob pi_P\n",
    "        #   - fixed_k: number of multi-type potencies when potency_mode == \"fixed_k\"\n",
    "        #   - pi_P: inclusion probability for each multi-type potency when using \"bernoulli\" mode\n",
    "        #   - rho: prior probability that any admissible edge (P->Q) exists\n",
    "        # ------------------------------------------------------------\n",
    "        self.potency_mode=potency_mode\n",
    "        self.fixed_k=fixed_k\n",
    "        self.pi_P=pi_P\n",
    "        self.rho=rho\n",
    "    def log_prior_Z(self, S: List[str], Z_active:Set[FrozenSet[str]])->float: #Z_active = the set of active potencies (both singletons and multis).\n",
    "        # ------------------------------------------------------------\n",
    "        # Computes log P(Z): the log prior over WHICH potencies are active.\n",
    "        #\n",
    "        # Inputs:\n",
    "        #   - S: list of all cell types (leaf types), e.g., [\"A\",\"B\",\"C\",\"D\"]\n",
    "        #   - Z_active: set of active potencies (as frozensets). Includes singletons by construction.\n",
    "        #\n",
    "        # Key ideas:\n",
    "        #   - Singletons are always considered active (terminal states), we don't penalize/score them.\n",
    "        #   - We only place a prior over multi-type potencies (size >= 2).\n",
    "        #   - Two modes:\n",
    "        #       * \"fixed_k\": valid only if exactly `fixed_k` multis are active.\n",
    "        #                    Prior is uniform over all C(M, k) choices, where M = #all possible multis.\n",
    "        #       * \"bernoulli\": each possible multi is included independently with prob pi_P.\n",
    "        #                      log prior sums log(pi_P) for included multis and log(1-pi_P) for excluded ones.\n",
    "        # Returns:\n",
    "        #   - log prior (float), or -inf if configuration violates \"fixed_k\".\n",
    "        # ------------------------------------------------------------\n",
    "        singles = singletons(S)\n",
    "        multis = [P for P in Z_active if len(P)>=2] #P is a particular potency set\n",
    "        # count available multi potencies (for fixed-k uniform)\n",
    "        all_multis = [P for P in all_nonempty_subsets(S) if len(P)>=2]\n",
    "        if self.potency_mode==\"fixed_k\":\n",
    "            # ------------------------------\n",
    "            # Uniform prior over all subsets of multi-type potencies with EXACTLY k elements.\n",
    "            # If the current Z_active has not exactly k multis, return -inf (outside prior support).\n",
    "            # Otherwise, log prior = -log( number of such subsets ) = -log( nCk ).\n",
    "            # ------------------------------\n",
    "            k=len(multis)\n",
    "            if k!=self.fixed_k:\n",
    "                return float(\"-inf\")\n",
    "            # uniform over all C(|all_multis|, k)\n",
    "            total = math.comb(len(all_multis), k) #this is nCk\n",
    "            return -math.log(total) if total>0 else float(\"-inf\")\n",
    "        else:\n",
    "            # ------------------------------\n",
    "            # Bernoulli prior on each multi-type potency:\n",
    "            #   P(Z) = ∏_{P in all_multis} pi_P^{I[P in Z]} (1 - pi_P)^{I[P not in Z]}\n",
    "            # We sum logs across all possible multi-type potencies (singletons ignored).\n",
    "            # ------------------------------\n",
    "            k_log=0.0\n",
    "            for P in all_multis:\n",
    "                if P in Z_active: k_log += math.log(self.pi_P)\n",
    "                else: k_log += math.log(1-self.pi_P)\n",
    "            return k_log\n",
    "    def log_prior_A(self, Z_active:Set[FrozenSet[str]], A:Dict[Tuple[FrozenSet[str],FrozenSet[str]],int], unit_drop=True)->float:\n",
    "        # ------------------------------------------------------------\n",
    "        # Computes log P(A | Z): the log prior over EDGE EXISTENCE between active potencies.\n",
    "        #\n",
    "        # Inputs:\n",
    "        #   - Z_active: set of active potencies (nodes in the potency DAG)\n",
    "        #   - A: adjacency dictionary mapping (P,Q) -> {0,1}, indicating whether edge P->Q is present\n",
    "        #   - unit_drop: if True, an admissible edge must drop EXACTLY one fate (|P\\Q| == 1);\n",
    "        #                otherwise any monotone subset drop (Q ⊂ P) is admissible.\n",
    "        #\n",
    "        # Prior:\n",
    "        #   - For every admissible pair (P,Q):\n",
    "        #         A_{P->Q} ~ Bernoulli(rho)\n",
    "        #     So:\n",
    "        #         log P(A|Z) = ∑_{(P,Q) admissible} [ A_{P->Q} log(rho) + (1 - A_{P->Q}) log(1 - rho) ]\n",
    "        #\n",
    "        # Notes:\n",
    "        #   - \"Admissible\" enforces graph shape constraints (subset-monotone and possibly unit-drop).\n",
    "        #   - If an edge (P,Q) is not admissible, it does not contribute to the product/sum at all.\n",
    "        # ------------------------------------------------------------\n",
    "        labels=list(Z_active)\n",
    "        # admissible set is pairs with subset monotone (and optionally unit-drop)\n",
    "        logp=0.0\n",
    "        for P in labels:\n",
    "            for Q in labels:\n",
    "                if admissible_edge(P,Q,unit_drop):\n",
    "                    # a == 1 if the edge is present in A, else 0\n",
    "                    a = 1 if A.get((P,Q),0)==1 else 0\n",
    "                    # add Bernoulli log-prob for this edge\n",
    "                    logp += math.log(self.rho) if a==1 else math.log(1-self.rho)\n",
    "        return logp\n",
    "# ----------------------------\n",
    "# Structure container and proposals\n",
    "# ----------------------------\n",
    "class Structure:\n",
    "    def __init__(self,\n",
    "                 S: List[str],\n",
    "                 Z_active: Set[FrozenSet[str]],\n",
    "                 A: Dict[Tuple[FrozenSet[str],FrozenSet[str]],int],\n",
    "                 unit_drop: bool = True):\n",
    "        self.S=S\n",
    "        self.Z_active=set(Z_active)  # includes singletons\n",
    "        self.A=dict(A)\n",
    "        self.unit_drop=unit_drop\n",
    "        self.labels_list=self._sorted_labels()\n",
    "        self.Reach = transitive_closure(self.labels_list, self.A)\n",
    "    def _sorted_labels(self)->List[FrozenSet[str]]:\n",
    "        return sorted(list(self.Z_active), key=lambda x: (len(x), tuple(sorted(list(x)))))\n",
    "    def recompute_reach(self):\n",
    "        self.labels_list=self._sorted_labels()\n",
    "        self.Reach = transitive_closure(self.labels_list, self.A)\n",
    "    def clone(self)->\"Structure\":\n",
    "        return Structure(self.S, set(self.Z_active), dict(self.A), self.unit_drop)\n",
    "    # --- Moves ---\n",
    "    def potencies_multi_all(self)->List[FrozenSet[str]]:\n",
    "        return [P for P in all_nonempty_subsets(self.S) if len(P)>=2]\n",
    "    def propose_add_potency(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        candidates = [P for P in self.potencies_multi_all() if P not in self.Z_active]\n",
    "        if not candidates: return None\n",
    "        P = rng.choice(candidates)\n",
    "        new = self.clone()\n",
    "        new.Z_active.add(P)\n",
    "        # add edges that respect admissibility? keep edges as-is and allow edge moves separately\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "    def propose_remove_potency(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        candidates = [P for P in self.Z_active if len(P)>=2]\n",
    "        if not candidates: return None\n",
    "        P = rng.choice(candidates)\n",
    "        new = self.clone()\n",
    "        # remove potency and incident edges\n",
    "        new.Z_active.remove(P)\n",
    "        new.A = {e:v for e,v in new.A.items() if P not in e}\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "    def propose_swap_potency(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        remove_candidates = [P for P in self.Z_active if len(P)>=2]\n",
    "        add_candidates = [P for P in self.potencies_multi_all() if P not in self.Z_active]\n",
    "        if not remove_candidates or not add_candidates: return None\n",
    "        P_rm = rng.choice(remove_candidates)\n",
    "        P_add = rng.choice(add_candidates)\n",
    "        new = self.clone()\n",
    "        new.Z_active.remove(P_rm)\n",
    "        new.A = {e:v for e,v in new.A.items() if P_rm not in e}\n",
    "        new.Z_active.add(P_add)\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "    def all_edge_pairs(self)->List[Tuple[FrozenSet[str],FrozenSet[str]]]:\n",
    "        L=list(self.Z_active)\n",
    "        pairs=[]\n",
    "        for P in L:\n",
    "            for Q in L:\n",
    "                if admissible_edge(P,Q,self.unit_drop):\n",
    "                    pairs.append((P,Q))\n",
    "        return pairs\n",
    "    def propose_add_edge(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        pairs = [e for e in self.all_edge_pairs() if self.A.get(e,0)==0]\n",
    "        if not pairs: return None\n",
    "        e = rng.choice(pairs)\n",
    "        new = self.clone()\n",
    "        new.A[e]=1\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "    def propose_remove_edge(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        edges = [e for e,v in self.A.items() if v==1]\n",
    "        if not edges: return None\n",
    "        e = rng.choice(edges)\n",
    "        new = self.clone()\n",
    "        del new.A[e]\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "# ----------------------------\n",
    "# Scoring: log posterior\n",
    "# ----------------------------\n",
    "def score_structure(struct: Structure,\n",
    "                    trees: List[TreeNode],\n",
    "                    leaf_type_maps: List[Dict[str,str]],\n",
    "                    priors: Priors,\n",
    "                    prune_eps: float = 0.0) -> Tuple[float, List[float]]:\n",
    "    # log prior\n",
    "    logp = priors.log_prior_Z(struct.S, struct.Z_active)\n",
    "    #print(f\"logp:{logp}\")\n",
    "    if not math.isfinite(logp):\n",
    "        logp = float(\"-inf\")\n",
    "    logp += priors.log_prior_A(struct.Z_active, struct.A, unit_drop=struct.unit_drop)\n",
    "    # likelihood\n",
    "    logLs=[]\n",
    "    for root, leaf_to_type in zip(trees, leaf_type_maps):\n",
    "        B_sets = compute_B_sets(root, leaf_to_type)\n",
    "        # --- NEW: if the root has no labels at all, skip this tree (neutral evidence) ---\n",
    "        root_labels = B_sets.get(root, set())\n",
    "        if not root_labels:\n",
    "            logLs.append(0.0)\n",
    "            continue\n",
    "        # -------------------------------------------------------------------------------\n",
    "        C = dp_tree_root_table(root, struct.labels_list, struct.Reach, B_sets, prune_eps=prune_eps)\n",
    "        P_T = tree_marginal_from_root_table(C)\n",
    "        #print(f\"P_T:{P_T}\")\n",
    "        if P_T <= 0 or not math.isfinite(P_T):\n",
    "            return float(\"-inf\"), []\n",
    "        logLs.append(math.log(P_T))\n",
    "    return logp + sum(logLs), logLs\n",
    "def map_search(\n",
    "    S: List[str],\n",
    "    trees: List[TreeNode],\n",
    "    leaf_type_maps: List[Dict[str,str]],\n",
    "    priors: Priors,\n",
    "    unit_drop_edges: bool = True,\n",
    "    fixed_k: Optional[int] = None,\n",
    "    init_seed: int = 0,\n",
    "    iters: int = 500,\n",
    "    restarts: int = 3,\n",
    "    temp_init: float = 1.0,\n",
    "    temp_decay: float = 0.995,\n",
    "    move_probs = (0.25, 0.25, 0.25, 0.25),  # addP, rmP, addE, rmE (swap used when fixed_k)\n",
    "    prune_eps: float = 0.0,\n",
    "    progress: bool = False,\n",
    "):\n",
    "    rng = random.Random(init_seed)\n",
    "    best_global = None\n",
    "    best_score = float(\"-inf\")\n",
    "    best_logs = None\n",
    "    for rs in range(restarts):\n",
    "        # --- init structure\n",
    "        print(rs)\n",
    "        if priors.potency_mode==\"fixed_k\":\n",
    "            Z = build_Z_active(S, fixed_k=priors.fixed_k, max_potency_size=len(S), seed=rng.randint(0,10**9))\n",
    "        else:\n",
    "            base = build_Z_active(S, fixed_k=0, max_potency_size=len(S), seed=rng.randint(0,10**9))\n",
    "            Z = base\n",
    "        A = {}\n",
    "        current = Structure(S, Z, A, unit_drop=unit_drop_edges)\n",
    "        curr_score, _ = score_structure(current, trees, leaf_type_maps, priors, prune_eps)\n",
    "        # fallback: if invalid, keep sampling until valid\n",
    "        attempts = 0\n",
    "        while not math.isfinite(curr_score) and attempts < 720:\n",
    "            Z = build_Z_active(S, fixed_k=(priors.fixed_k if priors.potency_mode==\"fixed_k\" else 0),\n",
    "                               max_potency_size=len(S), seed=rng.randint(0,10**9))\n",
    "            A = build_edges(Z, forbid_fn=None, unit_drop=unit_drop_edges)\n",
    "            # A = {}\n",
    "            # print(f\"Z:{Z}\"); print(f\"A:{A}\")\n",
    "            current = Structure(S, Z, A, unit_drop=unit_drop_edges)\n",
    "            curr_score, _ = score_structure(current, trees, leaf_type_maps, priors, prune_eps)\n",
    "            # print(curr_score)\n",
    "            attempts += 1\n",
    "        if not math.isfinite(curr_score):\n",
    "            raise RuntimeError(\"Failed to initialize a valid structure; consider easing settings.\")\n",
    "        local_best = current.clone()\n",
    "        local_best_score = curr_score\n",
    "        tau = temp_init\n",
    "        addP, rmP, addE, rmE = move_probs\n",
    "        # iterator respects the 'progress' flag\n",
    "        iterator = trange(iters, desc=f\"Restart {rs+1}/{restarts}\", leave=True) if progress else range(iters)\n",
    "        for _ in iterator:\n",
    "            # choose move\n",
    "            prop = None\n",
    "            r = rng.random()\n",
    "            if priors.potency_mode==\"fixed_k\":\n",
    "                if r < addE:\n",
    "                    prop = current.propose_add_edge(rng)\n",
    "                elif r < addE + rmE:\n",
    "                    prop = current.propose_remove_edge(rng)\n",
    "                else:\n",
    "                    prop = current.propose_swap_potency(rng)\n",
    "            else:\n",
    "                if r < addP:\n",
    "                    prop = current.propose_add_potency(rng)\n",
    "                elif r < addP + rmP:\n",
    "                    prop = current.propose_remove_potency(rng)\n",
    "                elif r < addP + rmP + addE:\n",
    "                    prop = current.propose_add_edge(rng)\n",
    "                else:\n",
    "                    prop = current.propose_remove_edge(rng)\n",
    "            if prop is None:\n",
    "                tau *= temp_decay\n",
    "                if progress:\n",
    "                    iterator.set_postfix({\"Best\": f\"{best_score:.3f}\", \"Curr\": f\"{curr_score:.3f}\", \"Temp\": f\"{tau:.3f}\"})\n",
    "                continue\n",
    "            prop_score, _ = score_structure(prop, trees, leaf_type_maps, priors, prune_eps)\n",
    "            delta = prop_score - curr_score\n",
    "            accept = (delta >= 0) or (rng.random() < math.exp(delta / max(tau,1e-12)))\n",
    "            if accept:\n",
    "                current = prop\n",
    "                curr_score = prop_score\n",
    "                if curr_score > local_best_score:\n",
    "                    local_best = current.clone()\n",
    "                    local_best_score = curr_score\n",
    "                if curr_score > best_score:\n",
    "                    best_global = current.clone()\n",
    "                    best_score = curr_score\n",
    "                    best_logs = None  # compute later if needed\n",
    "            tau *= temp_decay\n",
    "            print(f\"rs:{rs},curr{curr_score}\")\n",
    "            if progress:\n",
    "                iterator.set_postfix({\"Best\": f\"{best_score:.3f}\", \"Curr\": f\"{curr_score:.3f}\", \"Temp\": f\"{tau:.3f}\"})\n",
    "    # after restarts, recompute detailed logs for best_global\n",
    "    final_score, logLs = score_structure(best_global, trees, leaf_type_maps, priors, prune_eps)\n",
    "    return best_global, final_score, logLs\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import os\n",
    "def _map_search_worker(args):\n",
    "    (S, trees, leaf_type_maps, priors, unit_drop_edges, fixed_k,\n",
    "     init_seed, iters, restarts, temp_init, temp_decay, move_probs, prune_eps) = args\n",
    "    # progress=False inside workers to avoid tqdm noise\n",
    "    return map_search(\n",
    "        S=S,\n",
    "        trees=trees,\n",
    "        leaf_type_maps=leaf_type_maps,\n",
    "        priors=priors,\n",
    "        unit_drop_edges=unit_drop_edges,\n",
    "        fixed_k=fixed_k,\n",
    "        init_seed=init_seed,\n",
    "        iters=iters,\n",
    "        restarts=restarts,\n",
    "        temp_init=temp_init,\n",
    "        temp_decay=temp_decay,\n",
    "        move_probs=move_probs,\n",
    "        prune_eps=prune_eps,\n",
    "        progress=False,\n",
    "    )\n",
    "def map_search_parallel(\n",
    "    S: List[str],\n",
    "    trees: List[TreeNode],\n",
    "    leaf_type_maps: List[Dict[str,str]],\n",
    "    priors: Priors,\n",
    "    unit_drop_edges: bool = True,\n",
    "    fixed_k: Optional[int] = None,\n",
    "    init_seed: int = 0,\n",
    "    iters: int = 500,\n",
    "    restarts: int = 12,\n",
    "    temp_init: float = 1.0,\n",
    "    temp_decay: float = 0.995,\n",
    "    move_probs = (0.25, 0.25, 0.25, 0.25),\n",
    "    prune_eps: float = 0.0,\n",
    "    n_jobs: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallelizes restarts across processes and returns the best result.\n",
    "    NOTE: On Windows/macOS, call this under `if __name__ == \"__main__\":` to avoid spawn issues.\n",
    "    \"\"\"\n",
    "    if n_jobs is None:\n",
    "        n_jobs = max(1, (os.cpu_count() or 2) - 1)\n",
    "    # Split restarts across jobs\n",
    "    per_job = [restarts // n_jobs] * n_jobs\n",
    "    for i in range(restarts % n_jobs):\n",
    "        per_job[i] += 1\n",
    "    per_job = [r for r in per_job if r > 0]\n",
    "    n_jobs = len(per_job)\n",
    "    # Unique seeds per worker to diversify trajectories\n",
    "    seeds = [init_seed + 10_000 * i for i in range(n_jobs)]\n",
    "    tasks = []\n",
    "    for r, seed in zip(per_job, seeds):\n",
    "        tasks.append((S, trees, leaf_type_maps, priors, unit_drop_edges, fixed_k,\n",
    "                      seed, iters, r, temp_init, temp_decay, move_probs, prune_eps))\n",
    "    best_global = None\n",
    "    best_score = float(\"-inf\")\n",
    "    best_logs = None\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as ex:\n",
    "        futures = [ex.submit(_map_search_worker, t) for t in tasks]\n",
    "        for fut in as_completed(futures):\n",
    "            bestF, score, logs = fut.result()\n",
    "            if score > best_score:\n",
    "                best_global, best_score, best_logs = bestF, score, logs\n",
    "    return best_global, best_score, best_logs\n",
    "########\n",
    "#\n",
    "#\n",
    "# MAP TXT READING\n",
    "#\n",
    "########\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "def read_leaf_type_map(path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Read a leaf->type mapping from a file.\n",
    "    Supported:\n",
    "      - JSON dict: { \"LeafName\": \"Type\", ... }\n",
    "      - CSV/TSV/TXT with 2 columns (header optional):\n",
    "          * If header present, typical field names could be:\n",
    "              - leaf, type\n",
    "              - cellBC, cell_state (your .txt example)\n",
    "    Returns: dict {leaf_name: type_symbol} (types are coerced to str)\n",
    "    \"\"\"\n",
    "    import os, csv, json\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".json\",):\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(f\"{path}: JSON must be an object mapping leaf->type.\")\n",
    "        return {str(k): str(v) for k, v in data.items()}\n",
    "    elif ext in (\".csv\", \".tsv\", \".txt\"):\n",
    "        # treat .txt as TSV by default (your example is tab-delimited)\n",
    "        delim = \"\\t\" if ext in (\".tsv\", \".txt\") else \",\"\n",
    "        out = {}\n",
    "        with open(path, \"r\", newline=\"\") as f:\n",
    "            reader = csv.reader(f, delimiter=delim)\n",
    "            rows = list(reader)\n",
    "            if not rows:\n",
    "                raise ValueError(f\"{path}: empty file\")\n",
    "            # Detect header\n",
    "            start_idx = 0\n",
    "            header = [h.strip().lower() for h in rows[0]] if rows and rows[0] else []\n",
    "            has_header = False\n",
    "            if len(header) >= 2:\n",
    "                # Common header names we accept\n",
    "                if (\"leaf\" in header[0] or \"cellbc\" in header[0]) and (\"type\" in header[1] or \"cell_state\" in header[1]):\n",
    "                    has_header = True\n",
    "                # Or any header line where at least one of ('leaf','cellbc') and one of ('type','cell_state') appear\n",
    "                if not has_header:\n",
    "                    left_has = any(x in header for x in (\"leaf\", \"cellbc\"))\n",
    "                    right_has = any(x in header for x in (\"type\", \"cell_state\"))\n",
    "                    has_header = left_has and right_has\n",
    "            if has_header:\n",
    "                start_idx = 1\n",
    "            for i in range(start_idx, len(rows)):\n",
    "                row = rows[i]\n",
    "                if len(row) < 2:\n",
    "                    raise ValueError(f\"{path}: line {i+1} needs at least 2 columns (leaf,type)\")\n",
    "                leaf = row[0].strip()\n",
    "                typ  = row[1].strip()\n",
    "                if not leaf or not typ:\n",
    "                    raise ValueError(f\"{path}: line {i+1} has empty leaf/type\")\n",
    "                if leaf in out:\n",
    "                    raise ValueError(f\"{path}: duplicate leaf '{leaf}' at line {i+1}\")\n",
    "                out[leaf] = str(typ)  # coerce types to string (handles negatives like -7, -9)\n",
    "        return out\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mapping file type: {path} (use .csv, .tsv, .txt, or .json)\")\n",
    "\n",
    "def validate_leaf_type_map(root: TreeNode, leaf_map: Dict[str,str], S: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Ensure mapping covers exactly the leaves in the tree, and types are in S.\n",
    "    Raises ValueError if not valid.\n",
    "    \"\"\"\n",
    "    leaves_in_tree = set(collect_leaf_names(root))\n",
    "    leaves_in_map  = set(leaf_map.keys())\n",
    "    missing = leaves_in_tree - leaves_in_map\n",
    "    extra   = leaves_in_map  - leaves_in_tree\n",
    "    if missing:\n",
    "        raise ValueError(f\"Leaf map missing leaves: {sorted(missing)}\")\n",
    "    if extra:\n",
    "        raise ValueError(f\"Leaf map has unknown leaves not in tree: {sorted(extra)}\")\n",
    "    allowed = set(S)\n",
    "    bad_types = {t for t in leaf_map.values() if t not in allowed}\n",
    "    if bad_types:\n",
    "        raise ValueError(f\"Leaf map contains types not in S={S}: {sorted(bad_types)}\")\n",
    "\n",
    "def filter_leaf_map_to_tree(root: TreeNode, leaf_map: Dict[str, str]) -> Dict[str, str]:\n",
    "    leaves = set(collect_leaf_names(root))\n",
    "    return {leaf: str(typ) for leaf, typ in leaf_map.items() if leaf in leaves}\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# Parsing the custom TXT \"map\" format and scoring\n",
    "# ----------------------------\n",
    "import json\n",
    "def _read_json_objects_exact(path: str):\n",
    "    \"\"\"Read one JSON object per non-empty line (your file format).\"\"\"\n",
    "    objs = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            objs.append(json.loads(line))\n",
    "    if not objs:\n",
    "        raise ValueError(f\"{path}: no JSON objects found\")\n",
    "    return objs\n",
    "def _extract_vertices_edges_from_adj(adj):\n",
    "    V = set(adj.keys())\n",
    "    for chs in adj.values():\n",
    "        if isinstance(chs, list):\n",
    "            V.update(chs)\n",
    "    E = []\n",
    "    for u, chs in adj.items():\n",
    "        if isinstance(chs, list):\n",
    "            for v in chs:\n",
    "                E.append((str(u), str(v)))\n",
    "    V = sorted(map(str, V), key=lambda x: (len(x), x))\n",
    "    E = sorted(E, key=lambda e: (e[0], e[1]))\n",
    "    return V, E\n",
    "def _normalize_adj_remove_synthetic_root(adj: dict) -> dict:\n",
    "    \"\"\"Drop a synthetic 'root' node (if present) from adjacency for building F.\"\"\"\n",
    "    adj2 = {str(k): (list(v) if isinstance(v, list) else v) for k, v in adj.items()}\n",
    "    if \"root\" in adj2:\n",
    "        ch = adj2[\"root\"]\n",
    "        if not isinstance(ch, list) or len(ch) != 1:\n",
    "            raise ValueError(\"Synthetic 'root' must have exactly one child\")\n",
    "        del adj2[\"root\"]\n",
    "    return adj2\n",
    "def _resolve_id_to_set(id_str: str, comp_map: dict, memo: dict, visiting: set) -> frozenset:\n",
    "    \"\"\"\n",
    "    Recursively resolve an id to a frozenset of base (negative-string) types.\n",
    "    - negative id: returns {id}\n",
    "    - list value: union of resolves\n",
    "    - single value: resolve that\n",
    "    Detects cycles and missing entries.\n",
    "    \"\"\"\n",
    "    id_str = str(id_str)\n",
    "    if id_str.startswith(\"-\"):\n",
    "        return frozenset([id_str])\n",
    "    if id_str in memo:\n",
    "        return memo[id_str]\n",
    "    if id_str in visiting:\n",
    "        raise ValueError(f\"Cycle detected while resolving potency '{id_str}'\")\n",
    "    if id_str not in comp_map:\n",
    "        raise ValueError(f\"Positive id '{id_str}' appears but not defined in composition map\")\n",
    "    visiting.add(id_str)\n",
    "    val = comp_map[id_str]\n",
    "    acc = set()\n",
    "    if isinstance(val, list):\n",
    "        for child in val:\n",
    "            acc |= _resolve_id_to_set(str(child), comp_map, memo, visiting)\n",
    "    else:\n",
    "        acc |= _resolve_id_to_set(str(val), comp_map, memo, visiting)\n",
    "    visiting.remove(id_str)\n",
    "    memo[id_str] = frozenset(acc)\n",
    "    return memo[id_str]\n",
    "def _build_ZA_from_txt(adj: dict, comp_map: dict, unit_drop_edges: bool):\n",
    "    \"\"\"\n",
    "    Build F = (Z_active, A) from adjacency + hierarchical composition map.\n",
    "    Returns: Z_active, A, base_types(list), potency_id_to_set(dict id->frozenset)\n",
    "    \"\"\"\n",
    "    # Drop synthetic \"root\" from adjacency for structure building\n",
    "    adj = _normalize_adj_remove_synthetic_root(adj)\n",
    "    # Collect all ids we need to resolve\n",
    "    ids_seen = set(map(str, comp_map.keys()))\n",
    "    for u, chs in adj.items():\n",
    "        ids_seen.add(str(u))\n",
    "        if isinstance(chs, list):\n",
    "            for v in chs:\n",
    "                ids_seen.add(str(v))\n",
    "    memo = {}\n",
    "    potency_id_to_set = {}\n",
    "    base_types = set()\n",
    "    # Resolve every id\n",
    "    for idv in ids_seen:\n",
    "        if idv.startswith(\"-\"):\n",
    "            memo[idv] = frozenset([idv])\n",
    "        else:\n",
    "            s = _resolve_id_to_set(idv, comp_map, memo, visiting=set())\n",
    "            potency_id_to_set[idv] = s\n",
    "    # Gather base types\n",
    "    for s in memo.values():\n",
    "        for t in s:\n",
    "            if t.startswith(\"-\"):\n",
    "                base_types.add(t)\n",
    "    # Z: singletons for all base types + multi-type potencies (size >=2)\n",
    "    Z_active = {frozenset([t]) for t in base_types}\n",
    "    for pid, s in potency_id_to_set.items():\n",
    "        if len(s) >= 2:\n",
    "            Z_active.add(s)\n",
    "    # A: only edges in adjacency, mapped via expansion; keep admissible ones\n",
    "    A = {}\n",
    "    def id_to_set(x: str) -> frozenset:\n",
    "        x = str(x)\n",
    "        if x.startswith(\"-\"):\n",
    "            return frozenset([x])\n",
    "        return potency_id_to_set[x]  # safe after resolution above\n",
    "    for u, chs in adj.items():\n",
    "        Pu = id_to_set(u)\n",
    "        for v in chs:\n",
    "            Qv = id_to_set(v)\n",
    "            if admissible_edge(Pu, Qv, unit_drop_edges):\n",
    "                A[(Pu, Qv)] = 1\n",
    "    return Z_active, A, sorted(base_types), potency_id_to_set\n",
    "def score_given_map_and_trees(txt_path: str,\n",
    "                              unit_drop_edges: bool = False ) -> float:\n",
    "    \"\"\"\n",
    "    Parse your EXACT file:\n",
    "      1) adjacency dict\n",
    "      2) node weights (ignored)\n",
    "      3) composition map (CRUCIAL: defines potencies as mixtures; can reference other positives)\n",
    "      4) root id\n",
    "      5) leaf counts (ignored)\n",
    "      6) split probs (ignored)\n",
    "    Build F=(Z,A) from the composition map + adjacency, then compute\n",
    "    total log-likelihood across 0002_* trees with the same DP/Beta logic.\n",
    "    \"\"\"\n",
    "    objs = _read_json_objects_exact(txt_path)\n",
    "    if len(objs) < 4:\n",
    "        raise ValueError(\"Expected at least 4 JSON lines (adjacency, weights, composition map, root).\")\n",
    "    # 1) adjacency (first dict with list values)\n",
    "    adj = None\n",
    "    for o in objs:\n",
    "        if isinstance(o, dict) and any(isinstance(v, list) for v in o.values()):\n",
    "            adj = {str(k): [str(x) for x in v] for k, v in o.items() if isinstance(v, list)}\n",
    "            break\n",
    "    if adj is None:\n",
    "        raise ValueError(\"Could not locate adjacency dict in the file.\")\n",
    "    # 2) composition map (third object)\n",
    "    comp_map = objs[2]\n",
    "    if not isinstance(comp_map, dict):\n",
    "        raise ValueError(\"Third JSON must be the composition map (dict).\")\n",
    "    # 3) root id (fourth object) -- only for printing/sanity\n",
    "    root_id = objs[3]\n",
    "    if isinstance(root_id, dict) and \"root_id\" in root_id:\n",
    "        root_id = root_id[\"root_id\"]\n",
    "    root_id = str(root_id)\n",
    "    # Print vertices & edges of the given graph (raw, including 'root' if present)\n",
    "    V, E = _extract_vertices_edges_from_adj(adj)\n",
    "    print(\"=== Parsed Graph: Vertices ===\")\n",
    "    for v in V: print(\" \", v)\n",
    "    print(\"\\n=== Parsed Graph: Edges (u -> v) ===\")\n",
    "    for u, v in E: print(f\"  {u} -> {v}\")\n",
    "    # Build F = (Z, A) strictly from your map info (hierarchical potencies respected)\n",
    "    Z_from_map, A_from_map, base_types_map, potency_def = _build_ZA_from_txt(\n",
    "        adj=adj,\n",
    "        comp_map=comp_map,\n",
    "        unit_drop_edges=unit_drop_edges  # False allows multi-drop; True enforces unit-drop\n",
    "    )\n",
    "    # Optional: print expanded potency definitions\n",
    "    print(\"\\n=== Potency definitions (expanded) ===\")\n",
    "    for pid in sorted(potency_def, key=lambda x: (len(x), x)):\n",
    "        s = \",\".join(sorted(potency_def[pid]))\n",
    "        print(f\"  {pid} := {{{s}}}\")\n",
    "    # ----------------- Load your experimental trees + leaf maps -----------------\n",
    "    trees = [read_newick_file(\"./0002_tree_0.txt\"),\n",
    "             read_newick_file(\"./0002_tree_1.txt\"),\n",
    "             read_newick_file(\"./0002_tree_2.txt\"),\n",
    "             read_newick_file(\"./0002_tree_3.txt\"),\n",
    "             read_newick_file(\"./0002_tree_4.txt\")]\n",
    "    meta_paths = [\"./0002_meta_0.txt\",\"./0002_meta_1.txt\",\"./0002_meta_2.txt\",\"./0002_meta_3.txt\",\"./0002_meta_4.txt\"]\n",
    "    raw_maps = [read_leaf_type_map(p) for p in meta_paths]\n",
    "    leaf_type_maps = [filter_leaf_map_to_tree(root, m) for root, m in zip(trees, raw_maps)]\n",
    "    base_types_data = sorted({str(t) for m in leaf_type_maps for t in m.values()})\n",
    "    # Final S = union of base types from data and from the map\n",
    "    S_all = sorted(set(base_types_map) | set(base_types_data))\n",
    "    # Ensure all singletons exist for S_all and add the map-defined potencies\n",
    "    Z_active = set(Z_from_map) | {frozenset([t]) for t in S_all}\n",
    "    A = dict(A_from_map)\n",
    "    # print(f\"A:{A}\")\n",
    "    # print(f\"Z:{Z_active}\")\n",
    "    # Build Structure and score\n",
    "    struct = Structure(S=S_all, Z_active=Z_active, A=A, unit_drop=unit_drop_edges)\n",
    "    dummy_priors = Priors(potency_mode=\"fixed_k\", fixed_k=5, rho=0.2)  # priors ignored for printed likelihoods\n",
    "    k_multis = sum(1 for P in struct.Z_active if len(P) >= 2)\n",
    "    # print(f\"k:{k_multis}\")\n",
    "    log_post, per_tree_logs = score_structure(\n",
    "        struct=struct,\n",
    "        trees=trees,\n",
    "        leaf_type_maps=leaf_type_maps,\n",
    "        priors=dummy_priors,\n",
    "        prune_eps=0.0\n",
    "    )\n",
    "    total_ll = sum(per_tree_logs)\n",
    "    print(\"\\n=== Log-likelihoods (given F from map) ===\")\n",
    "    for i, lg in enumerate(per_tree_logs, 1):\n",
    "        print(f\"Tree {i}: log P(T|F) = {lg:.6f}\")\n",
    "    print(f\"Total log-likelihood = {total_ll:.6f}\")\n",
    "    return total_ll\n",
    "def main():\n",
    "    import random\n",
    "    random.seed(7)\n",
    "    # Load Newick strings from .txt (same format as .nwk)\n",
    "    trees = [read_newick_file(\"./0002_tree_0.txt\"),\n",
    "             read_newick_file(\"./0002_tree_1.txt\"),\n",
    "             read_newick_file(\"./0002_tree_2.txt\"),\n",
    "             read_newick_file(\"./0002_tree_3.txt\"),\n",
    "             read_newick_file(\"./0002_tree_4.txt\")]\n",
    "    # TAB-delimited maps with header 'cellBC\\tcell_state'\n",
    "    map_paths = [\n",
    "        \"./0002_meta_0.txt\",\n",
    "        \"./0002_meta_1.txt\",\n",
    "        \"./0002_meta_2.txt\",\n",
    "        \"./0002_meta_3.txt\",\n",
    "        \"./0002_meta_4.txt\",\n",
    "    ]\n",
    "    raw_maps = [read_leaf_type_map(p) for p in map_paths]\n",
    "    # Drop dictionary entries not present in the corresponding tree\n",
    "    leaf_type_maps = [filter_leaf_map_to_tree(root, m) for root, m in zip(trees, raw_maps)]\n",
    "    # Build S from types that are actually used after filtering\n",
    "    S = sorted({str(t) for m in leaf_type_maps for t in m.values()})\n",
    "    # (Optional) soft warnings; never raise\n",
    "    for idx, (root, m_raw, m_used) in enumerate(zip(trees, raw_maps, leaf_type_maps), 1):\n",
    "        leaves_tree = set(collect_leaf_names(root))\n",
    "        extra = sorted(set(m_raw.keys()) - leaves_tree)\n",
    "        missing = sorted(leaves_tree - set(m_used.keys()))  # leaves in tree with no mapping\n",
    "        if extra:\n",
    "            print(f\"[warn] Tree {idx}: {len(extra)} map entries are not in the tree and were ignored \"\n",
    "                  f\"(e.g., {extra[:5]}{'...' if len(extra)>5 else ''})\")\n",
    "        if missing:\n",
    "            print(f\"[warn] Tree {idx}: {len(missing)} tree leaves have no mapping and were ignored \"\n",
    "                  f\"(e.g., {missing[:5]}{'...' if len(missing)>5 else ''})\")\n",
    "        if not any(True for _ in m_used):\n",
    "            print(f\"[warn] Tree {idx}: no mapped leaves; treating as neutral evidence.\")\n",
    "    priors = Priors(potency_mode=\"fixed_k\", fixed_k=5, rho=0.2)\n",
    "    bestF, best_score, per_tree_logs = map_search_parallel(\n",
    "        S=S,\n",
    "        trees=trees,\n",
    "        leaf_type_maps=leaf_type_maps,\n",
    "        priors=priors,\n",
    "        unit_drop_edges=False,\n",
    "        fixed_k=priors.fixed_k if priors.potency_mode==\"fixed_k\" else None,\n",
    "        init_seed=123,\n",
    "        iters=1000,\n",
    "        restarts=10,\n",
    "        temp_init=1.0,\n",
    "        temp_decay=0.995,\n",
    "        move_probs=(0.3, 0.2, 0.3, 0.2),\n",
    "        prune_eps=0.0,\n",
    "        n_jobs=os.cpu_count(),   # or a smaller number if memory-bound\n",
    "    )\n",
    "    # --- Pretty-print best map ---\n",
    "    def pot_str(P): return \"{\" + \",\".join(sorted(list(P))) + \"}\"\n",
    "    print(\"\\n=== BEST MAP (F*) ===\")\n",
    "    multi_sorted = sorted([P for P in bestF.Z_active if len(P)>=2], key=lambda x:(len(x), tuple(sorted(list(x)))))\n",
    "    print(\"Active potencies (multi-type):\")\n",
    "    for P in multi_sorted: print(\"  \", pot_str(P))\n",
    "    print(\"Singletons (always active):\")\n",
    "    for t in S: print(\"  \", \"{\"+t+\"}\")\n",
    "    print(\"\\nEdges:\")\n",
    "    edges = sorted([e for e,v in bestF.A.items() if v==1], key=lambda e:(len(e[0]), len(e[1]), tuple(sorted(list(e[0]))), tuple(sorted(list(e[1])))))\n",
    "    for P,Q in edges:\n",
    "        print(f\"  {pot_str(P)} -> {pot_str(Q)}\")\n",
    "    print(\"\\nScores:\")\n",
    "    print(f\"  log posterior: {best_score:.6f}\")\n",
    "    for i,lg in enumerate(per_tree_logs,1):\n",
    "        print(f\"  Tree {i} log P(T|F*): {lg:.6f}\")\n",
    "\n",
    "def _ask_yes_no(prompt: str, default: bool = True) -> bool:\n",
    "    y = \"Y/n\" if default else \"y/N\"\n",
    "    while True:\n",
    "        ans = input(f\"{prompt} [{y}]: \").strip().lower()\n",
    "        if ans == \"\" and default is not None:\n",
    "            return default\n",
    "        if ans in (\"y\", \"yes\"): return True\n",
    "        if ans in (\"n\", \"no\"): return False\n",
    "        print(\"Please answer y or n.\")\n",
    "def main_cli():\n",
    "    print(\"Select mode:\")\n",
    "    print(\"  1) Run MAP search demo (uses files in code)\")\n",
    "    print(\"  2) Score a given TXT map (compute log-likelihood only)\")\n",
    "    choice = input(\"Enter 1 or 2: \").strip()\n",
    "    if choice == \"1\":\n",
    "        print(\"\\n[Mode 1] Running MAP search demo...\\n\")\n",
    "        main()  # your existing demo function\n",
    "        return\n",
    "    if choice == \"2\":\n",
    "        # print(\"\\n[Mode 2] Score a given TXT map\")\n",
    "        # txt_path = input(\"Path to TXT file: \").strip()\n",
    "        # if not txt_path:\n",
    "        txt_path=\"main.txt\"\n",
    "        print(\"ERROR: TXT path required.\")\n",
    "            #return\n",
    "        #unit_drop_edges = _ask_yes_no(\"Use unit-drop edges (|P\\\\Q| == 1)?\", default=True)\n",
    "        print(\"\\nParsing and scoring...\\n\")\n",
    "        try:\n",
    "            score_given_map_and_trees(\n",
    "                txt_path=txt_path)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        return\n",
    "    print(\"Invalid choice. Please run again and enter 1 or 2.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4477a5-cba3-41ef-a2c9-8c339c3643d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select mode:\n",
      "  1) Run MAP search demo (uses files in code)\n",
      "  2) Score a given TXT map (compute log-likelihood only)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 1 or 2:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Mode 1] Running MAP search demo...\n",
      "\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1244\u001b[0m\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid choice. Please run again and enter 1 or 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1244\u001b[0m     \u001b[43mmain_cli\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 1220\u001b[0m, in \u001b[0;36mmain_cli\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Mode 1] Running MAP search demo...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1220\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# your existing demo function\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[3], line 1165\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1161\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[warn] Tree \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: no mapped leaves; treating as neutral evidence.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1163\u001b[0m priors \u001b[38;5;241m=\u001b[39m Priors(potency_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed_k\u001b[39m\u001b[38;5;124m\"\u001b[39m, fixed_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, rho\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m-> 1165\u001b[0m bestF, best_score, per_tree_logs \u001b[38;5;241m=\u001b[39m \u001b[43mmap_search_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleaf_type_maps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleaf_type_maps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpriors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpriors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit_drop_edges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpriors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpriors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpotency_mode\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfixed_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m123\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrestarts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.995\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmove_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprune_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# or a smaller number if memory-bound\u001b[39;49;00m\n\u001b[0;32m   1180\u001b[0m \n\u001b[0;32m   1181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;66;03m# --- Pretty-print best map ---\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpot_str\u001b[39m(P): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(P))) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 785\u001b[0m, in \u001b[0;36mmap_search_parallel\u001b[1;34m(S, trees, leaf_type_maps, priors, unit_drop_edges, fixed_k, init_seed, iters, restarts, temp_init, temp_decay, move_probs, prune_eps, n_jobs)\u001b[0m\n\u001b[0;32m    783\u001b[0m futures \u001b[38;5;241m=\u001b[39m [ex\u001b[38;5;241m.\u001b[39msubmit(_map_search_worker, t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tasks]\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fut \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[1;32m--> 785\u001b[0m     bestF, score, logs \u001b[38;5;241m=\u001b[39m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m best_score:\n\u001b[0;32m    787\u001b[0m         best_global, best_score, best_logs \u001b[38;5;241m=\u001b[39m bestF, score, logs\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "MAP structure search for Carta-CDMIP model.\n",
    "\n",
    "Score(F) = log P(F) + sum_T log P(T|F),\n",
    "where P(T|F) = sum_{labelings} B(O+1, D+1), with per-node counts:\n",
    "  obs = |L ∩ B(v)|, miss = |L \\ B(v)|, and B(·) is Beta function.\n",
    "p ~ Beta(1,1) is integrated out exactly.\n",
    "\n",
    "- Newick parser (no external deps)\n",
    "- DP over labelings with (O,D) sparse tables\n",
    "- Priors: fixed-k (uniform over potency sets) OR Bernoulli(pi_P); edges Bernoulli(rho)\n",
    "- Stochastic hill-climb + simulated annealing over F=(Z,A)\n",
    "\"\"\"\n",
    "from tqdm import trange\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Tuple, List, Optional, Set, FrozenSet\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# Tree structures and Newick\n",
    "# ----------------------------\n",
    "def build_mid_sized_connected_dag(Z_active, keep_prob=0.3, unit_drop=False, rng=None):\n",
    "    \"\"\"\n",
    "    Build a valid mid-density DAG:\n",
    "      * Uses only admissible edges\n",
    "      * Guarantees connectivity from the root node (frozenset of all singletons)\n",
    "      * Keeps density moderate, controlled by `keep_prob`\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = random.Random()\n",
    "\n",
    "    # --- Identify root node (the potency containing all singletons) ---\n",
    "    root = frozenset().union(*Z_active)  # union of all labels gives the full set\n",
    "    print(\"ROot \",root)\n",
    "    if root not in Z_active:\n",
    "        raise ValueError(\"Root potency (all singletons) not present in Z_active.\")\n",
    "\n",
    "    nodes = list(Z_active)\n",
    "\n",
    "    # --- Step 1: Build full admissible edge set ---\n",
    "    full_edges = {\n",
    "        (P, Q): 1\n",
    "        for P in Z_active\n",
    "        for Q in Z_active\n",
    "        if P != Q and admissible_edge(P, Q, unit_drop)\n",
    "    }\n",
    "\n",
    "    # --- Step 2: Start with a spanning tree to guarantee connectivity ---\n",
    "    A = {}\n",
    "    visited = {root}\n",
    "    to_visit = set(nodes) - {root}\n",
    "\n",
    "    while to_visit:\n",
    "        # pick a node already in the tree\n",
    "        parent = rng.choice(list(visited))\n",
    "\n",
    "        # find valid edges from parent to some unvisited node\n",
    "        candidates = [(parent, q) for q in to_visit if (parent, q) in full_edges]\n",
    "\n",
    "        if not candidates:\n",
    "            # fallback: pick any edge between visited and unvisited nodes\n",
    "            candidates = [\n",
    "                (p, q) for p in visited for q in to_visit if (p, q) in full_edges\n",
    "            ]\n",
    "\n",
    "        edge = rng.choice(candidates)\n",
    "        A[edge] = 1\n",
    "        visited.add(edge[1])\n",
    "        to_visit.remove(edge[1])\n",
    "\n",
    "    # --- Step 3: Add extra edges randomly to reach desired density ---\n",
    "    for edge in full_edges:\n",
    "        if edge in A:\n",
    "            continue\n",
    "        if rng.random() < keep_prob:\n",
    "            A[edge] = 1\n",
    "\n",
    "    return A\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        self.name: Optional[str] = name\n",
    "        self.children: List[\"TreeNode\"] = []\n",
    "        self.parent: Optional[\"TreeNode\"] = None\n",
    "\n",
    "    def is_leaf(self): return len(self.children) == 0\n",
    "    def add_child(self, child: \"TreeNode\"):\n",
    "        self.children.append(child); child.parent = self\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Leaf({self.name})\" if self.is_leaf() else f\"Node({self.name}, k={len(self.children)})\"\n",
    "\n",
    "# def parse_newick(newick: str) -> TreeNode:\n",
    "#     s = newick.strip()\n",
    "#     if not s.endswith(\";\"): raise ValueError(\"Newick must end with ';'\")\n",
    "#     s = s[:-1]; i = 0\n",
    "#     def parse() -> TreeNode:\n",
    "#         nonlocal i, s\n",
    "#         if i >= len(s): raise ValueError(\"Unexpected end\")\n",
    "#         if s[i] == '(':\n",
    "#             i += 1\n",
    "#             node = TreeNode()\n",
    "#             while True:\n",
    "#                 node.add_child(parse())\n",
    "#                 if i >= len(s): raise ValueError(\"Unbalanced\")\n",
    "#                 if s[i] == ',':\n",
    "#                     i += 1; continue\n",
    "#                 elif s[i] == ')':\n",
    "#                     i += 1; break\n",
    "#                 else: raise ValueError(f\"Unexpected char: {s[i]} at {i}\")\n",
    "#             j = i\n",
    "#             while j < len(s) and s[j] not in ',()': j += 1\n",
    "#             name = s[i:j].strip()\n",
    "#             if name: node.name = name\n",
    "#             i = j\n",
    "#             return node\n",
    "#         else:\n",
    "#             j = i\n",
    "#             while j < len(s) and s[j] not in ',()': j += 1\n",
    "#             name = s[i:j].strip()\n",
    "#             if not name: raise ValueError(\"Leaf without name\")\n",
    "#             i = j\n",
    "#             return TreeNode(name=name)\n",
    "#     root = parse()\n",
    "#     if i != len(s): raise ValueError(f\"Trailing characters: '{s[i:]}'\")\n",
    "#     return root\n",
    "\n",
    "def parse_newick(newick: str) -> TreeNode:\n",
    "    # Helper: strip branch length and numeric-only labels\n",
    "    def _clean_label(tok: str) -> str:\n",
    "        # remove branch length: keep part before first ':'\n",
    "        tok = tok.split(\":\", 1)[0].strip()\n",
    "        # drop pure numeric internal labels like \"357\"\n",
    "        if tok and tok.replace(\".\", \"\", 1).isdigit():\n",
    "            return \"\"\n",
    "        return tok\n",
    "\n",
    "    s = newick.strip()\n",
    "    if not s.endswith(\";\"): raise ValueError(\"Newick must end with ';'\")\n",
    "    s = s[:-1]; i = 0\n",
    "\n",
    "    def parse() -> TreeNode:\n",
    "        nonlocal i, s\n",
    "        if i >= len(s): raise ValueError(\"Unexpected end\")\n",
    "        if s[i] == '(':\n",
    "            i += 1\n",
    "            node = TreeNode()\n",
    "            while True:\n",
    "                node.add_child(parse())\n",
    "                if i >= len(s): raise ValueError(\"Unbalanced\")\n",
    "                if s[i] == ',':\n",
    "                    i += 1; continue\n",
    "                elif s[i] == ')':\n",
    "                    i += 1; break\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected char: {s[i]} at {i}\")\n",
    "\n",
    "            # optional internal node label (may include branch length)\n",
    "            j = i\n",
    "            while j < len(s) and s[j] not in ',()': j += 1\n",
    "            name_raw = s[i:j].strip()\n",
    "            name = _clean_label(name_raw)\n",
    "            if name:  # keep non-empty, non-numeric labels only\n",
    "                node.name = name\n",
    "            i = j\n",
    "            return node\n",
    "        else:\n",
    "            # leaf label (may include branch length)\n",
    "            j = i\n",
    "            while j < len(s) and s[j] not in ',()': j += 1\n",
    "            name_raw = s[i:j].strip()\n",
    "            name = _clean_label(name_raw)\n",
    "            if not name:\n",
    "                raise ValueError(\"Leaf without name\")\n",
    "            i = j\n",
    "            return TreeNode(name=name)\n",
    "\n",
    "    root = parse()\n",
    "    if i != len(s): raise ValueError(f\"Trailing characters: '{s[i:]}'\")\n",
    "    return root\n",
    "\n",
    "def to_newick(root: TreeNode) -> str:\n",
    "    def rec(n: TreeNode) -> str:\n",
    "        if n.is_leaf(): return n.name or \"\"\n",
    "        return f\"({','.join(rec(c) for c in n.children)}){n.name or ''}\"\n",
    "    return rec(root) + \";\"\n",
    "\n",
    "def read_newick_file(path: str) -> TreeNode:\n",
    "    with open(path, \"r\") as f: s = f.read().strip()\n",
    "    return parse_newick(s)\n",
    "\n",
    "def write_newick_file(path: str, root: TreeNode):\n",
    "    with open(path, \"w\") as f: f.write(to_newick(root) + \"\\n\")\n",
    "\n",
    "def random_tree_newick(n_leaves: int, leaf_prefix=\"L\") -> Tuple[TreeNode, List[str]]:\n",
    "    leaves = [TreeNode(f\"{leaf_prefix}{i+1}\") for i in range(n_leaves)]\n",
    "    nodes = leaves[:]\n",
    "    while len(nodes) > 1:\n",
    "        k = 2 if len(nodes) < 4 else random.choice([2,2,2,3])\n",
    "        k = min(k, len(nodes))\n",
    "        picks = random.sample(nodes, k)\n",
    "        for p in picks: nodes.remove(p)\n",
    "        parent = TreeNode()\n",
    "        for p in picks: parent.add_child(p)\n",
    "        nodes.append(parent)\n",
    "    return nodes[0], [l.name for l in leaves]\n",
    "\n",
    "def collect_leaf_names(root: TreeNode) -> List[str]:\n",
    "    out=[]\n",
    "    def dfs(v):\n",
    "        if v.is_leaf(): out.append(v.name)\n",
    "        else:\n",
    "            for c in v.children: dfs(c)\n",
    "    dfs(root); return out\n",
    "\n",
    "# ----------------------------\n",
    "# Potency universe and structure\n",
    "# ----------------------------\n",
    "\n",
    "def all_nonempty_subsets(S: List[str], max_size: Optional[int]=None) -> List[FrozenSet[str]]:\n",
    "    R=len(S); max_k = R if max_size is None else min(max_size, R)\n",
    "    res=[]\n",
    "    for k in range(1, max_k+1):\n",
    "        for comb in itertools.combinations(S, k): res.append(frozenset(comb))\n",
    "    return res\n",
    "\n",
    "def singletons(S: List[str]) -> Set[FrozenSet[str]]:\n",
    "    return {frozenset([t]) for t in S}\n",
    "\n",
    "def build_Z_active(S: List[str], fixed_k: Optional[int], max_potency_size: Optional[int], seed=0) -> Set[FrozenSet[str]]:\n",
    "    rng = random.Random(seed)\n",
    "    P_all = all_nonempty_subsets(S, max_potency_size)\n",
    "    singles = singletons(S)\n",
    "    multis = [P for P in P_all if len(P)>=2]\n",
    "    Z = set(singles)\n",
    "    if fixed_k is not None:\n",
    "        if fixed_k > len(multis):\n",
    "            raise ValueError(\"fixed_k too large\")\n",
    "        root = frozenset(S)\n",
    "        Z.add(root)\n",
    "        remaining_multis = [P for P in multis if P != root]\n",
    "        Z.update(rng.sample(remaining_multis, fixed_k - 1))  # pick k-1 more\n",
    "    else:\n",
    "        Z.update(multis)\n",
    "    return Z\n",
    "\n",
    "def admissible_edge(P: FrozenSet[str], Q: FrozenSet[str], unit_drop: bool) -> bool:\n",
    "    if Q == P: return False\n",
    "    if not Q.issubset(P): return False\n",
    "    if len(Q) >= len(P): return False\n",
    "    if unit_drop and len(P - Q) != 1: return False\n",
    "    return True\n",
    "\n",
    "def build_edges(Z_active: Set[FrozenSet[str]], forbid_fn=None, unit_drop=True) -> Dict[Tuple[FrozenSet[str],FrozenSet[str]], int]:\n",
    "    A={}\n",
    "    for P in Z_active:\n",
    "        for Q in Z_active:\n",
    "            if not admissible_edge(P,Q,unit_drop): continue\n",
    "            if forbid_fn and forbid_fn(P,Q): continue\n",
    "            A[(P,Q)] = 1\n",
    "    return A\n",
    "\n",
    "def transitive_closure(labels: List[FrozenSet[str]], A: Dict[Tuple[FrozenSet[str],FrozenSet[str]], int]) -> Dict[FrozenSet[str], Set[FrozenSet[str]]]:\n",
    "    idx = {L:i for i,L in enumerate(labels)}\n",
    "    n=len(labels)\n",
    "    M=[[False]*n for _ in range(n)]\n",
    "    for i in range(n): M[i][i]=True\n",
    "    for (P,Q),v in A.items():\n",
    "        if v:\n",
    "            i,j=idx[P],idx[Q]; M[i][j]=True\n",
    "    for k in range(n):\n",
    "        Mk=M[k]\n",
    "        for i in range(n):\n",
    "            if M[i][k]:\n",
    "                Mi=M[i]\n",
    "                for j in range(n):\n",
    "                    if Mk[j]: Mi[j]=True\n",
    "    Reach={L:set() for L in labels}\n",
    "    for i,L in enumerate(labels):\n",
    "        for j,U in enumerate(labels):\n",
    "            if M[i][j]: Reach[L].add(U)\n",
    "    return Reach\n",
    "\n",
    "# ----------------------------\n",
    "# DP over labelings (integrated Beta)\n",
    "# ----------------------------\n",
    "\n",
    "def compute_B_sets(root: TreeNode, leaf_to_type: Dict[str,str]) -> Dict[TreeNode, Set[str]]:\n",
    "    B={}\n",
    "    def post(v: TreeNode) -> Set[str]:\n",
    "        if v.is_leaf():\n",
    "            t = leaf_to_type.get(v.name)\n",
    "            # Missing mapping? Ignore this leaf by contributing an empty set.\n",
    "            B[v] = {t} if t is not None else set()\n",
    "            return B[v]\n",
    "        acc=set()\n",
    "        for c in v.children: acc |= post(c)\n",
    "        B[v]=acc; return acc\n",
    "    post(root); return B\n",
    "\n",
    "def beta_integral(O:int,D:int)->float:\n",
    "    # ∫ p^O (1-p)^D dp over [0,1] = B(O+1,D+1)\n",
    "    return math.exp(math.lgamma(O+1)+math.lgamma(D+1)-math.lgamma(O+D+2))\n",
    "\n",
    "def sparse_convolve_2d(A: Dict[Tuple[int,int],float], B: Dict[Tuple[int,int],float]) -> Dict[Tuple[int,int],float]:\n",
    "    if not A: return B.copy()\n",
    "    if not B: return A.copy()\n",
    "    out=defaultdict(float)\n",
    "    for (o1,d1),w1 in A.items():\n",
    "        for (o2,d2),w2 in B.items():\n",
    "            out[(o1+o2,d1+d2)] += w1*w2\n",
    "    return dict(out)\n",
    "\n",
    "def dp_tree_root_table(\n",
    "    root: TreeNode,\n",
    "    active_labels: List[FrozenSet[str]],\n",
    "    Reach: Dict[FrozenSet[str], Set[FrozenSet[str]]],\n",
    "    B_sets: Dict[TreeNode, Set[str]],\n",
    "    prune_eps: float = 0.0\n",
    ")->Dict[Tuple[int,int],float]:\n",
    "    label_index={L:i for i,L in enumerate(active_labels)}\n",
    "    memo: Dict[Tuple[int,int], Dict[Tuple[int,int],float]]={}\n",
    "    def nid(v:TreeNode)->int: return id(v)\n",
    "\n",
    "    def M(v:TreeNode, P: Optional[FrozenSet[str]])->Dict[Tuple[int,int],float]:\n",
    "        key=(nid(v), -1 if P is None else label_index[P])\n",
    "        if key in memo: return memo[key]\n",
    "        if v.is_leaf():\n",
    "            memo[key] = {(0,0):1.0}; return memo[key]\n",
    "        Bv=B_sets[v]\n",
    "        out=defaultdict(float)\n",
    "        if P is None:\n",
    "            parent_reach = active_labels\n",
    "        else:\n",
    "            parent_reach = list(Reach[P])\n",
    "\n",
    "        for L in parent_reach:\n",
    "            if not Bv.issubset(L):  # containment constraint\n",
    "                continue\n",
    "            o_local=len(L & Bv); d_local=len(L - Bv)\n",
    "            # children messages conditioned on parent label = L\n",
    "            child_tabs=[]\n",
    "            ok=True\n",
    "            for u in v.children:\n",
    "                tab = M(u, L)\n",
    "                if not tab: ok=False; break\n",
    "                child_tabs.append(tab)\n",
    "            if not ok: continue\n",
    "            conv = child_tabs[0] if child_tabs else {(0,0):1.0}\n",
    "            for t in child_tabs[1:]:\n",
    "                conv = sparse_convolve_2d(conv, t)\n",
    "            for (Oc,Dc),w in conv.items():\n",
    "                out[(Oc+o_local, Dc+d_local)] += w\n",
    "\n",
    "        if prune_eps>0 and out:\n",
    "            total=sum(out.values()); thresh=prune_eps*total\n",
    "            out={k:v for k,v in out.items() if v>=thresh}\n",
    "        memo[key]=dict(out); return memo[key]\n",
    "\n",
    "    return M(root, None)\n",
    "\n",
    "def tree_marginal_from_root_table(C: Dict[Tuple[int,int],float])->float:\n",
    "    return sum(w * beta_integral(O,D) for (O,D),w in C.items())\n",
    "\n",
    "# ----------------------------\n",
    "# Priors and scoring\n",
    "# ----------------------------\n",
    "\n",
    "class Priors:\n",
    "    def __init__(self,\n",
    "                 potency_mode:str=\"fixed_k\",  # \"fixed_k\" or \"bernoulli\"\n",
    "                 fixed_k:int=2,\n",
    "                 pi_P:float=0.25,    # used if potency_mode=\"bernoulli\"\n",
    "                 rho:float=0.25):    # edge Bernoulli prob\n",
    "        # ------------------------------------------------------------\n",
    "        # Stores hyperparameters for the prior over the structure F=(Z,A)\n",
    "        #     Z: The latent assignment of \"potencies\" or features to nodes (the sets like {A,B,C}, {B,C,D}, etc. that you saw in the MAP output).\n",
    "        #     A: The active structure (the adjacency or edge set) consistent with those potencies -- basically the graph/hypergraph that the algorithm thinks best explains the observed trees.\n",
    "        #   - potency_mode: which prior to use over active multi-type potencies Z\n",
    "        #       * \"fixed_k\": exactly k multi-type potencies are active (uniform over choices)\n",
    "        #       * \"bernoulli\": each multi-type potency is independently active with prob pi_P\n",
    "        #   - fixed_k: number of multi-type potencies when potency_mode == \"fixed_k\"\n",
    "        #   - pi_P: inclusion probability for each multi-type potency when using \"bernoulli\" mode\n",
    "        #   - rho: prior probability that any admissible edge (P->Q) exists\n",
    "        # ------------------------------------------------------------\n",
    "        self.potency_mode=potency_mode\n",
    "        self.fixed_k=fixed_k\n",
    "        self.pi_P=pi_P\n",
    "        self.rho=rho\n",
    "\n",
    "    def log_prior_Z(self, S: List[str], Z_active:Set[FrozenSet[str]])->float: #Z_active = the set of active potencies (both singletons and multis).\n",
    "        # ------------------------------------------------------------\n",
    "        # Computes log P(Z): the log prior over WHICH potencies are active.\n",
    "        #\n",
    "        # Inputs:\n",
    "        #   - S: list of all cell types (leaf types), e.g., [\"A\",\"B\",\"C\",\"D\"]\n",
    "        #   - Z_active: set of active potencies (as frozensets). Includes singletons by construction.\n",
    "        #\n",
    "        # Key ideas:\n",
    "        #   - Singletons are always considered active (terminal states), we don't penalize/score them.\n",
    "        #   - We only place a prior over multi-type potencies (size >= 2).\n",
    "        #   - Two modes:\n",
    "        #       * \"fixed_k\": valid only if exactly `fixed_k` multis are active.\n",
    "        #                    Prior is uniform over all C(M, k) choices, where M = #all possible multis.\n",
    "        #       * \"bernoulli\": each possible multi is included independently with prob pi_P.\n",
    "        #                      log prior sums log(pi_P) for included multis and log(1-pi_P) for excluded ones.\n",
    "        # Returns:\n",
    "        #   - log prior (float), or -inf if configuration violates \"fixed_k\".\n",
    "        # ------------------------------------------------------------\n",
    "        singles = singletons(S)\n",
    "        multis = [P for P in Z_active if len(P)>=2] #P is a particular potency set\n",
    "        # count available multi potencies (for fixed-k uniform)\n",
    "        all_multis = [P for P in all_nonempty_subsets(S) if len(P)>=2]\n",
    "\n",
    "        if self.potency_mode==\"fixed_k\":\n",
    "            # ------------------------------\n",
    "            # Uniform prior over all subsets of multi-type potencies with EXACTLY k elements.\n",
    "            # If the current Z_active has not exactly k multis, return -inf (outside prior support).\n",
    "            # Otherwise, log prior = -log( number of such subsets ) = -log( nCk ).\n",
    "            # ------------------------------\n",
    "            k=len(multis)\n",
    "            if k!=self.fixed_k:\n",
    "                return float(\"-inf\")\n",
    "            # uniform over all C(|all_multis|, k)\n",
    "            total = math.comb(len(all_multis), k) #this is nCk\n",
    "            return -math.log(total) if total>0 else float(\"-inf\")\n",
    "        else:\n",
    "            # ------------------------------\n",
    "            # Bernoulli prior on each multi-type potency:\n",
    "            #   P(Z) = ∏_{P in all_multis} pi_P^{I[P in Z]} (1 - pi_P)^{I[P not in Z]}\n",
    "            # We sum logs across all possible multi-type potencies (singletons ignored).\n",
    "            # ------------------------------\n",
    "            k_log=0.0\n",
    "            for P in all_multis:\n",
    "                if P in Z_active: k_log += math.log(self.pi_P)\n",
    "                else: k_log += math.log(1-self.pi_P)\n",
    "            return k_log\n",
    "\n",
    "    def log_prior_A(self, Z_active:Set[FrozenSet[str]], A:Dict[Tuple[FrozenSet[str],FrozenSet[str]],int], unit_drop=True)->float:\n",
    "        # ------------------------------------------------------------\n",
    "        # Computes log P(A | Z): the log prior over EDGE EXISTENCE between active potencies.\n",
    "        #\n",
    "        # Inputs:\n",
    "        #   - Z_active: set of active potencies (nodes in the potency DAG)\n",
    "        #   - A: adjacency dictionary mapping (P,Q) -> {0,1}, indicating whether edge P->Q is present\n",
    "        #   - unit_drop: if True, an admissible edge must drop EXACTLY one fate (|P\\Q| == 1);\n",
    "        #                otherwise any monotone subset drop (Q ⊂ P) is admissible.\n",
    "        #\n",
    "        # Prior:\n",
    "        #   - For every admissible pair (P,Q):\n",
    "        #         A_{P->Q} ~ Bernoulli(rho)\n",
    "        #     So:\n",
    "        #         log P(A|Z) = ∑_{(P,Q) admissible} [ A_{P->Q} log(rho) + (1 - A_{P->Q}) log(1 - rho) ]\n",
    "        #\n",
    "        # Notes:\n",
    "        #   - \"Admissible\" enforces graph shape constraints (subset-monotone and possibly unit-drop).\n",
    "        #   - If an edge (P,Q) is not admissible, it does not contribute to the product/sum at all.\n",
    "        # ------------------------------------------------------------\n",
    "        labels=list(Z_active)\n",
    "        # admissible set is pairs with subset monotone (and optionally unit-drop)\n",
    "        logp=0.0\n",
    "        for P in labels:\n",
    "            for Q in labels:\n",
    "                if admissible_edge(P,Q,unit_drop):\n",
    "                    # a == 1 if the edge is present in A, else 0\n",
    "                    a = 1 if A.get((P,Q),0)==1 else 0\n",
    "                    # add Bernoulli log-prob for this edge\n",
    "                    logp += math.log(self.rho) if a==1 else math.log(1-self.rho)\n",
    "        return logp\n",
    "\n",
    "# ----------------------------\n",
    "# Structure container and proposals\n",
    "# ----------------------------\n",
    "\n",
    "class Structure:\n",
    "    def __init__(self,\n",
    "                 S: List[str],\n",
    "                 Z_active: Set[FrozenSet[str]],\n",
    "                 A: Dict[Tuple[FrozenSet[str],FrozenSet[str]],int],\n",
    "                 unit_drop: bool = True):\n",
    "        self.S=S\n",
    "        self.Z_active=set(Z_active)  # includes singletons\n",
    "        self.A=dict(A)\n",
    "        self.unit_drop=unit_drop\n",
    "        self.labels_list=self._sorted_labels()\n",
    "        self.Reach = transitive_closure(self.labels_list, self.A)\n",
    "\n",
    "    def _sorted_labels(self)->List[FrozenSet[str]]:\n",
    "        return sorted(list(self.Z_active), key=lambda x: (len(x), tuple(sorted(list(x)))))\n",
    "\n",
    "    def recompute_reach(self):\n",
    "        self.labels_list=self._sorted_labels()\n",
    "        self.Reach = transitive_closure(self.labels_list, self.A)\n",
    "\n",
    "    def clone(self)->\"Structure\":\n",
    "        return Structure(self.S, set(self.Z_active), dict(self.A), self.unit_drop)\n",
    "\n",
    "    # --- Moves ---\n",
    "    def potencies_multi_all(self)->List[FrozenSet[str]]:\n",
    "        return [P for P in all_nonempty_subsets(self.S) if len(P)>=2]\n",
    "\n",
    "    def propose_add_potency(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        candidates = [P for P in self.potencies_multi_all() if P not in self.Z_active]\n",
    "        if not candidates: return None\n",
    "        P = rng.choice(candidates)\n",
    "        new = self.clone()\n",
    "        new.Z_active.add(P)\n",
    "        # add edges that respect admissibility? keep edges as-is and allow edge moves separately\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "\n",
    "    def propose_remove_potency(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        candidates = [P for P in self.Z_active if len(P)>=2]\n",
    "        if not candidates: return None\n",
    "        P = rng.choice(candidates)\n",
    "        new = self.clone()\n",
    "        # remove potency and incident edges\n",
    "        new.Z_active.remove(P)\n",
    "        new.A = {e:v for e,v in new.A.items() if P not in e}\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "\n",
    "    def propose_swap_potency(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        remove_candidates = [P for P in self.Z_active if len(P)>=2]\n",
    "        add_candidates = [P for P in self.potencies_multi_all() if P not in self.Z_active]\n",
    "        if not remove_candidates or not add_candidates: return None\n",
    "        P_rm = rng.choice(remove_candidates)\n",
    "        P_add = rng.choice(add_candidates)\n",
    "        new = self.clone()\n",
    "        new.Z_active.remove(P_rm)\n",
    "        new.A = {e:v for e,v in new.A.items() if P_rm not in e}\n",
    "        new.Z_active.add(P_add)\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "\n",
    "    def all_edge_pairs(self)->List[Tuple[FrozenSet[str],FrozenSet[str]]]:\n",
    "        L=list(self.Z_active)\n",
    "        pairs=[]\n",
    "        for P in L:\n",
    "            for Q in L:\n",
    "                if admissible_edge(P,Q,self.unit_drop):\n",
    "                    pairs.append((P,Q))\n",
    "        return pairs\n",
    "\n",
    "    def propose_add_edge(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        pairs = [e for e in self.all_edge_pairs() if self.A.get(e,0)==0]\n",
    "        if not pairs: return None\n",
    "        e = rng.choice(pairs)\n",
    "        new = self.clone()\n",
    "        new.A[e]=1\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "\n",
    "    def propose_remove_edge(self, rng:random.Random)->Optional[\"Structure\"]:\n",
    "        edges = [e for e,v in self.A.items() if v==1]\n",
    "        if not edges: return None\n",
    "        e = rng.choice(edges)\n",
    "        new = self.clone()\n",
    "        del new.A[e]\n",
    "        new.recompute_reach()\n",
    "        return new\n",
    "\n",
    "# ----------------------------\n",
    "# Scoring: log posterior\n",
    "# ----------------------------\n",
    "\n",
    "def score_structure(struct: Structure,\n",
    "                    trees: List[TreeNode],\n",
    "                    leaf_type_maps: List[Dict[str,str]],\n",
    "                    priors: Priors,\n",
    "                    prune_eps: float = 0.0) -> Tuple[float, List[float]]:\n",
    "    # log prior\n",
    "    logp = priors.log_prior_Z(struct.S, struct.Z_active)\n",
    "    #print(f\"logp:{logp}\")\n",
    "    if not math.isfinite(logp):\n",
    "        logp = float(\"-inf\")\n",
    "    logp += priors.log_prior_A(struct.Z_active, struct.A, unit_drop=struct.unit_drop)\n",
    "\n",
    "    # likelihood\n",
    "    logLs=[]\n",
    "    for root, leaf_to_type in zip(trees, leaf_type_maps):\n",
    "        B_sets = compute_B_sets(root, leaf_to_type)\n",
    "\n",
    "        # --- NEW: if the root has no labels at all, skip this tree (neutral evidence) ---\n",
    "        root_labels = B_sets.get(root, set())\n",
    "        if not root_labels:\n",
    "            logLs.append(0.0)\n",
    "            continue\n",
    "        # -------------------------------------------------------------------------------\n",
    "\n",
    "        C = dp_tree_root_table(root, struct.labels_list, struct.Reach, B_sets, prune_eps=prune_eps)\n",
    "        P_T = tree_marginal_from_root_table(C)\n",
    "        #print(f\"P_T:{P_T}\")\n",
    "        if P_T <= 0 or not math.isfinite(P_T):\n",
    "            return float(\"-inf\"), []\n",
    "        logLs.append(math.log(P_T))\n",
    "    return logp + sum(logLs), logLs\n",
    "def map_search(\n",
    "    S: List[str],\n",
    "    trees: List[TreeNode],\n",
    "    leaf_type_maps: List[Dict[str,str]],\n",
    "    priors: Priors,\n",
    "    unit_drop_edges: bool = True,\n",
    "    fixed_k: Optional[int] = None,\n",
    "    init_seed: int = 0,\n",
    "    iters: int = 500,\n",
    "    restarts: int = 3,\n",
    "    temp_init: float = 1.0,\n",
    "    temp_decay: float = 0.995,\n",
    "    move_probs = (0.25, 0.25, 0.25, 0.25),  # addP, rmP, addE, rmE (swap used when fixed_k)\n",
    "    prune_eps: float = 0.0,\n",
    "    progress: bool = True,\n",
    "):\n",
    "    rng = random.Random(init_seed)\n",
    "\n",
    "    best_global = None\n",
    "    best_score = float(\"-inf\")\n",
    "    best_logs = None\n",
    "\n",
    "    for rs in range(restarts):\n",
    "        # --- init structure\n",
    "        print(rs)\n",
    "        if priors.potency_mode==\"fixed_k\":\n",
    "            Z = build_Z_active(S, fixed_k=priors.fixed_k, max_potency_size=len(S), seed=rng.randint(0,10**9))\n",
    "        else:\n",
    "            base = build_Z_active(S, fixed_k=0, max_potency_size=len(S), seed=rng.randint(0,10**9))\n",
    "            Z = base\n",
    "        A = build_mid_sized_connected_dag(Z,keep_prob = 0.3,rng = None)\n",
    "        current = Structure(S, Z, A, unit_drop=unit_drop_edges)\n",
    "        curr_score, _ = score_structure(current, trees, leaf_type_maps, priors, prune_eps)\n",
    "\n",
    "        # fallback: if invalid, keep sampling until valid\n",
    "        attempts = 0\n",
    "        while not math.isfinite(curr_score) and attempts < 720:\n",
    "            Z = build_Z_active(S, fixed_k=(priors.fixed_k if priors.potency_mode==\"fixed_k\" else 0),\n",
    "                               max_potency_size=len(S), seed=rng.randint(0,10**9))\n",
    "            A = build_mid_sized_connected_dag(Z,keep_prob = 0.3,rng = None)\n",
    "            # A = {}\n",
    "            # print(f\"Z:{Z}\"); print(f\"A:{A}\")\n",
    "            current = Structure(S, Z, A, unit_drop=unit_drop_edges)\n",
    "            curr_score, _ = score_structure(current, trees, leaf_type_maps, priors, prune_eps)\n",
    "            # print(curr_score)\n",
    "            attempts += 1\n",
    "\n",
    "        if not math.isfinite(curr_score):\n",
    "            raise RuntimeError(\"Failed to initialize a valid structure; consider easing settings.\")\n",
    "\n",
    "        local_best = current.clone()\n",
    "        local_best_score = curr_score\n",
    "\n",
    "        tau = temp_init\n",
    "        addP, rmP, addE, rmE = move_probs\n",
    "\n",
    "        # iterator respects the 'progress' flag\n",
    "        iterator = trange(iters, desc=f\"Restart {rs+1}/{restarts}\", leave=True) if progress else range(iters)\n",
    "\n",
    "        for _ in iterator:\n",
    "            # choose move\n",
    "            prop = None\n",
    "            r = rng.random()\n",
    "            if priors.potency_mode==\"fixed_k\":\n",
    "                if r < addE:\n",
    "                    prop = current.propose_add_edge(rng)\n",
    "                elif r < addE + rmE:\n",
    "                    prop = current.propose_remove_edge(rng)\n",
    "                else:\n",
    "                    prop = current.propose_swap_potency(rng)\n",
    "            else:\n",
    "                if r < addP:\n",
    "                    prop = current.propose_add_potency(rng)\n",
    "                elif r < addP + rmP:\n",
    "                    prop = current.propose_remove_potency(rng)\n",
    "                elif r < addP + rmP + addE:\n",
    "                    prop = current.propose_add_edge(rng)\n",
    "                else:\n",
    "                    prop = current.propose_remove_edge(rng)\n",
    "\n",
    "            if prop is None:\n",
    "                tau *= temp_decay\n",
    "                if progress:\n",
    "                    iterator.set_postfix({\"Best\": f\"{best_score:.3f}\", \"Curr\": f\"{curr_score:.3f}\", \"Temp\": f\"{tau:.3f}\"})\n",
    "                continue\n",
    "\n",
    "            prop_score, _ = score_structure(prop, trees, leaf_type_maps, priors, prune_eps)\n",
    "\n",
    "            delta = prop_score - curr_score\n",
    "            accept = (delta >= 0) or (rng.random() < math.exp(delta / max(tau,1e-12)))\n",
    "            if accept:\n",
    "                current = prop\n",
    "                curr_score = prop_score\n",
    "\n",
    "                if curr_score > local_best_score:\n",
    "                    local_best = current.clone()\n",
    "                    local_best_score = curr_score\n",
    "\n",
    "                if curr_score > best_score:\n",
    "                    best_global = current.clone()\n",
    "                    best_score = curr_score\n",
    "                    best_logs = None  # compute later if needed\n",
    "\n",
    "            tau *= temp_decay\n",
    "            print(f\"rs:{rs},curr{curr_score}\")\n",
    "            if progress:\n",
    "                iterator.set_postfix({\"Best\": f\"{best_score:.3f}\", \"Curr\": f\"{curr_score:.3f}\", \"Temp\": f\"{tau:.3f}\"})\n",
    "\n",
    "    # after restarts, recompute detailed logs for best_global\n",
    "    final_score, logLs = score_structure(best_global, trees, leaf_type_maps, priors, prune_eps)\n",
    "    return best_global, final_score, logLs\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import os\n",
    "\n",
    "def _map_search_worker(args):\n",
    "    (S, trees, leaf_type_maps, priors, unit_drop_edges, fixed_k,\n",
    "     init_seed, iters, restarts, temp_init, temp_decay, move_probs, prune_eps) = args\n",
    "    # progress=False inside workers to avoid tqdm noise\n",
    "    return map_search(\n",
    "        S=S,\n",
    "        trees=trees,\n",
    "        leaf_type_maps=leaf_type_maps,\n",
    "        priors=priors,\n",
    "        unit_drop_edges=unit_drop_edges,\n",
    "        fixed_k=fixed_k,\n",
    "        init_seed=init_seed,\n",
    "        iters=iters,\n",
    "        restarts=restarts,\n",
    "        temp_init=temp_init,\n",
    "        temp_decay=temp_decay,\n",
    "        move_probs=move_probs,\n",
    "        prune_eps=prune_eps,\n",
    "        progress=True,\n",
    "    )\n",
    "\n",
    "def map_search_parallel(\n",
    "    S: List[str],\n",
    "    trees: List[TreeNode],\n",
    "    leaf_type_maps: List[Dict[str,str]],\n",
    "    priors: Priors,\n",
    "    unit_drop_edges: bool = True,\n",
    "    fixed_k: Optional[int] = None,\n",
    "    init_seed: int = 0,\n",
    "    iters: int = 500,\n",
    "    restarts: int = 12,\n",
    "    temp_init: float = 1.0,\n",
    "    temp_decay: float = 0.995,\n",
    "    move_probs = (0.25, 0.25, 0.25, 0.25),\n",
    "    prune_eps: float = 0.0,\n",
    "    n_jobs: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallelizes restarts across processes and returns the best result.\n",
    "    NOTE: On Windows/macOS, call this under `if __name__ == \"__main__\":` to avoid spawn issues.\n",
    "    \"\"\"\n",
    "    if n_jobs is None:\n",
    "        n_jobs = max(1, (os.cpu_count() or 2) - 1)\n",
    "\n",
    "    # Split restarts across jobs\n",
    "    per_job = [restarts // n_jobs] * n_jobs\n",
    "    for i in range(restarts % n_jobs):\n",
    "        per_job[i] += 1\n",
    "    per_job = [r for r in per_job if r > 0]\n",
    "    n_jobs = len(per_job)\n",
    "\n",
    "    # Unique seeds per worker to diversify trajectories\n",
    "    seeds = [init_seed + 10_000 * i for i in range(n_jobs)]\n",
    "\n",
    "    tasks = []\n",
    "    for r, seed in zip(per_job, seeds):\n",
    "        tasks.append((S, trees, leaf_type_maps, priors, unit_drop_edges, fixed_k,\n",
    "                      seed, iters, r, temp_init, temp_decay, move_probs, prune_eps))\n",
    "\n",
    "    best_global = None\n",
    "    best_score = float(\"-inf\")\n",
    "    best_logs = None\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as ex:\n",
    "        futures = [ex.submit(_map_search_worker, t) for t in tasks]\n",
    "        for fut in as_completed(futures):\n",
    "            bestF, score, logs = fut.result()\n",
    "            if score > best_score:\n",
    "                best_global, best_score, best_logs = bestF, score, logs\n",
    "\n",
    "    return best_global, best_score, best_logs\n",
    "\n",
    "########\n",
    "#\n",
    "#\n",
    "# MAP TXT READING\n",
    "#\n",
    "########\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def read_leaf_type_map(path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Read a leaf->type mapping from a file.\n",
    "\n",
    "    Supported:\n",
    "      - JSON dict: { \"LeafName\": \"Type\", ... }\n",
    "      - CSV/TSV/TXT with 2 columns (header optional):\n",
    "          * If header present, typical field names could be:\n",
    "              - leaf, type\n",
    "              - cellBC, cell_state (your .txt example)\n",
    "    Returns: dict {leaf_name: type_symbol} (types are coerced to str)\n",
    "    \"\"\"\n",
    "    import os, csv, json\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".json\",):\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(f\"{path}: JSON must be an object mapping leaf->type.\")\n",
    "        return {str(k): str(v) for k, v in data.items()}\n",
    "\n",
    "    elif ext in (\".csv\", \".tsv\", \".txt\"):\n",
    "        # treat .txt as TSV by default (your example is tab-delimited)\n",
    "        delim = \"\\t\" if ext in (\".tsv\", \".txt\") else \",\"\n",
    "        out = {}\n",
    "        with open(path, \"r\", newline=\"\") as f:\n",
    "            reader = csv.reader(f, delimiter=delim)\n",
    "            rows = list(reader)\n",
    "            if not rows:\n",
    "                raise ValueError(f\"{path}: empty file\")\n",
    "\n",
    "            # Detect header\n",
    "            start_idx = 0\n",
    "            header = [h.strip().lower() for h in rows[0]] if rows and rows[0] else []\n",
    "            has_header = False\n",
    "            if len(header) >= 2:\n",
    "                # Common header names we accept\n",
    "                if (\"leaf\" in header[0] or \"cellbc\" in header[0]) and (\"type\" in header[1] or \"cell_state\" in header[1]):\n",
    "                    has_header = True\n",
    "                # Or any header line where at least one of ('leaf','cellbc') and one of ('type','cell_state') appear\n",
    "                if not has_header:\n",
    "                    left_has = any(x in header for x in (\"leaf\", \"cellbc\"))\n",
    "                    right_has = any(x in header for x in (\"type\", \"cell_state\"))\n",
    "                    has_header = left_has and right_has\n",
    "\n",
    "            if has_header:\n",
    "                start_idx = 1\n",
    "\n",
    "            for i in range(start_idx, len(rows)):\n",
    "                row = rows[i]\n",
    "                if len(row) < 2:\n",
    "                    raise ValueError(f\"{path}: line {i+1} needs at least 2 columns (leaf,type)\")\n",
    "                leaf = row[0].strip()\n",
    "                typ  = row[1].strip()\n",
    "                if not leaf or not typ:\n",
    "                    raise ValueError(f\"{path}: line {i+1} has empty leaf/type\")\n",
    "                if leaf in out:\n",
    "                    raise ValueError(f\"{path}: duplicate leaf '{leaf}' at line {i+1}\")\n",
    "                out[leaf] = str(typ)  # coerce types to string (handles negatives like -7, -9)\n",
    "        return out\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mapping file type: {path} (use .csv, .tsv, .txt, or .json)\")\n",
    "\n",
    "def validate_leaf_type_map(root: TreeNode, leaf_map: Dict[str,str], S: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Ensure mapping covers exactly the leaves in the tree, and types are in S.\n",
    "    Raises ValueError if not valid.\n",
    "    \"\"\"\n",
    "    leaves_in_tree = set(collect_leaf_names(root))\n",
    "    leaves_in_map  = set(leaf_map.keys())\n",
    "\n",
    "    missing = leaves_in_tree - leaves_in_map\n",
    "    extra   = leaves_in_map  - leaves_in_tree\n",
    "    if missing:\n",
    "        raise ValueError(f\"Leaf map missing leaves: {sorted(missing)}\")\n",
    "    if extra:\n",
    "        raise ValueError(f\"Leaf map has unknown leaves not in tree: {sorted(extra)}\")\n",
    "\n",
    "    allowed = set(S)\n",
    "    bad_types = {t for t in leaf_map.values() if t not in allowed}\n",
    "    if bad_types:\n",
    "        raise ValueError(f\"Leaf map contains types not in S={S}: {sorted(bad_types)}\")\n",
    "\n",
    "def filter_leaf_map_to_tree(root: TreeNode, leaf_map: Dict[str, str]) -> Dict[str, str]:\n",
    "    leaves = set(collect_leaf_names(root))\n",
    "    return {leaf: str(typ) for leaf, typ in leaf_map.items() if leaf in leaves}\n",
    "\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# Parsing the custom TXT \"map\" format and scoring\n",
    "# ----------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "def _read_json_objects_exact(path: str):\n",
    "    \"\"\"Read one JSON object per non-empty line (your file format).\"\"\"\n",
    "    objs = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            objs.append(json.loads(line))\n",
    "    if not objs:\n",
    "        raise ValueError(f\"{path}: no JSON objects found\")\n",
    "    return objs\n",
    "\n",
    "def _extract_vertices_edges_from_adj(adj):\n",
    "    V = set(adj.keys())\n",
    "    for chs in adj.values():\n",
    "        if isinstance(chs, list):\n",
    "            V.update(chs)\n",
    "    E = []\n",
    "    for u, chs in adj.items():\n",
    "        if isinstance(chs, list):\n",
    "            for v in chs:\n",
    "                E.append((str(u), str(v)))\n",
    "    V = sorted(map(str, V), key=lambda x: (len(x), x))\n",
    "    E = sorted(E, key=lambda e: (e[0], e[1]))\n",
    "    return V, E\n",
    "\n",
    "def _normalize_adj_remove_synthetic_root(adj: dict) -> dict:\n",
    "    \"\"\"Drop a synthetic 'root' node (if present) from adjacency for building F.\"\"\"\n",
    "    adj2 = {str(k): (list(v) if isinstance(v, list) else v) for k, v in adj.items()}\n",
    "    if \"root\" in adj2:\n",
    "        ch = adj2[\"root\"]\n",
    "        if not isinstance(ch, list) or len(ch) != 1:\n",
    "            raise ValueError(\"Synthetic 'root' must have exactly one child\")\n",
    "        del adj2[\"root\"]\n",
    "    return adj2\n",
    "\n",
    "def _resolve_id_to_set(id_str: str, comp_map: dict, memo: dict, visiting: set) -> frozenset:\n",
    "    \"\"\"\n",
    "    Recursively resolve an id to a frozenset of base (negative-string) types.\n",
    "    - negative id: returns {id}\n",
    "    - list value: union of resolves\n",
    "    - single value: resolve that\n",
    "    Detects cycles and missing entries.\n",
    "    \"\"\"\n",
    "    id_str = str(id_str)\n",
    "    if id_str.startswith(\"-\"):\n",
    "        return frozenset([id_str])\n",
    "\n",
    "    if id_str in memo:\n",
    "        return memo[id_str]\n",
    "    if id_str in visiting:\n",
    "        raise ValueError(f\"Cycle detected while resolving potency '{id_str}'\")\n",
    "    if id_str not in comp_map:\n",
    "        raise ValueError(f\"Positive id '{id_str}' appears but not defined in composition map\")\n",
    "\n",
    "    visiting.add(id_str)\n",
    "    val = comp_map[id_str]\n",
    "    acc = set()\n",
    "    if isinstance(val, list):\n",
    "        for child in val:\n",
    "            acc |= _resolve_id_to_set(str(child), comp_map, memo, visiting)\n",
    "    else:\n",
    "        acc |= _resolve_id_to_set(str(val), comp_map, memo, visiting)\n",
    "    visiting.remove(id_str)\n",
    "\n",
    "    memo[id_str] = frozenset(acc)\n",
    "    return memo[id_str]\n",
    "\n",
    "def _build_ZA_from_txt(adj: dict, comp_map: dict, unit_drop_edges: bool):\n",
    "    \"\"\"\n",
    "    Build F = (Z_active, A) from adjacency + hierarchical composition map.\n",
    "    Returns: Z_active, A, base_types(list), potency_id_to_set(dict id->frozenset)\n",
    "    \"\"\"\n",
    "    # Drop synthetic \"root\" from adjacency for structure building\n",
    "    adj = _normalize_adj_remove_synthetic_root(adj)\n",
    "\n",
    "    # Collect all ids we need to resolve\n",
    "    ids_seen = set(map(str, comp_map.keys()))\n",
    "    for u, chs in adj.items():\n",
    "        ids_seen.add(str(u))\n",
    "        if isinstance(chs, list):\n",
    "            for v in chs:\n",
    "                ids_seen.add(str(v))\n",
    "\n",
    "    memo = {}\n",
    "    potency_id_to_set = {}\n",
    "    base_types = set()\n",
    "\n",
    "    # Resolve every id\n",
    "    for idv in ids_seen:\n",
    "        if idv.startswith(\"-\"):\n",
    "            memo[idv] = frozenset([idv])\n",
    "        else:\n",
    "            s = _resolve_id_to_set(idv, comp_map, memo, visiting=set())\n",
    "            potency_id_to_set[idv] = s\n",
    "\n",
    "    # Gather base types\n",
    "    for s in memo.values():\n",
    "        for t in s:\n",
    "            if t.startswith(\"-\"):\n",
    "                base_types.add(t)\n",
    "\n",
    "    # Z: singletons for all base types + multi-type potencies (size >=2)\n",
    "    Z_active = {frozenset([t]) for t in base_types}\n",
    "    for pid, s in potency_id_to_set.items():\n",
    "        if len(s) >= 2:\n",
    "            Z_active.add(s)\n",
    "\n",
    "    # A: only edges in adjacency, mapped via expansion; keep admissible ones\n",
    "    A = {}\n",
    "    def id_to_set(x: str) -> frozenset:\n",
    "        x = str(x)\n",
    "        if x.startswith(\"-\"):\n",
    "            return frozenset([x])\n",
    "        return potency_id_to_set[x]  # safe after resolution above\n",
    "\n",
    "    for u, chs in adj.items():\n",
    "        Pu = id_to_set(u)\n",
    "        for v in chs:\n",
    "            Qv = id_to_set(v)\n",
    "            if admissible_edge(Pu, Qv, unit_drop_edges):\n",
    "                A[(Pu, Qv)] = 1\n",
    "\n",
    "    return Z_active, A, sorted(base_types), potency_id_to_set\n",
    "\n",
    "def score_given_map_and_trees(txt_path: str,\n",
    "                              unit_drop_edges: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Parse your EXACT file:\n",
    "      1) adjacency dict\n",
    "      2) node weights (ignored)\n",
    "      3) composition map (CRUCIAL: defines potencies as mixtures; can reference other positives)\n",
    "      4) root id\n",
    "      5) leaf counts (ignored)\n",
    "      6) split probs (ignored)\n",
    "\n",
    "    Build F=(Z,A) from the composition map + adjacency, then compute\n",
    "    total log-likelihood across 0002_* trees with the same DP/Beta logic.\n",
    "    \"\"\"\n",
    "    objs = _read_json_objects_exact(txt_path)\n",
    "    if len(objs) < 4:\n",
    "        raise ValueError(\"Expected at least 4 JSON lines (adjacency, weights, composition map, root).\")\n",
    "\n",
    "    # 1) adjacency (first dict with list values)\n",
    "    adj = None\n",
    "    for o in objs:\n",
    "        if isinstance(o, dict) and any(isinstance(v, list) for v in o.values()):\n",
    "            adj = {str(k): [str(x) for x in v] for k, v in o.items() if isinstance(v, list)}\n",
    "            break\n",
    "    if adj is None:\n",
    "        raise ValueError(\"Could not locate adjacency dict in the file.\")\n",
    "\n",
    "    # 2) composition map (third object)\n",
    "    comp_map = objs[2]\n",
    "    if not isinstance(comp_map, dict):\n",
    "        raise ValueError(\"Third JSON must be the composition map (dict).\")\n",
    "\n",
    "    # 3) root id (fourth object) -- only for printing/sanity\n",
    "    root_id = objs[3]\n",
    "    if isinstance(root_id, dict) and \"root_id\" in root_id:\n",
    "        root_id = root_id[\"root_id\"]\n",
    "    root_id = str(root_id)\n",
    "\n",
    "    # Print vertices & edges of the given graph (raw, including 'root' if present)\n",
    "    V, E = _extract_vertices_edges_from_adj(adj)\n",
    "    print(\"=== Parsed Graph: Vertices ===\")\n",
    "    for v in V: print(\" \", v)\n",
    "    print(\"\\n=== Parsed Graph: Edges (u -> v) ===\")\n",
    "    for u, v in E: print(f\"  {u} -> {v}\")\n",
    "\n",
    "    # Build F = (Z, A) strictly from your map info (hierarchical potencies respected)\n",
    "    Z_from_map, A_from_map, base_types_map, potency_def = _build_ZA_from_txt(\n",
    "        adj=adj,\n",
    "        comp_map=comp_map,\n",
    "        unit_drop_edges=unit_drop_edges  # False allows multi-drop; True enforces unit-drop\n",
    "    )\n",
    "\n",
    "    # Optional: print expanded potency definitions\n",
    "    print(\"\\n=== Potency definitions (expanded) ===\")\n",
    "    for pid in sorted(potency_def, key=lambda x: (len(x), x)):\n",
    "        s = \",\".join(sorted(potency_def[pid]))\n",
    "        print(f\"  {pid} := {{{s}}}\")\n",
    "\n",
    "    # ----------------- Load your experimental trees + leaf maps -----------------\n",
    "    trees = [read_newick_file(\"./0002_tree_0.txt\"),\n",
    "             read_newick_file(\"./0002_tree_1.txt\"),\n",
    "             read_newick_file(\"./0002_tree_2.txt\"),\n",
    "             read_newick_file(\"./0002_tree_3.txt\"),\n",
    "             read_newick_file(\"./0002_tree_4.txt\")]\n",
    "\n",
    "    meta_paths = [\"./0002_meta_0.txt\",\"./0002_meta_1.txt\",\"./0002_meta_2.txt\",\"./0002_meta_3.txt\",\"./0002_meta_4.txt\"]\n",
    "    raw_maps = [read_leaf_type_map(p) for p in meta_paths]\n",
    "    leaf_type_maps = [filter_leaf_map_to_tree(root, m) for root, m in zip(trees, raw_maps)]\n",
    "    base_types_data = sorted({str(t) for m in leaf_type_maps for t in m.values()})\n",
    "\n",
    "    # Final S = union of base types from data and from the map\n",
    "    S_all = sorted(set(base_types_map) | set(base_types_data))\n",
    "\n",
    "    # Ensure all singletons exist for S_all and add the map-defined potencies\n",
    "    Z_active = set(Z_from_map) | {frozenset([t]) for t in S_all}\n",
    "    A = dict(A_from_map)\n",
    "    print(f\"A:{A}\")\n",
    "    print(f\"Z:{Z_active}\")\n",
    "\n",
    "    # Build Structure and score\n",
    "    struct = Structure(S=S_all, Z_active=Z_active, A=A, unit_drop=unit_drop_edges)\n",
    "    dummy_priors = Priors(potency_mode=\"fixed_k\", fixed_k=5, rho=0.2)  # priors ignored for printed likelihoods\n",
    "    k_multis = sum(1 for P in struct.Z_active if len(P) >= 2)\n",
    "    print(f\"k:{k_multis}\")\n",
    "    log_post, per_tree_logs = score_structure(\n",
    "        struct=struct,\n",
    "        trees=trees,\n",
    "        leaf_type_maps=leaf_type_maps,\n",
    "        priors=dummy_priors,\n",
    "        prune_eps=0.0\n",
    "    )\n",
    "\n",
    "    total_ll = sum(per_tree_logs)\n",
    "    print(\"\\n=== Log-likelihoods (given F from map) ===\")\n",
    "    for i, lg in enumerate(per_tree_logs, 1):\n",
    "        print(f\"Tree {i}: log P(T|F) = {lg:.6f}\")\n",
    "    print(f\"Total log-likelihood = {total_ll:.6f}\")\n",
    "    return total_ll\n",
    "\n",
    "def main():\n",
    "    import random\n",
    "    random.seed(7)\n",
    "\n",
    "    # Load Newick strings from .txt (same format as .nwk)\n",
    "    trees = [read_newick_file(\"./0002_tree_0.txt\"),\n",
    "             read_newick_file(\"./0002_tree_1.txt\"),\n",
    "             read_newick_file(\"./0002_tree_2.txt\"),\n",
    "             read_newick_file(\"./0002_tree_3.txt\"),\n",
    "             read_newick_file(\"./0002_tree_4.txt\")]\n",
    "\n",
    "    # TAB-delimited maps with header 'cellBC\\tcell_state'\n",
    "    map_paths = [\n",
    "        \"./0002_meta_0.txt\",\n",
    "        \"./0002_meta_1.txt\",\n",
    "        \"./0002_meta_2.txt\",\n",
    "        \"./0002_meta_3.txt\",\n",
    "        \"./0002_meta_4.txt\",\n",
    "    ]\n",
    "    raw_maps = [read_leaf_type_map(p) for p in map_paths]\n",
    "\n",
    "    # Drop dictionary entries not present in the corresponding tree\n",
    "    leaf_type_maps = [filter_leaf_map_to_tree(root, m) for root, m in zip(trees, raw_maps)]\n",
    "\n",
    "    # Build S from types that are actually used after filtering\n",
    "    S = sorted({str(t) for m in leaf_type_maps for t in m.values()})\n",
    "\n",
    "    # (Optional) soft warnings; never raise\n",
    "    for idx, (root, m_raw, m_used) in enumerate(zip(trees, raw_maps, leaf_type_maps), 1):\n",
    "        leaves_tree = set(collect_leaf_names(root))\n",
    "        extra = sorted(set(m_raw.keys()) - leaves_tree)\n",
    "        missing = sorted(leaves_tree - set(m_used.keys()))  # leaves in tree with no mapping\n",
    "        if extra:\n",
    "            print(f\"[warn] Tree {idx}: {len(extra)} map entries are not in the tree and were ignored \"\n",
    "                  f\"(e.g., {extra[:5]}{'...' if len(extra)>5 else ''})\")\n",
    "        if missing:\n",
    "            print(f\"[warn] Tree {idx}: {len(missing)} tree leaves have no mapping and were ignored \"\n",
    "                  f\"(e.g., {missing[:5]}{'...' if len(missing)>5 else ''})\")\n",
    "        if not any(True for _ in m_used):\n",
    "            print(f\"[warn] Tree {idx}: no mapped leaves; treating as neutral evidence.\")\n",
    "\n",
    "    priors = Priors(potency_mode=\"fixed_k\", fixed_k=5, rho=0.2)\n",
    "\n",
    "    bestF, best_score, per_tree_logs = map_search_parallel(\n",
    "        S=S,\n",
    "        trees=trees,\n",
    "        leaf_type_maps=leaf_type_maps,\n",
    "        priors=priors,\n",
    "        unit_drop_edges=False,\n",
    "        fixed_k=priors.fixed_k if priors.potency_mode==\"fixed_k\" else None,\n",
    "        init_seed=123,\n",
    "        iters=100,\n",
    "        restarts=5,\n",
    "        temp_init=1.0,\n",
    "        temp_decay=0.995,\n",
    "        move_probs=(0.3, 0.2, 0.3, 0.2),\n",
    "        prune_eps=0.0,\n",
    "        n_jobs=os.cpu_count(),   # or a smaller number if memory-bound\n",
    "\n",
    "    )\n",
    "\n",
    "    # --- Pretty-print best map ---\n",
    "    def pot_str(P): return \"{\" + \",\".join(sorted(list(P))) + \"}\"\n",
    "    print(\"\\n=== BEST MAP (F*) ===\")\n",
    "    multi_sorted = sorted([P for P in bestF.Z_active if len(P)>=2], key=lambda x:(len(x), tuple(sorted(list(x)))))\n",
    "    print(\"Active potencies (multi-type):\")\n",
    "    for P in multi_sorted: print(\"  \", pot_str(P))\n",
    "    print(\"Singletons (always active):\")\n",
    "    for t in S: print(\"  \", \"{\"+t+\"}\")\n",
    "\n",
    "    print(\"\\nEdges:\")\n",
    "    edges = sorted([e for e,v in bestF.A.items() if v==1], key=lambda e:(len(e[0]), len(e[1]), tuple(sorted(list(e[0]))), tuple(sorted(list(e[1])))))\n",
    "    for P,Q in edges:\n",
    "        print(f\"  {pot_str(P)} -> {pot_str(Q)}\")\n",
    "\n",
    "    print(\"\\nScores:\")\n",
    "    print(f\"  log posterior: {best_score:.6f}\")\n",
    "    for i,lg in enumerate(per_tree_logs,1):\n",
    "        print(f\"  Tree {i} log P(T|F*): {lg:.6f}\")\n",
    "\n",
    "def _ask_yes_no(prompt: str, default: bool = True) -> bool:\n",
    "    y = \"Y/n\" if default else \"y/N\"\n",
    "    while True:\n",
    "        ans = input(f\"{prompt} [{y}]: \").strip().lower()\n",
    "        if ans == \"\" and default is not None:\n",
    "            return default\n",
    "        if ans in (\"y\", \"yes\"): return True\n",
    "        if ans in (\"n\", \"no\"): return False\n",
    "        print(\"Please answer y or n.\")\n",
    "\n",
    "def main_cli():\n",
    "    print(\"Select mode:\")\n",
    "    print(\"  1) Run MAP search demo (uses files in code)\")\n",
    "    print(\"  2) Score a given TXT map (compute log-likelihood only)\")\n",
    "    choice = input(\"Enter 1 or 2: \").strip()\n",
    "\n",
    "    if choice == \"1\":\n",
    "        print(\"\\n[Mode 1] Running MAP search demo...\\n\")\n",
    "        main()  # your existing demo function\n",
    "        return\n",
    "\n",
    "    if choice == \"2\":\n",
    "        print(\"\\n[Mode 2] Score a given TXT map\")\n",
    "        txt_path = input(\"Path to TXT file: \").strip()\n",
    "        if not txt_path:\n",
    "            txt_path=\"main.txt\"\n",
    "            print(\"ERROR: TXT path required.\")\n",
    "            #return\n",
    "        #unit_drop_edges = _ask_yes_no(\"Use unit-drop edges (|P\\\\Q| == 1)?\", default=True)\n",
    "        print(\"\\nParsing and scoring...\\n\")\n",
    "        try:\n",
    "            score_given_map_and_trees(\n",
    "                txt_path=txt_path,\n",
    "                unit_drop_edges=False,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Invalid choice. Please run again and enter 1 or 2.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f4119-7f2a-48e8-a9ca-7fec9aed9171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
